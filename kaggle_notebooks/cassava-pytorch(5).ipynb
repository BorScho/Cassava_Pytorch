{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# directories and paths to data:\n\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n\nbase_path = Path('../input/cassava-leaf-disease-classification')\ntrain_img_dir = os.path.join(base_path,'train_images')\ntest_img_dir = os.path.join(base_path,'test_images')\n\ntrain_images = os.listdir(train_img_dir)\ntest_images = os.listdir(test_img_dir)\n\ntrain_df = pd.read_csv(os.path.join(base_path,'train.csv'))\n\ndiseaseMapping = pd.read_json(os.path.join(base_path,'label_num_to_disease_map.json'), typ='series')","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# test if training data have been loaded:\ntrain_df.head()","metadata":{"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"         image_id  label\n0  1000015157.jpg      0\n1  1000201771.jpg      3\n2   100042118.jpg      1\n3  1000723321.jpg      1\n4  1000812911.jpg      3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000015157.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000201771.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100042118.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000723321.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000812911.jpg</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Plan:**\n- look up necessary preprocessing for resnet50 input: implement it:  ok\n- load a batch of 2 images to test the snapmix augmentation part: ok\n\nWhen that is ok:\n- write data-loader to read in data and label: ok\n- test for snapmix batch-loss-function:\n- write training-loop:\n- add classifier to network:\n- train classifier:\n- thaw last layer, train some more: \n- submit code to Kaggle:\n- if better than 44% - upload to github:\n- finally: add midlayer information like described in the paper\n- if that is hopefully even better - upload to github:","metadata":{}},{"cell_type":"markdown","source":"# Training-loop:","metadata":{}},{"cell_type":"markdown","source":"1. load data\n2. augment data with given probability\n3. apply net\n4. calculate loss - store loss and accuracy\n5. back-prob\n6. update net via optimizer SGD\n7. repeat: back to 1.","metadata":{}},{"cell_type":"markdown","source":"# Data splitting, data-sets, data-loader here:","metadata":{}},{"cell_type":"code","source":"# split data in train_df into training an evaluation and test set:\n# shuffle:\n\n# choose splitting fractions:\nfraction_training = 0.8\nfraction_evaluation = 0.1\nfraction_test = 0.1\n\nassert fraction_training + fraction_evaluation + fraction_test == 1, \"fraction_training + fraction_evaluation + fraction_test must sum to 1\"\n\nimport random\n\nindex = np.arange(train_df.shape[0])\nrandom.shuffle(index)\nsplit_indeces = np.floor([len(index) * fraction_training, len(index) * (fraction_training + fraction_evaluation)]).astype(int)\neval_df = train_df[split_indeces[0]: split_indeces[1]]\ntest_df = train_df[split_indeces[1] :]\ntrain_df = train_df[: split_indeces[0]]\n\neval_df.shape, test_df.shape, train_df.shape","metadata":{"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"((2140, 2), (2140, 2), (17117, 2))"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nclass CassavaImageDataset(Dataset):\n    \"\"\"\n    Reads image name and label from pandas data-frame, loads data via PIL, applies transform to image and label and return (image, label) pair.\n    \n    Args:\n        label_image_dataframe (pandas datafram): pandas data-frame containing image name and label\n        img_dir_path (string): path to the directory containing the images\n        transform: transform to be applied to the data/images\n        label_transform: transform to be applied to the labels\n    \n    Returns:\n        sample (dictionary): {\"image\": image, \"label\": label}\n    \"\"\"\n    def __init__(self, image_label_dataframe, img_dir_path, transform=None, label_transform=None):\n        self.image_label_df = image_label_dataframe\n        self.img_dir_path = img_dir_path\n        self.transform = transform\n        self.label_transform = label_transform\n\n    def __len__(self):\n        return len(self.image_label_df)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir_path, self.image_label_df.iloc[idx, 0])\n        image = Image.open(img_path)\n        label = self.image_label_df.iloc[idx, 1]\n        if self.transform is not None:\n            image = self.transform(image)\n        if self.label_transform is not None:\n            label = self.label_transform(label)\n        sample = {\"image\": image, \"label\": label}\n        \n        return sample","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# construction of datasets and dataloader:\n\nfrom torchvision import transforms\n\n# necessary resnet50 preprocessor:\npreprocess_resnet50 = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[4.485, 0.456, 0.406],\n        std= [0.229, 0.224, 0.225]),\n])\n\n\ntrain_ds = CassavaImageDataset(train_df, train_img_dir, transform=preprocess_resnet50)\neval_ds = CassavaImageDataset(eval_df, train_img_dir, transform=preprocess_resnet50)\ntest_ds = CassavaImageDataset(test_df, train_img_dir, transform=preprocess_resnet50)\n\n\nfrom torch.utils.data import DataLoader\n\neval_size = eval_df.shape[0]\ntest_size = test_df.shape[0]\n\n# definition of dataloaders:\ntrain_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\neval_dl = DataLoader(eval_ds, batch_size=32, shuffle=False)\ntest_dl = DataLoader(test_ds, batch_size=test_size, shuffle=False)","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# test dataloader:\n\nfor i, sample in enumerate(train_dl):\n    if i > 2:\n        break\n    print(sample[\"image\"].shape)\n    print(sample[\"label\"].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model definition: resnet50, featureNet, cassava-classifier here:","metadata":{}},{"cell_type":"code","source":"# DEFINITION OF THE MODEL(S):\n\nimport torch.nn as nn\nimport torchvision.models as models\n\nclass Cassava_resnet50(nn.Module):\n    def __init__(self):\n        super(Cassava_resnet50, self).__init__()\n        resnet50 = models.resnet50(pretrained=True)\n        resnet_no_classifier = list(resnet50.children())[:-2]\n        self.featureNet = nn.Sequential(*resnet_no_classifier)\n        self.avgPool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n        self.cassava_fc = nn.Linear(2048, 5, bias=True)\n        \n    def forward(self, x):\n        \"\"\"\n        Returns: \n            has two outputs: feature_model_output, classification_model_output\n        \"\"\"\n        x = self.featureNet(x)\n        y = self.avgPool(x)\n        y = torch.squeeze(y)\n        y = self.cassava_fc(y)\n        \n        return x,y\n\n#model = Cassava_resnet50()\n#print(model.cassava_fc.weight)\n#print(next(model.named_children()))","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# overview over network architectures:\n#resnet50 # has the 1000 features linear output in the end\n#resnet50_featureNet # has the last conv layer of the resnet50 as last layer\n#cassava_resnet50 # has the 5 features linear output in the end","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing stuff here...: from logits to probabs, top-5 etc.:\n\n# Show top categories per image\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\n\ntop5_prob, top5_catid = torch.topk(probabilities, 1)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions for snapmix here:","metadata":{}},{"cell_type":"code","source":"import torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nimport numpy as np\n\n\ndef snapmix_batch_loss(label_batch, y_scores, box_weights1 = None, box_weights2 = None):\n    \"\"\"\n    Calculates the loss according to snap-mix algorithm.\n    \n    Args:\n        label_batch : true labels\n        y_scores : raw-score vectors for label-prediction\n        box_weights1 : semantic box weights of patched-into images\n        box_weights2 : semantic box weights of patched-in images\n    \n    Returns:\n        snap-mix loss\n    \"\"\"\n    loss = torch.nn.CrossEntropyLoss()(y_scores, label_batch)\n    return torch.mean(torch.mul(loss, (1 - box_weights1 + box_weights2)), dim=0)\n\n\ndef snapmix_batch_augmentation(model, img_batch, label_batch, alpha=0.2):\n    \"\"\"\n    Applies, the SnapMix-augmentation to the images and labels within a data batch with respect to a model.\n\n    Args:\n        model (Cassava_resnet50) : the part of the model ending in the last feature map (convolution) of the resnet50\n        img_batch (torch.tensor) : batch with images, all the same shape\n        label_batch (numpy list) : batch with labels for the images\n        alpha (float), optional: parameter for beta-distribution generating image shrinking-factor for box-area\n\n    Returns:\n        augmented_images : the augmented input-images\n        label_batch2 : the labels of the images that have been patched into the input-images\n        box_weights1 : batch of semantic weights of cut-out-boxes\n        box_weights2 : batch of semantic weights of patched-in-boxes\n    \"\"\"\n    # pytorch uses: B x C x H x W:\n    input_batch_size = img_batch.shape[0]\n    input_img_height = img_batch.shape[2]\n    input_img_width = img_batch.shape[3]\n        \n    box1 = random_box(input_img_width, input_img_height, alpha=alpha)\n    box2 = random_box(input_img_width, input_img_height, alpha=alpha)\n    \n    # To increase speed we copy and permutate the images of the batch and patch the images from this\n    # new batch - so we have allready the semantic percentage map for the copied batch:\n\n    # build another image batch from the input batch:\n    permutation = torch.randperm(input_batch_size)\n    label_batch = label_batch.type(torch.int64)\n    img_batch2 = torch.clone(img_batch.detach())\n    img_batch2 = img_batch2[permutation]\n    label_batch2 = torch.clone(label_batch)\n    label_batch2 = label_batch2[permutation]\n\n    # get spm and calculate boxweights:\n    SPM1 = batch_semantic_percentage_map(\n        model = model,\n        img_batch = img_batch,\n        label_batch = label_batch,\n    )\n    \n    # copy and permute the semantic percentage maps of the first batch in the same way as the\n    # images of img_batch2 :\n    SPM2 = torch.clone(SPM1)\n    SPM2 = SPM2[permutation, :, :]\n    \n    # crop boxes:\n    x11, y11, x12, y12 = box1\n    x21, y21, x22, y22 = box2\n    height_box1 = x12 - x11\n    width_box1 = y12 - y11\n    height_box2 = x22 - x21\n    width_box2 = y22 - y21\n    \n    cropped_SPM1 = TF.crop(SPM1, top=x11, left=y11, height=height_box1, width=width_box1)\n    box_weights1 = torch.sum(cropped_SPM1, dim=(1, 2))\n    cropped_SPM2 = TF.crop(SPM2, top=x21, left=y21, height=height_box2, width=width_box2)\n    box_weights2 = torch.sum(cropped_SPM2, dim=(1, 2))\n    \n    # some normalization for patching with equal labels: #--- this seems to be wrong: the box-weights depend on the localization of the boxes\n    #same_label = label_batch == label_batch2\n    #tmp = np.copy(box_weights1)\n    #box_weights1[same_label] += box_weights2[same_label]\n    #box_weights2[same_label] += tmp[same_label]\n\n    # fix for cases where box_weights are not well defined we take the relative areas of the boxes:\n    rel_area1 = height_box1 * width_box1 /  (input_img_width * input_img_height)\n    rel_area2 = height_box2 * width_box2 / (input_img_width * input_img_height)\n    box_weights1[torch.isnan(box_weights1)] = rel_area1\n    box_weights2[torch.isnan(box_weights2)] = rel_area2\n\n    #crop and paste images:\n    cropped_img2s = TF.crop(img_batch2, top=x11, left=y11, height=height_box1, width=width_box1)\n    resized_cropped_img2s = T.Resize((height_box1, width_box1))(cropped_img2s)\n    img_batch[:, :, x11: x12, y11:y12] = resized_cropped_img2s\n    \n    #return img_batch, label_batch2, box_weights1, box_weights2 -- we don't need label_batch2 outside this function\n    return img_batch, box_weights1, box_weights2\n\n\ndef batch_semantic_percentage_map(model, img_batch, label_batch):\n    \"\"\"\n    Calculates the SPM - Semantic Percentage Map of a batch of images.\n\n    Args:\n        model (Cassava_resnet50): \n        img_batch: batch of input images\n        label_batch: batch of the images labels\n\n    Returns:\n        the SPMs (Semantic Percentage Maps) for a batch of images.\n    \"\"\"\n    # weights for determining the contribution to the final classification:\n    classing_weights = model.cassava_fc.weight\n    # the batch of all feature map batches for all images in the img_batch:\n    feature_maps_batch, _ = model(img_batch) \n\n    # Calculate Class Activation Map (CAM):\n    # for the numbers: feature_maps_batch.shape = [number of images, channels, height, width]\n    img_batch_size = feature_maps_batch.shape[0] \n    feature_map_height = feature_maps_batch.shape[2]\n    feature_map_width = feature_maps_batch.shape[3]\n    CAM_batch = torch.zeros((img_batch_size, feature_map_width, feature_map_height))\n\n    clw_batch_matrix = classing_weights[label_batch, :]\n    for i in range(img_batch_size):\n        class_weights = clw_batch_matrix[i,:].detach()\n        feature_map = feature_maps_batch[i,:,:,:].detach()\n        CAM_batch[i] = torch.tensordot(class_weights, feature_map, dims=1)\n        \n    # upsampling feature map to size of image:\n    image_width = img_batch.shape[-1]\n    image_height = img_batch.shape[-2]\n    resized_CAM_batch = T.Resize((image_height, image_width))(CAM_batch)\n    \n    # move minimal value in tensor to zero, to avoid extinction when summing over the tensor:\n    resized_CAM_batch -= torch.min(resized_CAM_batch)\n    normalization_factor = torch.sum(resized_CAM_batch) + 1e-8\n    resized_CAM_batch /= normalization_factor\n\n    return resized_CAM_batch\n\n\ndef random_box(im_width, im_height, alpha, minimal_width=2, minimal_height=2):\n    \"\"\"\n    Returns a random box=(x1, y1, x2, y2) with \n    0 < x1, x2 < im_width\n    and \n    0< y1, y2, < im_height \n    that spans an area equal to \n    lambda_img * (x2 - x1) * (y2 - y1), \n    where lambda_img is randomly drawn from a beta-distribution beta(alpha, alpha)\n    \"\"\"\n    random_width = im_width + 1\n    random_height = 0\n    area = 0\n\n    while random_width > im_width or \\\n        random_height > im_height or \\\n        random_height < minimal_height or \\\n        random_width < minimal_width:\n        \n        lambda_img = torch.distributions.beta.Beta(torch.tensor([alpha]), torch.tensor([alpha])).sample().item()\n        if (lambda_img < 1 and lambda_img > 0):\n            rand_w = torch.randint(minimal_width, im_width, (1,1)).item()\n            rand_h = torch.randint(minimal_height, im_height, (1,1)).item()\n            random_width = int( rand_w * np.sqrt(lambda_img) // 1)\n            random_height = int( rand_h * np.sqrt(lambda_img) // 1)\n\n    left_upper_x = torch.randint(0, im_width - random_width + 1, (1,1)).item()\n    left_upper_y = torch.randint(0, im_height - random_height + 1, (1,1)).item()\n    box = (left_upper_x,\n           left_upper_y,\n           left_upper_x + random_width - 1,\n           left_upper_y + random_height - 1)\n\n    return box","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Code for training and evaluation","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\n\ndef simple_train_model(model, augmentation_transform, optimizer, inputs, labels):\n    \"\"\"\n    Per batch training - no snapmix augmentation.\n    \"\"\"\n    optimizer.zero_grad()\n    _, y_scores = model(augmentation_transform(inputs))\n    loss = nn.CrossEntropyLoss()(y_scores, labels)\n    loss.backward()\n    optimizer.step()\n\n    # give some feedback how it is going:\n    return loss.item()\n    \n    \ndef snapmix_train_model(model, optimizer, inputs, labels, alpha = 3.):\n    \"\"\"\n    Per batch training - with snapmix augmentation.\n    Uses GPU if possible\n    \"\"\"\n    #use GPU if possible\n    device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n\n    with torch.no_grad():\n        img_batch, box_weights1, box_weights2 = snapmix_batch_augmentation(model, inputs, labels, alpha=alpha)\n\n    img_batch = img_batch.to(device=device)\n    box_weights1 = box_weights1.to(device=device)\n    box_weights2 = box_weights2.to(device=device)\n    optimizer.zero_grad()\n    _, y_scores = model(img_batch) # y_scores: predicted y's in raw-score/logit-form\n    loss = snapmix_batch_loss(labels, y_scores, box_weights1 = box_weights1, box_weights2 = box_weights2)\n    loss.backward()\n    optimizer.step\n    \n    # give some feedback how it is going:\n    return loss.item()\n        \n\ndef predict(y_scores):\n    probabs = nn.Softmax(dim=1)\n    return torch.argmax(probabs(y_scores), dim=1)\n    \n    ","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# training- and evaluation-loop here:\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms as T\nimport time\n\n\n# use GPU if available:\ndevice = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\nprint(f\"Training on device {device}.\")\n\n\n# model construction:\nmodel = Cassava_resnet50().to(device=device) \noptimizer= torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9) # do this after moving to GPU\n\n\n# parameters:\nepochs = 10 # number of epochs to train\nsnapmix_probab = 0.5 # probability if snapmix augmentation should be applied\nsmix_alpha = 1. # alpha parameter for beta-distribution in snap-mix augmentation\naccuracies = []\nlosses = []\naugmentation = T.Compose([\n        T.RandomVerticalFlip(p=0.5),\n        T.RandomAffine(30, translate=[.2,.2], scale=None, shear=[-10,10,-10,10]),\n])\n\n\n# training:\nfor ep in range(epochs):\n    start_time = time.process_time()\n    print(\"--- Epoch: {} ---\".format(ep))\n    loss_train = 0.0\n    loss = 0.0\n    model.train()\n    for i, batch in enumerate(train_dl):\n        inputs = batch[\"image\"].to(device=device)\n        labels = batch[\"label\"].to(device=device)\n\n        # give some feedback to the user:\n        if i > 0 and i%100 == 0:\n            print(f\"- Batch {i} -\")\n            print(f\"- avg. loss: {loss_train / (i* len(labels))}\")\n        \n        s = np.random.uniform(0,1)\n        if s <= snapmix_probab:\n            loss = snapmix_train_model(model, optimizer, inputs, labels, alpha = smix_alpha)\n        else:\n            loss = simple_train_model(model, augmentation, optimizer, inputs, labels)\n        loss_train += loss\n    \n    model.eval()\n    print(f\"-- Evaluation Epoch {ep} Started --\")\n    for i, batch in enumerate(eval_dl):\n        inputs = batch[\"image\"].to(device=device)\n        labels = batch[\"label\"].to(device=device)\n        loss = 0.\n        _, y_scores = model(inputs)\n        preds = predict(y_scores)\n        accuracy = torch.sum(preds == labels)/ len(labels)\n        accuracies.append(accuracy.item())\n        loss = nn.CrossEntropyLoss()(y_scores, labels)\n        losses.append(loss.item())\n        \n        # give some feedback to the user:\n        if i > 0 and i%20 == 0:\n            print(f\"- Batch {i} -\")\n            print(\"eval batch-accuracy: {0} --- batch-loss: {1}\".format(accuracy.item(), loss.item()))\n            \n    print(\"Epoch mean accuracy: {0} --- mean loss: {1}\".format(np.mean(accuracies), np.mean(losses)))\n    elapsed_time = time.process_time() - start_time\n    print(f\"Epoch elapsed time: {elapsed_time} \")\n","metadata":{"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Training on device cuda.\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/97.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cbc8d2ee7694b4bb01d91564147088f"}},"metadata":{}},{"name":"stdout","text":"--- Epoch: 0 ---\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional_tensor.py:876: UserWarning: Argument fill/fillcolor is not supported for Tensor input. Fill value is zero\n  warnings.warn(\"Argument fill/fillcolor is not supported for Tensor input. Fill value is zero\")\n","output_type":"stream"},{"name":"stdout","text":"- Batch 100 -\n- avg. loss: 0.033822702690958974\n- Batch 200 -\n- avg. loss: 0.031265665395185355\n- Batch 300 -\n- avg. loss: 0.029567633069430787\n- Batch 400 -\n- avg. loss: 0.02828183077275753\n- Batch 500 -\n- avg. loss: 0.027323770709335803\n-- Evaluation Epoch 0 Started --\n- Batch 20 -\neval batch-accuracy: 0.6875 --- batch-loss: 0.9395585060119629\n- Batch 40 -\neval batch-accuracy: 0.6875 --- batch-loss: 0.8726247549057007\n- Batch 60 -\neval batch-accuracy: 0.78125 --- batch-loss: 0.6567948460578918\nEpoch mean accuracy: 0.6978944566712451 --- mean loss: 0.8467187836988649\nEpoch elapsed time: 325.446358099 \n--- Epoch: 1 ---\n- Batch 100 -\n- avg. loss: 0.022050203401595355\n- Batch 200 -\n- avg. loss: 0.02288370103109628\n- Batch 300 -\n- avg. loss: 0.022137066029633085\n- Batch 400 -\n- avg. loss: 0.02178815770195797\n- Batch 500 -\n- avg. loss: 0.02150015013292432\n-- Evaluation Epoch 1 Started --\n- Batch 20 -\neval batch-accuracy: 0.625 --- batch-loss: 0.8422561287879944\n- Batch 40 -\neval batch-accuracy: 0.8125 --- batch-loss: 0.6287230849266052\n- Batch 60 -\neval batch-accuracy: 0.78125 --- batch-loss: 0.6044101715087891\nEpoch mean accuracy: 0.7175173243479942 --- mean loss: 0.7652276169453094\nEpoch elapsed time: 321.27512510300005 \n--- Epoch: 2 ---\n- Batch 100 -\n- avg. loss: 0.020098411431536078\n- Batch 200 -\n- avg. loss: 0.02013726202305406\n- Batch 300 -\n- avg. loss: 0.020644230600446463\n- Batch 400 -\n- avg. loss: 0.020588935073465108\n- Batch 500 -\n- avg. loss: 0.020459719140082596\n-- Evaluation Epoch 2 Started --\n- Batch 20 -\neval batch-accuracy: 0.6875 --- batch-loss: 0.7731974124908447\n- Batch 40 -\neval batch-accuracy: 0.8125 --- batch-loss: 0.48513907194137573\n- Batch 60 -\neval batch-accuracy: 0.8125 --- batch-loss: 0.3597548305988312\nEpoch mean accuracy: 0.7423596308005983 --- mean loss: 0.7006053910474872\nEpoch elapsed time: 324.7794597589999 \n--- Epoch: 3 ---\n- Batch 100 -\n- avg. loss: 0.01951667569577694\n- Batch 200 -\n- avg. loss: 0.01951066975016147\n- Batch 300 -\n- avg. loss: 0.019398975684307516\n- Batch 400 -\n- avg. loss: 0.019600397396134214\n- Batch 500 -\n- avg. loss: 0.019259311417117717\n-- Evaluation Epoch 3 Started --\n- Batch 20 -\neval batch-accuracy: 0.6875 --- batch-loss: 0.8242583274841309\n- Batch 40 -\neval batch-accuracy: 0.8125 --- batch-loss: 0.5586161017417908\n- Batch 60 -\neval batch-accuracy: 0.78125 --- batch-loss: 0.47594407200813293\nEpoch mean accuracy: 0.7527651922471488 --- mean loss: 0.6763111650943756\nEpoch elapsed time: 322.406241381 \n--- Epoch: 4 ---\n- Batch 100 -\n- avg. loss: 0.018738394677639006\n- Batch 200 -\n- avg. loss: 0.018918346650898456\n- Batch 300 -\n- avg. loss: 0.018860433590598403\n- Batch 400 -\n- avg. loss: 0.01909832055796869\n- Batch 500 -\n- avg. loss: 0.018883866385556756\n-- Evaluation Epoch 4 Started --\n- Batch 20 -\neval batch-accuracy: 0.71875 --- batch-loss: 0.8284364938735962\n- Batch 40 -\neval batch-accuracy: 0.84375 --- batch-loss: 0.6557388305664062\n- Batch 60 -\neval batch-accuracy: 0.9375 --- batch-loss: 0.29832175374031067\nEpoch mean accuracy: 0.7636727082195567 --- mean loss: 0.653804878957236\nEpoch elapsed time: 323.55314489600005 \n--- Epoch: 5 ---\n- Batch 100 -\n- avg. loss: 0.017079688012599945\n- Batch 200 -\n- avg. loss: 0.01831323974998668\n- Batch 300 -\n- avg. loss: 0.018115649300937852\n- Batch 400 -\n- avg. loss: 0.01864663017797284\n- Batch 500 -\n- avg. loss: 0.0187996933599934\n-- Evaluation Epoch 5 Started --\n- Batch 20 -\neval batch-accuracy: 0.75 --- batch-loss: 0.7474714517593384\n- Batch 40 -\neval batch-accuracy: 0.78125 --- batch-loss: 0.5409898161888123\n- Batch 60 -\neval batch-accuracy: 0.875 --- batch-loss: 0.3201204240322113\nEpoch mean accuracy: 0.7717328540128262 --- mean loss: 0.639344061822144\nEpoch elapsed time: 323.87292890000003 \n--- Epoch: 6 ---\n- Batch 100 -\n- avg. loss: 0.01830075970850885\n- Batch 200 -\n- avg. loss: 0.017863731675315648\n- Batch 300 -\n- avg. loss: 0.017964289404141406\n- Batch 400 -\n- avg. loss: 0.01780021166196093\n- Batch 500 -\n- avg. loss: 0.017697179708629846\n-- Evaluation Epoch 6 Started --\n- Batch 20 -\neval batch-accuracy: 0.8125 --- batch-loss: 0.4324001669883728\n- Batch 40 -\neval batch-accuracy: 0.78125 --- batch-loss: 0.6025329828262329\n- Batch 60 -\neval batch-accuracy: 0.84375 --- batch-loss: 0.44749030470848083\nEpoch mean accuracy: 0.7794224037798737 --- mean loss: 0.6176420034947934\nEpoch elapsed time: 321.55780792999985 \n--- Epoch: 7 ---\n- Batch 100 -\n- avg. loss: 0.016881566541269422\n- Batch 200 -\n- avg. loss: 0.01685520542319864\n- Batch 300 -\n- avg. loss: 0.01722334523840497\n- Batch 400 -\n- avg. loss: 0.017256967442808673\n- Batch 500 -\n- avg. loss: 0.01730323039367795\n-- Evaluation Epoch 7 Started --\n- Batch 20 -\neval batch-accuracy: 0.78125 --- batch-loss: 0.5612342357635498\n- Batch 40 -\neval batch-accuracy: 0.84375 --- batch-loss: 0.6390513777732849\n- Batch 60 -\neval batch-accuracy: 0.875 --- batch-loss: 0.3892853260040283\nEpoch mean accuracy: 0.783540445588418 --- mean loss: 0.6085967020367953\nEpoch elapsed time: 323.2986845820001 \n--- Epoch: 8 ---\n- Batch 100 -\n- avg. loss: 0.017092089653015136\n- Batch 200 -\n- avg. loss: 0.01672511439304799\n- Batch 300 -\n- avg. loss: 0.016357082646961014\n- Batch 400 -\n- avg. loss: 0.017169942791806534\n- Batch 500 -\n- avg. loss: 0.017047657921910285\n-- Evaluation Epoch 8 Started --\n- Batch 20 -\neval batch-accuracy: 0.78125 --- batch-loss: 0.4951631426811218\n- Batch 40 -\neval batch-accuracy: 0.84375 --- batch-loss: 0.5122506618499756\n- Batch 60 -\neval batch-accuracy: 0.875 --- batch-loss: 0.3132103383541107\nEpoch mean accuracy: 0.787528133609797 --- mean loss: 0.5970366093303829\nEpoch elapsed time: 322.75626869200005 \n--- Epoch: 9 ---\n- Batch 100 -\n- avg. loss: 0.015061150658875705\n- Batch 200 -\n- avg. loss: 0.016512507249135524\n- Batch 300 -\n- avg. loss: 0.01693409406890472\n- Batch 400 -\n- avg. loss: 0.017134725813521073\n- Batch 500 -\n- avg. loss: 0.017264186797663568\n-- Evaluation Epoch 9 Started --\n- Batch 20 -\neval batch-accuracy: 0.84375 --- batch-loss: 0.46215176582336426\n- Batch 40 -\neval batch-accuracy: 0.8125 --- batch-loss: 0.5940102338790894\n- Batch 60 -\neval batch-accuracy: 0.875 --- batch-loss: 0.2631174623966217\nEpoch mean accuracy: 0.7913379535746219 --- mean loss: 0.5890438802206694\nEpoch elapsed time: 323.605690287 \n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Specify a path\nsave_path =\"./trained10\"\n\n# Save\ntorch.save(model, save_path)\n\n# Load\n#model = torch.load(save_path)\n#model.eval()","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Improvements:\n- Some sort of print-out while processing the batches would be nice - to see what is going on...\n- Move this code to the GPU...","metadata":{}},{"cell_type":"markdown","source":"# Test snapmix function","metadata":{}},{"cell_type":"code","source":"# load and preprocess images for resnet50:\n\nfrom PIL import Image\nfrom torchvision import transforms\n\nname_image1 = \"1000015157.jpg\"\nname_image2 = \"1000812911.jpg\"\nname_image3 = \"100042118.jpg\"\n\nlabel_batch = torch.tensor(np.array([0, 3, 1]))\n\ninput_image1 = Image.open(os.path.join(train_img_dir, name_image1))\ninput_image2 = Image.open(os.path.join(train_img_dir, name_image2))\ninput_image3 = Image.open(os.path.join(train_img_dir, name_image3))\n\n# preprocessing for torchvision resnet50 implementation:\npreprocess_resnet50 = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ninput_batch = torch.zeros((3,3,224,224))\ninput_batch[0] = preprocess_resnet50(input_image1)\ninput_batch[1] = preprocess_resnet50(input_image2)\ninput_batch[2] = preprocess_resnet50(input_image3)\n\npreprocessed_images = torch.clone(input_batch.detach())\n\n\n\nx = preprocessed_images\n\n# testing new definition of Cassava_restnet50 class:\nclass Cassava_resnet50_v2(nn.Module):\n    def __init__(self):\n        super(Cassava_resnet50_v2, self).__init__()\n        resnet50 = models.resnet50(pretrained=True)\n        resnet_no_classifier = list(resnet50.children())[:-2]\n        self.featureNet = nn.Sequential(*resnet_no_classifier)\n        self.avgPool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n        self.cassava_fc = nn.Linear(2048, 5, bias=True)\n        \n    def forward(self, x):\n        \"\"\"\n        Returns: feature_model_output, classification_model_output\n        \"\"\"\n        x = self.featureNet(x)\n        y = self.avgPool(x)\n        y = torch.squeeze(y)\n        y = self.cassava_fc(y)\n        \n        return x,y\n\n\ncas_model = Cassava_resnet50_v2()    \n\n# Call the snapmix augmentation function:\naugmented_imgs, box_weights1, box_weights2 = snapmix_batch_augmentation(cas_model, input_batch, label_batch, alpha=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cas_model","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augmented_imgs.shape, box_weights1, box_weights2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Snapmix paper includes intermediate localization results. First draft for this here:","metadata":{}},{"cell_type":"code","source":"# Snapmix paper includes intermediate localization results.\n# Try like this:\n\nimport torch.nn as nn\n\n# input plus first two \"layers\" - this is for adding the midlayer information later:\nfeature_one = nn.Sequential(*(list(resnet50.children())[0:6]))\n\n# thrid \"layer\" until end of layers, i.e. excluding the averaging and the fully connected layer:\nfeature_two = nn.Sequential(*(list(resnet50.children())[6:-2]))\n\nfor param in feature_one.parameters():\n    param.requires_grad = False\n\nfor param in feature_two.parameters():\n    param.requires_grad = False\n    \nexpert_one_gap = nn.sequential(...)\n\nexpert_one_gmp = nn.sequential(...)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}