<!doctype html><html lang="en"><head><script defer src="https://cdn.optimizely.com/js/16180790160.js"></script><title data-rh="true">Transfer Learning with Convolutional Neural Networks in PyTorch | by Will Koehrsen | Towards Data Science</title><meta data-rh="true" charset="utf-8"/><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"/><meta data-rh="true" name="theme-color" content="#000000"/><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"/><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"/><meta data-rh="true" property="al:ios:app_name" content="Medium"/><meta data-rh="true" property="al:ios:app_store_id" content="828256236"/><meta data-rh="true" property="al:android:package" content="com.medium.reader"/><meta data-rh="true" property="fb:app_id" content="542599432471018"/><meta data-rh="true" property="og:site_name" content="Medium"/><meta data-rh="true" property="og:type" content="article"/><meta data-rh="true" property="article:published_time" content="2018-11-26T19:37:01.733Z"/><meta data-rh="true" name="title" content="Transfer Learning with Convolutional Neural Networks in PyTorch | by Will Koehrsen | Towards Data Science"/><meta data-rh="true" property="og:title" content="Transfer Learning with Convolutional Neural Networks in PyTorch"/><meta data-rh="true" property="twitter:title" content="Transfer Learning with Convolutional Neural Networks in PyTorch"/><meta data-rh="true" name="twitter:site" content="@TDataScience"/><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/dd09190245ce"/><meta data-rh="true" property="al:android:url" content="medium://p/dd09190245ce"/><meta data-rh="true" property="al:ios:url" content="medium://p/dd09190245ce"/><meta data-rh="true" property="al:android:app_name" content="Medium"/><meta data-rh="true" name="description" content="Although Keras is a great library with a simple API for building neural networks, the recent excitement about PyTorch finally got me interested in exploring this library. While I’m one to blindly…"/><meta data-rh="true" property="og:description" content="How to use a pre-trained convolutional neural network for object recognition with PyTorch"/><meta data-rh="true" property="twitter:description" content="How to use a pre-trained convolutional neural network for object recognition with PyTorch"/><meta data-rh="true" property="og:url" content="https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce"/><meta data-rh="true" property="al:web:url" content="https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce"/><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/1024/1*niPDCrom9F0lrdFa3LrBqA.jpeg"/><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/1024/1*niPDCrom9F0lrdFa3LrBqA.jpeg"/><meta data-rh="true" name="twitter:card" content="summary_large_image"/><meta data-rh="true" property="article:author" content="https://williamkoehrsen.medium.com"/><meta data-rh="true" name="twitter:creator" content="@koehrsen_will"/><meta data-rh="true" name="author" content="Will Koehrsen"/><meta data-rh="true" name="robots" content="index,follow,max-image-preview:large"/><meta data-rh="true" name="referrer" content="unsafe-url"/><meta data-rh="true" name="twitter:label1" content="Reading time"/><meta data-rh="true" name="twitter:data1" content="15 min read"/><meta data-rh="true" name="parsely-post-id" content="dd09190245ce"/><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml"/><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/fit/c/152/152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/fit/c/120/120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/fit/c/76/76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/fit/c/60/60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg" color="#171717"/><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" rel="author" href="https://williamkoehrsen.medium.com"/><link data-rh="true" rel="canonical" href="https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce"/><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/dd09190245ce"/><link data-rh="true" rel="icon" href="https://miro.medium.com/fit/c/256/256/1*ChFMdf--f5jbm-AYv6VdYA@2x.png"/><link data-rh="true" rel="preload" href="https://miro.medium.com/max/2802/1*sfUruIusLq6tbpLx0sDYZQ.png" as="image"/><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1200\u002F1*niPDCrom9F0lrdFa3LrBqA.jpeg"],"url":"https:\u002F\u002Ftowardsdatascience.com\u002Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce","dateCreated":"2018-11-26T19:24:33.425Z","datePublished":"2018-11-26T19:24:33.425Z","dateModified":"2018-11-26T21:15:58.017Z","headline":"Transfer Learning with Convolutional Neural Networks in PyTorch","name":"Transfer Learning with Convolutional Neural Networks in PyTorch","description":"Although Keras is a great library with a simple API for building neural networks, the recent excitement about PyTorch finally got me interested in exploring this library. While I’m one to blindly…","identifier":"dd09190245ce","keywords":["Lite:true","Tag:Machine Learning","Tag:Data Science","Tag:Deep Learning","Tag:Pytorch","Tag:Education","Publication:towards-data-science","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_NONE","LayerCake:3"],"author":{"@type":"Person","name":"Will Koehrsen","url":"https:\u002F\u002Fwilliamkoehrsen.medium.com"},"creator":["Will Koehrsen"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"towardsdatascience.com","logo":{"@type":"ImageObject","width":165,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F330\u002F1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"}},"mainEntityOfPage":"https:\u002F\u002Ftowardsdatascience.com\u002Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce"}</script><link rel="preload" href="https://cdn.optimizely.com/js/16180790160.js" as="script"><style type="text/css" data-fela-rehydration="666" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}</style><style type="text/css" data-fela-rehydration="666" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@-moz-keyframes k1{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@keyframes k1{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@-webkit-keyframes k2{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@-moz-keyframes k2{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@keyframes k2{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@-webkit-keyframes k3{0%{transform:translateX(-1%)}20%{transform:translateX(1%)}40%{transform:translateX(-1%)}60%{transform:translateX(1%)}80%{transform:translateX(-1%)}100%{transform:translateX(1%)}}@-moz-keyframes k3{0%{transform:translateX(-1%)}20%{transform:translateX(1%)}40%{transform:translateX(-1%)}60%{transform:translateX(1%)}80%{transform:translateX(-1%)}100%{transform:translateX(1%)}}@keyframes k3{0%{transform:translateX(-1%)}20%{transform:translateX(1%)}40%{transform:translateX(-1%)}60%{transform:translateX(1%)}80%{transform:translateX(-1%)}100%{transform:translateX(1%)}}</style><style type="text/css" data-fela-rehydration="666" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{height:25px}.r{fill:rgba(41, 41, 41, 1)}.s{display:block}.t{margin-bottom:36px}.v{width:100%}.w{overflow-x:scroll}.x{white-space:nowrap}.y{scrollbar-width:none}.z{-ms-overflow-style:none}.ab::-webkit-scrollbar{display:none}.ac{box-shadow:inset 0 -1px 0 rgba(230, 230, 230, 1)}.ae{min-height:184px}.ah{flex-direction:column}.ai{background-color:#355876}.aj{display:none}.al{border-bottom:none}.am{position:relative}.an{z-index:500}.at{max-width:1192px}.au{min-width:0}.av{height:62px}.aw{flex-direction:row}.ax{flex:1 0 auto}.ay{margin-right:16px}.az{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.ba{font-size:14px}.bb{line-height:20px}.bc{color:rgba(233, 241, 250, 1)}.bd{padding:7px 16px 9px}.be{background:0}.bf{fill:rgba(233, 241, 250, 1)}.bg{border-color:rgba(215, 226, 238, 1)}.bl:disabled{cursor:inherit}.bm:disabled{opacity:0.3}.bn:disabled:hover{color:rgba(233, 241, 250, 1)}.bo:disabled:hover{fill:rgba(233, 241, 250, 1)}.bp:disabled:hover{border-color:rgba(215, 226, 238, 1)}.bq{border-radius:99em}.br{border-width:1px}.bs{border-style:solid}.bt{box-sizing:border-box}.bu{display:inline-block}.bv{text-decoration:none}.bw{margin-left:0px}.bx{color:rgba(197, 210, 225, 1)}.by{font-size:inherit}.bz{border:inherit}.ca{font-family:inherit}.cb{letter-spacing:inherit}.cc{font-weight:inherit}.cd{padding:0}.ce{margin:0}.cf:disabled{cursor:default}.cg:disabled{color:rgba(163, 208, 162, 0.5)}.ch:disabled{fill:rgba(163, 208, 162, 0.5)}.ci{min-height:115px}.cj{justify-content:space-between}.cp{align-items:flex-start}.cq{margin-bottom:0px}.cr{margin-top:-32px}.cs{flex-wrap:wrap}.cv{margin-top:32px}.cw{margin-right:24px}.cy{height:35px}.cz{width:112px}.da{flex:0 0 auto}.db{justify-self:flex-end}.dc{margin-bottom:-3px}.dd{margin-left:14px}.de{margin-top:-3px}.df{fill:rgba(251, 255, 255, 1)}.dg{padding-top:1px}.dh{height:70px}.dj{font-size:16px}.dk{line-height:24px}.dl:before{margin-bottom:-10px}.dm:before{content:""}.dn:before{display:table}.do:before{border-collapse:collapse}.dp:after{margin-top:-6px}.dq:after{content:""}.dr:after{display:table}.ds:after{border-collapse:collapse}.dt{color:rgba(117, 117, 117, 1)}.du{margin-right:32px}.dv{margin-bottom:-16px}.dw{margin-top:-14px}.dx{color:rgba(255, 255, 255, 1)}.dy{fill:rgba(255, 255, 255, 1)}.dz{background:rgba(102, 138, 170, 1)}.ea{border-color:rgba(102, 138, 170, 1)}.ed:disabled:hover{background:rgba(102, 138, 170, 1)}.ee:disabled:hover{border-color:rgba(102, 138, 170, 1)}.ef{margin-right:12px}.eg{display:inline-flex}.eh{color:inherit}.ei{fill:inherit}.el:disabled{color:rgba(117, 117, 117, 1)}.em:disabled{fill:rgba(117, 117, 117, 1)}.en{margin-left:12px}.eo{margin:0 12px}.ep{position:absolute}.eq{right:24px}.er{margin:0px}.es{border:0px}.et{padding:0px}.eu{cursor:pointer}.ev{stroke:rgba(117, 117, 117, 1)}.ey{border-top:none}.ez{left:0}.fa{opacity:0}.fb{position:fixed}.fc{right:0}.fd{top:0}.fe{visibility:hidden}.fg{height:60px}.fj{color:rgba(102, 138, 170, 1)}.fk{fill:rgba(102, 138, 170, 1)}.fn:disabled:hover{color:rgba(102, 138, 170, 1)}.fo:disabled:hover{fill:rgba(102, 138, 170, 1)}.fp{margin-left:16px}.fq{padding-left:24px}.fr{padding-right:24px}.fs{margin-left:auto}.ft{margin-right:auto}.fu{max-width:728px}.fv{background:rgba(255, 255, 255, 1)}.fw{border:1px solid rgba(230, 230, 230, 1)}.fx{border-radius:4px}.fy{box-shadow:0 1px 4px rgba(230, 230, 230, 1)}.fz{max-height:100vh}.ga{overflow-y:auto}.gb{top:calc(100vh + 100px)}.gc{bottom:calc(100vh + 100px)}.gd{width:10px}.ge{pointer-events:none}.gf{word-break:break-word}.gg{word-wrap:break-word}.gh:after{display:block}.gi:after{clear:both}.gj{clear:both}.gx{padding-bottom:5px}.gy{padding-top:5px}.ha{cursor:zoom-in}.hb{z-index:auto}.hd{transition:opacity 100ms 400ms}.he{height:100%}.hf{overflow:hidden}.hg{will-change:transform}.hh{transform:translateZ(0)}.hi{margin:auto}.hj{background-color:rgba(242, 242, 242, 1)}.hk{padding-bottom:66.69921875%}.hl{height:0}.hm{filter:blur(20px)}.hn{transform:scale(1.1)}.ho{visibility:visible}.hp{margin-top:10px}.hq{text-align:center}.ht{text-decoration:underline}.hu{max-width:680px}.hv{line-height:1.23}.hw{letter-spacing:0}.hx{font-style:normal}.hy{font-weight:700}.it{margin-bottom:-0.27em}.iu{color:rgba(41, 41, 41, 1)}.iv{line-height:1.394}.jl{margin-bottom:-0.42em}.jp{border-radius:50%}.jq{height:28px}.jr{width:28px}.js{margin:0 4px}.jt{margin:0 7px}.ju{align-items:flex-end}.kd{padding-right:8px}.ke{margin-right:8px}.kf{fill:rgba(117, 117, 117, 1)}.kg{line-height:1.58}.kh{letter-spacing:-0.004em}.ki{font-family:charter, Georgia, Cambria, "Times New Roman", Times, serif}.kw{margin-top:24px}.kx{margin-bottom:-0.46em}.ld{box-shadow:inset 3px 0 0 0 rgba(41, 41, 41, 1)}.le{padding-left:23px}.lf{margin-left:-20px}.lg{font-style:italic}.lh{max-width:891px}.ln{padding-bottom:31.425364758698095%}.lo{margin-bottom:14px}.lp{padding-top:24px}.lq{padding-bottom:10px}.lr{background-color:rgba(8, 8, 8, 1)}.ls{height:3px}.lt{width:3px}.lu{margin-right:20px}.lv{line-height:1.12}.lw{letter-spacing:-0.022em}.lx{font-weight:500}.mq{margin-bottom:-0.28em}.mw{max-width:638px}.mx{padding-bottom:56.269592476489024%}.my{list-style-type:decimal}.mz{margin-left:30px}.na{padding-left:0px}.ng{line-height:1.18}.no{margin-bottom:-0.31em}.np{padding:20px}.nq{background:rgba(242, 242, 242, 1)}.nr{overflow-x:auto}.ns{font-family:Menlo, Monaco, "Courier New", Courier, monospace}.nt{margin-top:-0.09em}.nu{margin-bottom:-0.09em}.nv{white-space:pre-wrap}.nw{padding-bottom:38.5531914893617%}.nx{max-width:631px}.ny{padding-bottom:61.33122028526149%}.nz{max-width:1728px}.oa{padding-bottom:99.47916666666667%}.ob{padding:2px 4px}.oc{font-size:75%}.od> strong{font-family:inherit}.oj{list-style-type:disc}.ok{padding-bottom:NaN%}.oq{max-width:471px}.or{padding-bottom:150.74309978768576%}.os{margin-top:0px}.ot{width:50.2%}.ou{margin-right:10px}.ov:last-of-type{margin-right:0}.ow{padding-bottom:77.24550898203593%}.ox{width:49.8%}.oy{padding-bottom:77.8672032193159%}.oz{width:200.99999999999997%}.pa{left:calc(-0.4999999999999858% - 8px)}.pb{transform:translateX(-50%)}.pc{padding-bottom:34.04255319148936%}.pd{padding-bottom:34.009661835748794%}.pe{padding-bottom:33.87872954764197%}.pf{padding-bottom:33.94406943105111%}.pg{max-width:424px}.ph{padding-bottom:102.83018867924528%}.pi{will-change:opacity}.pj{width:188px}.pk{left:50%}.pl{transform:translateX(406px)}.pm{top:calc(65px + 54px + 14px)}.pp{will-change:opacity, transform}.pq{transform:translateY(159px)}.ps{width:131px}.pt{padding-bottom:28px}.pu{border-bottom:1px solid rgba(230, 230, 230, 1)}.pv{padding-top:2px}.pw{padding-top:14px}.px{padding-top:28px}.py{margin-bottom:19px}.pz{margin-left:-3px}.qf{outline:0}.qg{border:0}.qh{user-select:none}.qi> svg{pointer-events:none}.qk{-webkit-user-select:none}.qu button{text-align:left}.qv{opacity:1}.qw{padding-right:9px}.rf{margin-top:40px}.rg{border-top:3px solid rgba(102, 138, 170, 1)}.rh{padding:32px 32px 26px 32px}.ri{margin-top:8px}.rj{margin-bottom:25px}.rk{background-color:rgba(250, 250, 250, 1)}.rm{padding-bottom:0px}.rn{padding-top:4px}.ro{font-size:13px}.rp{padding-top:8px}.sa{justify-content:flex-start}.sb{display:inline}.sc{height:56px}.sf{margin-top:}.sh{background:transparent}.si{border:none}.sj{outline:none}.sk::-webkit-inner-spin-button{-webkit-appearance:none}.sl::-webkit-inner-spin-button{-moz-appearance:none}.sm::-webkit-inner-spin-button{appearance:none}.sn::-webkit-inner-spin-button{margin:0}.so::-webkit-outer-spin-button{-webkit-appearance:none}.sp::-webkit-outer-spin-button{-moz-appearance:none}.sq::-webkit-outer-spin-button{appearance:none}.sr::-webkit-outer-spin-button{margin:0}.ss{width:375px}.st{font:inherit}.su{padding-left:auto}.sv{padding-right:auto}.sw{text-align:start}.sx::placeholder{color:rgba(117, 117, 117, 1)}.sy{padding-bottom:1px}.sz{border-bottom:1px solid rgba(168, 168, 168, 1)}.ta{margin-bottom:auto}.tb{margin-left:15px}.td{padding:7px 20px 9px}.te{padding-top:}.tf{font-size:11px}.tg{line-height:16px}.th{margin-bottom:15px}.ti{margin-top:5px}.tj{padding-bottom:25px}.tk{margin-top:25px}.tl{max-width:155px}.tp{top:1px}.ud{margin-left:-1px}.ue{margin-left:-4px}.um{padding-bottom:40px}.un{list-style-type:none}.uo{margin-bottom:8px}.up{line-height:22px}.uq{border-radius:3px}.ur{padding:5px 10px}.us{padding-bottom:4px}.ut{padding-top:32px}.ve{text-overflow:ellipsis}.vf{display:-webkit-box}.vg{-webkit-line-clamp:1}.vh{-webkit-box-orient:vertical}.vj{padding-right:168px}.vk{padding-top:25px}.vn{max-width:100%}.vo{margin-bottom:96px}.vp{margin-bottom:40px}.vq{padding-bottom:16px}.vr{margin-bottom:24px}.xb{flex-grow:0}.xc{padding-bottom:24px}.xd{max-width:500px}.xe{flex:0 1 auto}.xi{padding-bottom:8px}.xq{padding-bottom:100%}.yb{padding:32px 0}.yc{background-color:rgba(0, 0, 0, 0.9)}.yg:disabled{color:rgba(255, 255, 255, 0.7)}.yh:disabled{fill:rgba(255, 255, 255, 0.7)}.yi{height:22px}.yj{width:200px}.yl{color:rgba(255, 255, 255, 0.98)}.yo{color:rgba(255, 255, 255, 0.7)}.bh:hover{color:rgba(251, 255, 255, 1)}.bi:hover{fill:rgba(251, 255, 255, 1)}.bj:hover{border-color:rgba(251, 255, 255, 1)}.bk:hover{cursor:pointer}.eb:hover{background:rgba(90, 118, 144, 1)}.ec:hover{border-color:rgba(90, 118, 144, 1)}.ej:hover{color:rgba(25, 25, 25, 1)}.ek:hover{fill:rgba(25, 25, 25, 1)}.fl:hover{color:rgba(90, 118, 144, 1)}.fm:hover{fill:rgba(90, 118, 144, 1)}.qm:hover{fill:rgba(117, 117, 117, 1)}.xn:hover{text-decoration:underline}.ye:hover{color:rgba(255, 255, 255, 0.99)}.yf:hover{fill:rgba(255, 255, 255, 0.99)}.hc:focus{transform:scale(1.01)}.ql:focus{fill:rgba(117, 117, 117, 1)}.qj:active{border-style:none}</style><style type="text/css" data-fela-rehydration="666" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.as{margin:0 64px}.gr{max-width:1192px}.gw{margin-top:32px}.ip{font-size:46px}.iq{margin-top:0.6em}.ir{line-height:56px}.is{letter-spacing:-0.011em}.ji{font-size:22px}.jj{margin-top:0.92em}.jk{line-height:28px}.kb{margin-left:30px}.kt{font-size:21px}.ku{line-height:32px}.kv{letter-spacing:-0.003em}.lc{margin-top:2em}.lm{margin-top:56px}.mm{font-size:30px}.mn{margin-top:1.25em}.mo{line-height:36px}.mp{letter-spacing:0}.mv{margin-top:0.86em}.nf{margin-top:1.05em}.nn{margin-top:1.72em}.oi{margin-top:1.95em}.op{margin-top:1.91em}.qe{margin-right:5px}.qt{margin-top:5px}.re{padding-left:6px}.ry{font-size:16px}.rz{line-height:24px}.tr{display:inline-block}.tw{margin-left:7px}.tx{margin-top:8px}.uc{width:25px}.uk{padding-left:7px}.ul{top:3px}.vc{font-size:20px}.vd{max-height:24px}.vm{margin:0}.wg{width:calc(100% + 32px)}.wh{margin-left:-16px}.wi{margin-right:-16px}.wx{padding-left:16px}.wy{padding-right:16px}.wz{flex-basis:25%}.xa{max-width:25%}.xl{line-height:20px}.xz{min-width:70px}.ya{min-height:70px}</style><style type="text/css" data-fela-rehydration="666" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.hr{margin-left:auto}.hs{text-align:center}.ka{margin-left:30px}.qd{margin-right:5px}.qs{margin-top:5px}.rd{padding-left:6px}.tq{display:inline-block}.tu{margin-left:7px}.tv{margin-top:8px}.ub{width:25px}.ui{padding-left:7px}.uj{top:3px}</style><style type="text/css" data-fela-rehydration="666" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.jz{margin-left:30px}.qc{margin-right:5px}.qr{margin-top:5px}.rb{padding-left:6px}.rc{top:3px}.to{display:inline-block}.ts{margin-left:7px}.tt{margin-top:8px}.ua{width:15px}.uh{padding-left:3px}.xh{margin-right:16px}</style><style type="text/css" data-fela-rehydration="666" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.u{margin-bottom:20px}.af{box-shadow:inset 0 -1px 0 rgba(230, 230, 230, 1)}.ag{min-height:230px}.ak{display:block}.ck{min-height:98px}.cl{display:flex}.cm{align-items:flex-start}.cn{flex-direction:column}.co{justify-content:flex-end}.ct{margin-bottom:28px}.cu{margin-top:0px}.cx{margin-top:28px}.di{margin:0}.ew{border-top:1px solid rgba(230, 230, 230, 1)}.ex{border-bottom:1px solid rgba(230, 230, 230, 1)}.fh{align-items:center}.fi{flex:1 0 auto}.jn{margin-top:32px}.jo{flex-direction:column-reverse}.jx{margin-bottom:30px}.jy{margin-left:0px}.qb{margin-left:8px}.qp{margin-top:2px}.qq{margin-right:8px}.qz{padding-left:6px}.ra{top:3px}.rl{padding:24px 24px 28px 24px}.sd{padding-top:16px}.se{height:130px}.sg{margin-top:0}.tc{margin-top:15px}.tn{display:inline-block}.tz{width:15px}.ug{padding-left:3px}.vs{padding-bottom:12px}.vt{margin-top:16px}.xg{margin-right:16px}.xo{margin-left:16px}.xp{margin-right:0px}.yd{padding:32px 0}.yk{width:140px}.ym{margin-bottom:16px}.yn{margin-top:30px}.yp{width:100%}.yq{flex-direction:row}</style><style type="text/css" data-fela-rehydration="666" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.ao{margin:0 24px}.gk{margin:0}.gl{max-width:100%}.gs{margin-top:24px}.hz{font-size:32px}.ia{margin-top:0.64em}.ib{line-height:40px}.ic{letter-spacing:-0.016em}.iw{font-size:18px}.ix{margin-top:0.79em}.iy{line-height:24px}.jm{margin-top:32px}.jv{margin-bottom:30px}.jw{margin-left:0px}.kj{line-height:28px}.kk{letter-spacing:-0.003em}.ky{margin-top:1.56em}.li{margin-top:40px}.ly{font-size:22px}.lz{margin-top:0.93em}.ma{letter-spacing:0}.mr{margin-top:0.67em}.nb{margin-top:1.34em}.nh{font-size:20px}.ni{margin-top:1.23em}.oe{margin-top:1.2em}.ol{margin-top:1.41em}.qa{margin-left:8px}.qn{margin-top:2px}.qo{margin-right:8px}.qx{padding-left:6px}.qy{top:3px}.rq{font-size:14px}.rr{line-height:20px}.tm{display:inline-block}.ty{width:15px}.uf{padding-left:3px}.uu{font-size:16px}.uv{max-height:20px}.vu{width:calc(100% + 24px)}.vv{margin-left:-12px}.vw{margin-right:-12px}.wj{padding-left:12px}.wk{padding-right:12px}.wl{flex-basis:100%}.xf{margin-right:16px}.xm{margin-bottom:0px}.xr{min-width:48px}.xs{min-height:48px}</style><style type="text/css" data-fela-rehydration="666" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.ar{margin:0 64px}.gq{max-width:1192px}.gv{margin-top:32px}.il{font-size:46px}.im{margin-top:0.6em}.in{line-height:56px}.io{letter-spacing:-0.011em}.jf{font-size:22px}.jg{margin-top:0.92em}.jh{line-height:28px}.kq{font-size:21px}.kr{line-height:32px}.ks{letter-spacing:-0.003em}.lb{margin-top:2em}.ll{margin-top:56px}.mi{font-size:30px}.mj{margin-top:1.25em}.mk{line-height:36px}.ml{letter-spacing:0}.mu{margin-top:0.86em}.ne{margin-top:1.05em}.nm{margin-top:1.72em}.oh{margin-top:1.95em}.oo{margin-top:1.91em}.rw{font-size:16px}.rx{line-height:24px}.va{font-size:20px}.vb{max-height:24px}.vl{margin:0}.wd{width:calc(100% + 32px)}.we{margin-left:-16px}.wf{margin-right:-16px}.wt{padding-left:16px}.wu{padding-right:16px}.wv{flex-basis:25%}.ww{max-width:25%}.xk{line-height:20px}.xx{min-width:70px}.xy{min-height:70px}</style><style type="text/css" data-fela-rehydration="666" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.aq{margin:0 48px}.go{margin:0}.gp{max-width:100%}.gu{margin-top:32px}.ih{font-size:46px}.ii{margin-top:0.6em}.ij{line-height:56px}.ik{letter-spacing:-0.011em}.jc{font-size:22px}.jd{margin-top:0.92em}.je{line-height:28px}.kn{font-size:21px}.ko{line-height:32px}.kp{letter-spacing:-0.003em}.la{margin-top:2em}.lk{margin-top:56px}.me{font-size:30px}.mf{margin-top:1.25em}.mg{line-height:36px}.mh{letter-spacing:0}.mt{margin-top:0.86em}.nd{margin-top:1.05em}.nl{margin-top:1.72em}.og{margin-top:1.95em}.on{margin-top:1.91em}.ru{font-size:16px}.rv{line-height:24px}.uy{font-size:20px}.uz{max-height:24px}.wa{width:calc(100% + 28px)}.wb{margin-left:-14px}.wc{margin-right:-14px}.wp{padding-left:14px}.wq{padding-right:14px}.wr{flex-basis:50%}.ws{max-width:50%}.xj{line-height:20px}.xv{min-width:48px}.xw{min-height:48px}</style><style type="text/css" data-fela-rehydration="666" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.ap{margin:0 24px}.gm{margin:0}.gn{max-width:100%}.gt{margin-top:24px}.id{font-size:32px}.ie{margin-top:0.64em}.if{line-height:40px}.ig{letter-spacing:-0.016em}.iz{font-size:18px}.ja{margin-top:0.79em}.jb{line-height:24px}.kl{line-height:28px}.km{letter-spacing:-0.003em}.kz{margin-top:1.56em}.lj{margin-top:40px}.mb{font-size:22px}.mc{margin-top:0.93em}.md{letter-spacing:0}.ms{margin-top:0.67em}.nc{margin-top:1.34em}.nj{font-size:20px}.nk{margin-top:1.23em}.of{margin-top:1.2em}.om{margin-top:1.41em}.rs{font-size:14px}.rt{line-height:20px}.uw{font-size:16px}.ux{max-height:20px}.vx{width:calc(100% + 24px)}.vy{margin-left:-12px}.vz{margin-right:-12px}.wm{padding-left:12px}.wn{padding-right:12px}.wo{flex-basis:100%}.xt{min-width:48px}.xu{min-height:48px}</style><style type="text/css" data-fela-rehydration="666" data-fela-type="RULE" media="print">.kc{display:none}</style><style type="text/css" data-fela-rehydration="666" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.ff{animation:k2 .2s ease-in-out both}.gz{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.pn{transition:opacity 200ms}</style><style type="text/css" data-fela-rehydration="666" data-fela-type="RULE" media="all and (max-width: 1230px)">.po{display:none}</style><style type="text/css" data-fela-rehydration="666" data-fela-type="RULE" media="all and (max-width: 1198px)">.pr{display:none}</style><style type="text/css" data-fela-rehydration="666" data-fela-type="RULE" media="(orientation: landscape) and (max-width: 903.98px)">.vi{max-height:none}</style></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div class="s"><div class="t s u"><div class="ac ae s af ag"><div class="n ah ai"><div class="aj ak"><div class="al s am an"><div class="n p"><div class="ao ap aq ar as at au v"><div class="av n o"><div class="n o aw ax"><div class="ay s"><span><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&amp;source=post_page-----dd09190245ce---------------------nav_reg-----------" class="az b ba bb bc bd be bf bg bh bi bj bk bl bm bn bo bp bq br bs bt bu bv" rel="noopener">Get started</a></span></div><div class="bw aj ak"><span class="az b ba bb bx"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdd09190245ce&amp;~feature=LoOpenInAppButton&amp;~channel=ShowPostUnderCollection&amp;~stage=mobileNavBar&amp;source=post_page-----dd09190245ce--------------------------------" class="bc bf by bz ca cb cc cd ce bk bh bi cf cg ch" rel="noopener nofollow">Open in app</a></span></div></div><a href="https://medium.com/?source=post_page-----dd09190245ce--------------------------------" aria-label="Homepage" rel="noopener"><svg viewBox="0 0 1043.63 592.71" class="q bf"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div><div class="n p"><div class="ao ap aq ar as at au v"><div class="ci n o aw cj ck cl cm cn co"><div class="v n cp cj"><div class="n v"><div class="cq cr v n o aw cs ct cu cl cm cn"><div class="cv cw s cx"><a href="/?source=post_page-----dd09190245ce--------------------------------" aria-label="Publication Homepage" rel="noopener"><div class="cy cz s"><img alt="Towards Data Science" class="" src="https://miro.medium.com/max/224/1*AGyTPCaRzVqL77kFwUwHKg.png" width="112" height="35"/></div></a></div></div></div><div class="n o da db an g"><p class="az b ba bb bx"><span><a href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&amp;source=post_page-----dd09190245ce---------------------nav_reg-----------" class="bc bf by bz ca cb cc cd ce bk bh bi cf cg ch" rel="noopener">Sign in</a></span></p><div class="dc dd de cw s"><span><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&amp;source=post_page-----dd09190245ce---------------------nav_reg-----------" class="az b ba bb bc bd be bf bg bh bi bj bk bl bm bn bo bp bq br bs bt bu bv" rel="noopener">Get started</a></span></div><a href="https://medium.com/?source=post_page-----dd09190245ce--------------------------------" aria-label="Homepage" rel="noopener"><svg viewBox="0 0 1043.63 592.71" class="q df"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div></div><div class="s ak"><div class="n p"><div class="ao ap aq ar as at au v"><div class="w x y z ab"><div class="dg dh n o"><div class="s di"><span class="az b dj dk dl dm dn do dp dq dr ds dt"><div class="n o"><div class="du s"><div class="dv dw s"><div class="bu" aria-hidden="false" aria-describedby="collectionFollowPopover" aria-labelledby="collectionFollowPopover"><span><a href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&amp;source=post_page-----dd09190245ce---------------------follow_header-----------" class="az b ba bb dx bd dy dz ea eb ec bk bl bm ed ee bq br bs bt bu bv" rel="noopener"><div class="n aw">Follow</div></a></span></div></div></div><div class="ef eg ah"><a class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" rel="noopener" href="/followers?source=post_page-----dd09190245ce--------------------------------">581K Followers</a></div><div class="en s g">·</div><div class="en s g"><nav class="n o"><span class="eo n ah"><a href="https://towardsdatascience.com/tagged/editors-pick?source=post_page-----dd09190245ce--------------------------------" class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" rel="noopener">Editors&#x27; Picks</a></span><span class="eo n ah"><a href="https://towardsdatascience.com/tagged/tds-features?source=post_page-----dd09190245ce--------------------------------" class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" rel="noopener">Features</a></span><span class="eo n ah"><a href="https://towardsdatascience.com/tagged/deep-dives?source=post_page-----dd09190245ce--------------------------------" class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" rel="noopener">Deep Dives</a></span><span class="eo n ah"><a class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" rel="noopener" href="/how-to-get-the-most-out-of-towards-data-science-3bf37f75a345?source=post_page-----dd09190245ce--------------------------------">Grow</a></span><span class="eo n ah"><a class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" rel="noopener" href="/questions-96667b06af5?source=post_page-----dd09190245ce--------------------------------">Contribute</a></span></nav></div><div class="en n ah g"><a href="/about?source=post_page-----dd09190245ce--------------------------------" class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" rel="noopener">About</a></div></div></span></div><div class="aj ep eq ak"><button class="n o p er es et eu" aria-label="Expand navbar"><svg width="14" height="14" class="ev"><path d="M0 .5h14M0 7h14M0 13.5h14"></path></svg></button></div></div></div></div></div></div></div><div class="ew ex ey al c ez fa fb fc fd fe an ff"><div class="n p"><div class="ao ap aq ar as at au v"><div class="fg v aj fd an cl fh"><div class="aj cl fh fi"><span><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&amp;source=post_page-----dd09190245ce---------------------nav_reg-----------" class="az b ba bb fj bd be fk ea fl fm ec bk bl bm fn fo ee bq br bs bt bu bv" rel="noopener">Get started</a></span><div class="fp aj ak"><span class="az b ba bb dt"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdd09190245ce&amp;~feature=LoOpenInAppButton&amp;~channel=ShowPostUnderCollection&amp;~stage=mobileNavBar&amp;source=post_page-----dd09190245ce--------------------------------" class="fj fk by bz ca cb cc cd ce bk fl fm cf cg ch" rel="noopener nofollow">Open in app</a></span></div></div><a href="https://medium.com/?source=post_page-----dd09190245ce--------------------------------" aria-label="Homepage" rel="noopener"><svg viewBox="0 0 1043.63 592.71" class="q r"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div><article><section class="fq fr fs ft v fu bt s"></section><span class="s"></span><div><div class="ep ez gb gc gd ge"></div><section class="gf gg gh dq gi"><div class="gj"><div class="n p"><div class="gk gl gm gn go gp ar gq as gr au v"><figure class="gs gt gu gv gw gj gx gy paragraph-image"><div role="button" tabindex="0" class="gz ha am hb v hc"><div class="fs ft at"><div class="hi s am hj"><div class="hk hl s"><div class="fa hd ep fd ez he v hf hg hh"><img alt="" class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/1*niPDCrom9F0lrdFa3LrBqA.jpeg?q=20" width="1024" height="683" role="presentation"/></div><img alt="" class="fa hd ep fd ez he v c" width="1024" height="683" role="presentation"/><noscript><img alt="" class="ep fd ez he v" src="https://miro.medium.com/max/2048/1*niPDCrom9F0lrdFa3LrBqA.jpeg" width="1024" height="683" srcSet="https://miro.medium.com/max/552/1*niPDCrom9F0lrdFa3LrBqA.jpeg 276w, https://miro.medium.com/max/1104/1*niPDCrom9F0lrdFa3LrBqA.jpeg 552w, https://miro.medium.com/max/1280/1*niPDCrom9F0lrdFa3LrBqA.jpeg 640w, https://miro.medium.com/max/1456/1*niPDCrom9F0lrdFa3LrBqA.jpeg 728w, https://miro.medium.com/max/1632/1*niPDCrom9F0lrdFa3LrBqA.jpeg 816w, https://miro.medium.com/max/1808/1*niPDCrom9F0lrdFa3LrBqA.jpeg 904w, https://miro.medium.com/max/1984/1*niPDCrom9F0lrdFa3LrBqA.jpeg 992w, https://miro.medium.com/max/2000/1*niPDCrom9F0lrdFa3LrBqA.jpeg 1000w" sizes="1000px" role="presentation"/></noscript></div></div></div></div><figcaption class="hp hq fu fs ft hr hs az b ba bb dt">(<a href="https://www.flickr.com/photos/pinks2000/19160002254" class="eh ht" rel="noopener nofollow">Source</a>)</figcaption></figure></div></div></div><div class="n p"><div class="ao ap aq ar as hu au v"><div class=""><h1 id="4987" class="hv hw hx az hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it iu">Transfer Learning with Convolutional Neural Networks in PyTorch</h1></div><div class=""><h2 id="d748" class="iv hw hx az b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl dt">How to use a pre-trained convolutional neural network for object recognition with PyTorch</h2><div class="cv"><div class="n cj jm jn jo"><div class="o n"><div><a href="https://williamkoehrsen.medium.com/?source=post_page-----dd09190245ce--------------------------------" rel="noopener"><img alt="Will Koehrsen" class="s jp jq jr" src="https://miro.medium.com/fit/c/56/56/1*SckxdIFfjlR-cWXkL5ya-g.jpeg" width="28" height="28"/></a></div><div class="en v n cs"><div class="n"><div style="flex:1"><span class="az b ba bb iu"><a href="https://williamkoehrsen.medium.com/?source=post_page-----dd09190245ce--------------------------------" class="" rel="noopener"><p class="az b ba bb fj">Will Koehrsen</p></a></span></div></div><span class="az b ba bb dt"><a class="" rel="noopener" href="/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce?source=post_page-----dd09190245ce--------------------------------"><p class="az b ba bb dt"><span class="js"></span>Nov 26, 2018<span class="jt">·</span>15 min read</p></a></span></div></div><div class="n ju jv jw jx jy jz ka kb kc"><div class="n o"><div class="kd s"><div class="bu" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bu" role="tooltip" aria-hidden="false"><button class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div></div></div><div class="ke s"><div class="kf"><span><a href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd09190245ce&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&amp;source=post_actions_header--------------------------bookmark_preview-----------" class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></span></div></div><div class="s ax"></div></div></div></div></div></div><p id="16dc" class="kg kh hx ki b iw kj kk iz kl km kn ko kp kq kr ks kt ku kv kw kx gf iu">Although Keras is a great library with a simple API for building neural networks, the <a href="https://www.oreilly.com/ideas/why-ai-and-machine-learning-researchers-are-beginning-to-embrace-pytorch" class="eh ht" rel="noopener nofollow">recent excitement about PyTorch</a> finally got me interested in exploring this library. While I’m one to blindly follow the hype, the<a href="https://hub.packtpub.com/what-is-pytorch-and-how-does-it-work/" class="eh ht" rel="noopener nofollow"> adoption by researchers</a> and <a href="https://www.fast.ai/2017/09/08/introducing-pytorch-for-fastai/" class="eh ht" rel="noopener nofollow">inclusion in the fast.ai library </a>convinced me there must be something behind this new entry in deep learning.</p><p id="3743" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Since the <a class="eh ht" rel="noopener" href="/learn-by-sharing-4461cc93f8c1">best way to learn a new technolo<span id="rmm">g</span>y</a> is by using it to solve a problem, my efforts to learn PyTorch started out with a simple project: use a pre-trained convolutional neural network for an object recognition task. In this article, we’ll see how to use PyTorch to accomplish this goal, along the way, learning a little about the library and about the important concept of transfer learning.</p><blockquote class="ld le lf"><p id="3c31" class="kg kh lg ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">While PyTorch might not be for everyone, at this point it’s impossible to say which deep learning library will come out on top, and being able to quickly learn and use different tools is crucial to succeed as a data scientist.</p></blockquote><p id="1991" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">The complete code for this project is available as a <a href="https://github.com/WillKoehrsen/pytorch_challenge/blob/master/Transfer%20Learning%20in%20PyTorch.ipynb" class="eh ht" rel="noopener nofollow">Jupyter Notebook on GitHub</a>. This project was born out of my participation in the <a href="https://www.udacity.com/facebook-pytorch-scholarship" class="eh ht" rel="noopener nofollow">Udacity PyTorch scholarship challenge</a>.</p><figure class="li lj lk ll lm gj fs ft paragraph-image"><div role="button" tabindex="0" class="gz ha am hb v hc"><div class="fs ft lh"><div class="hi s am hj"><div class="ln hl s"><div class="fa hd ep fd ez he v hf hg hh"><img alt="" class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/1*WFzHpV_Q6GQ_ybInr4PnCA.png?q=20" width="891" height="280" role="presentation"/></div><img alt="" class="fa hd ep fd ez he v c" width="891" height="280" role="presentation"/><noscript><img alt="" class="ep fd ez he v" src="https://miro.medium.com/max/1782/1*WFzHpV_Q6GQ_ybInr4PnCA.png" width="891" height="280" srcSet="https://miro.medium.com/max/552/1*WFzHpV_Q6GQ_ybInr4PnCA.png 276w, https://miro.medium.com/max/1104/1*WFzHpV_Q6GQ_ybInr4PnCA.png 552w, https://miro.medium.com/max/1280/1*WFzHpV_Q6GQ_ybInr4PnCA.png 640w, https://miro.medium.com/max/1400/1*WFzHpV_Q6GQ_ybInr4PnCA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="hp hq fu fs ft hr hs az b ba bb dt">Predicted from trained network</figcaption></figure></div></div></section><div class="n p cv lo lp lq" role="separator"><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt"></span></div><section class="gf gg gh dq gi"><div class="n p"><div class="ao ap aq ar as hu au v"><h1 id="a556" class="lv lw hx az lx ly lz kj ma mb mc kl md me mf mg mh mi mj mk ml mm mn mo mp mq iu">Approach to Transfer Learning</h1><p id="4b69" class="kg kh hx ki b iw mr kj kk iz ms kl km kn mt ko kp kq mu kr ks kt mv ku kv kx gf iu">Our task will be to train a convolutional neural network (CNN) that can identify objects in images. We’ll be using the <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/" class="eh ht" rel="noopener nofollow">Caltech 101 dataset</a> which has images in 101 categories. Most categories only have 50 images which typically isn’t enough for a neural network to learn to high accuracy. Therefore, instead of building and training a CNN from scratch, we’ll use a pre-built and pre-trained model applying transfer learning.</p><p id="2df7" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">The basic premise of <a href="https://machinelearningmastery.com/transfer-learning-for-deep-learning/" class="eh ht" rel="noopener nofollow">transfer learning</a> is simple: take a model trained on a large dataset and <em class="lg">transfer </em>its knowledge to a smaller dataset. <a href="http://cs231n.github.io/transfer-learning/" class="eh ht" rel="noopener nofollow">For object recognition with a CNN</a>, we freeze the early convolutional layers of the network and only train the last few layers which make a prediction. The idea is the convolutional layers extract general, low-level features that are applicable across images — such as edges, patterns, gradients — and the later layers identify specific features within an image such as eyes or wheels.</p><p id="9579" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Thus, we can use a network trained on unrelated categories in a massive dataset (usually Imagenet) and apply it to our own problem because there are universal, low-level features shared between images. The images in the Caltech 101 dataset are very similar to those in the<a href="http://www.image-net.org/" class="eh ht" rel="noopener nofollow"> Imagenet dataset</a> and the knowledge a model learns on Imagenet should easily transfer to this task.</p><figure class="li lj lk ll lm gj fs ft paragraph-image"><div class="fs ft mw"><div class="hi s am hj"><div class="mx hl s"><div class="fa hd ep fd ez he v hf hg hh"><img alt="" class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/1*ZkPBqU8vx2vAgcLpz9pi5g.jpeg?q=20" width="638" height="359" role="presentation"/></div><img alt="" class="fa hd ep fd ez he v c" width="638" height="359" role="presentation"/><noscript><img alt="" class="ep fd ez he v" src="https://miro.medium.com/max/1276/1*ZkPBqU8vx2vAgcLpz9pi5g.jpeg" width="638" height="359" srcSet="https://miro.medium.com/max/552/1*ZkPBqU8vx2vAgcLpz9pi5g.jpeg 276w, https://miro.medium.com/max/1104/1*ZkPBqU8vx2vAgcLpz9pi5g.jpeg 552w, https://miro.medium.com/max/1276/1*ZkPBqU8vx2vAgcLpz9pi5g.jpeg 638w" sizes="638px" role="presentation"/></noscript></div></div></div><figcaption class="hp hq fu fs ft hr hs az b ba bb dt">Idea behind Transfer Learning (<a href="https://www.slideshare.net/xavigiro/transfer-learning-d2l4-insightdcu-machine-learning-workshop-2017" class="eh ht" rel="noopener nofollow">source</a>).</figcaption></figure><p id="f9ce" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Following is the general outline for transfer learning for object recognition:</p><ol class=""><li id="cdb2" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx my mz na iu">Load in a pre-trained CNN model trained on a large dataset</li><li id="2963" class="kg kh hx ki b iw nb kj kk iz nc kl km kn nd ko kp kq ne kr ks kt nf ku kv kx my mz na iu">Freeze parameters (weights) in model’s lower convolutional layers</li><li id="31cf" class="kg kh hx ki b iw nb kj kk iz nc kl km kn nd ko kp kq ne kr ks kt nf ku kv kx my mz na iu">Add custom classifier with several layers of trainable parameters to model</li><li id="9b89" class="kg kh hx ki b iw nb kj kk iz nc kl km kn nd ko kp kq ne kr ks kt nf ku kv kx my mz na iu">Train classifier layers on training data available for task</li><li id="c8f4" class="kg kh hx ki b iw nb kj kk iz nc kl km kn nd ko kp kq ne kr ks kt nf ku kv kx my mz na iu">Fine-tune hyperparameters and unfreeze more layers as needed</li></ol><p id="2573" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">This <a href="http://ruder.io/transfer-learning/" class="eh ht" rel="noopener nofollow">approach has proven successful for a wide range of domains</a>. It’s a great tool to have in your arsenal and generally the first approach that should be tried when confronted with a new image recognition problem.</p></div></div></section><div class="n p cv lo lp lq" role="separator"><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt"></span></div><section class="gf gg gh dq gi"><div class="n p"><div class="ao ap aq ar as hu au v"><h2 id="c5ec" class="ng lw hx az lx nh ni iy ma nj nk jb md jc nl je mh jf nm jh ml ji nn jk mp no iu">Data Set Up</h2><p id="6438" class="kg kh hx ki b iw mr kj kk iz ms kl km kn mt ko kp kq mu kr ks kt mv ku kv kx gf iu">With all data science problems, formatting the data correctly will determine the success or failure of the project. Fortunately, the Caltech 101 dataset images are clean and stored in the correct format. If we correctly set up the data directories, PyTorch makes it simple to associate the correct labels with each class. I separated the data into <em class="lg">training, validation, and testing</em> sets with a 50%, 25%, 25% split and then structured the directories as follows:</p><pre class="li lj lk ll lm np nq nr"><span id="8644" class="iu ng lw hx ns b dj nt nu s nv">/datadir<br/>    /train<br/>        /class1<br/>        /class2<br/>        .<br/>        .<br/>    /valid<br/>        /class1<br/>        /class2<br/>        .<br/>        .<br/>    /test<br/>        /class1<br/>        /class2<br/>        .<br/>        .</span></pre><p id="06ec" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">The number of training images by classes is below (I use the terms classes and categories interchangeably):</p></div></div><div class="gj"><div class="n p"><div class="gk gl gm gn go gp ar gq as gr au v"><figure class="li lj lk ll lm gj gx gy paragraph-image"><div role="button" tabindex="0" class="gz ha am hb v hc"><div class="fs ft at"><div class="hi s am hj"><div class="nw hl s"><div class="fa hd ep fd ez he v hf hg hh"><img alt="" class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/1*H6FG-Tw0Ashd9_BuGxekNg.png?q=20" width="1175" height="453" role="presentation"/></div><img alt="" class="fa hd ep fd ez he v c" width="1175" height="453" role="presentation"/><noscript><img alt="" class="ep fd ez he v" src="https://miro.medium.com/max/2350/1*H6FG-Tw0Ashd9_BuGxekNg.png" width="1175" height="453" srcSet="https://miro.medium.com/max/552/1*H6FG-Tw0Ashd9_BuGxekNg.png 276w, https://miro.medium.com/max/1104/1*H6FG-Tw0Ashd9_BuGxekNg.png 552w, https://miro.medium.com/max/1280/1*H6FG-Tw0Ashd9_BuGxekNg.png 640w, https://miro.medium.com/max/1456/1*H6FG-Tw0Ashd9_BuGxekNg.png 728w, https://miro.medium.com/max/1632/1*H6FG-Tw0Ashd9_BuGxekNg.png 816w, https://miro.medium.com/max/1808/1*H6FG-Tw0Ashd9_BuGxekNg.png 904w, https://miro.medium.com/max/1984/1*H6FG-Tw0Ashd9_BuGxekNg.png 992w, https://miro.medium.com/max/2000/1*H6FG-Tw0Ashd9_BuGxekNg.png 1000w" sizes="1000px" role="presentation"/></noscript></div></div></div></div><figcaption class="hp hq fu fs ft hr hs az b ba bb dt">Number of training images by category.</figcaption></figure></div></div></div><div class="n p"><div class="ao ap aq ar as hu au v"><p id="b450" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">We expect the model to do better on classes with <em class="lg">more</em> examples because it can better learn to map features to labels. To deal with the limited number of training examples we’ll use <em class="lg">data augmentation</em> during training (more later).</p><p id="4eb6" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">As another bit of data exploration, we can also look at the size distribution.</p><figure class="li lj lk ll lm gj fs ft paragraph-image"><div class="fs ft nx"><div class="hi s am hj"><div class="ny hl s"><div class="fa hd ep fd ez he v hf hg hh"><img alt="" class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/1*NNBg34r4lVD3rIYifVEd-Q.png?q=20" width="631" height="387" role="presentation"/></div><img alt="" class="fa hd ep fd ez he v c" width="631" height="387" role="presentation"/><noscript><img alt="" class="ep fd ez he v" src="https://miro.medium.com/max/1262/1*NNBg34r4lVD3rIYifVEd-Q.png" width="631" height="387" srcSet="https://miro.medium.com/max/552/1*NNBg34r4lVD3rIYifVEd-Q.png 276w, https://miro.medium.com/max/1104/1*NNBg34r4lVD3rIYifVEd-Q.png 552w, https://miro.medium.com/max/1262/1*NNBg34r4lVD3rIYifVEd-Q.png 631w" sizes="631px" role="presentation"/></noscript></div></div></div><figcaption class="hp hq fu fs ft hr hs az b ba bb dt">Distribution of average image sizes (in pixels) by category.</figcaption></figure><p id="f61d" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Imagenet models need an input size of 224 x 224 so one of the <em class="lg">preprocessing</em> steps will be to resize the images. Preprocessing is also where we will implement data augmentation for our training data.</p><h2 id="0556" class="ng lw hx az lx nh ni iy ma nj nk jb md jc nl je mh jf nm jh ml ji nn jk mp no iu">Data Augmentation</h2><p id="a23d" class="kg kh hx ki b iw mr kj kk iz ms kl km kn mt ko kp kq mu kr ks kt mv ku kv kx gf iu">The idea of <a href="http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf" class="eh ht" rel="noopener nofollow">data augmentation</a> is to artificially increase the number of training images our model sees by applying random transformations to the images. For example, we can randomly rotate or crop the images or flip them horizontally. We want our model to distinguish the objects regardless of orientation and data augmentation can also make a model <a href="https://stats.stackexchange.com/questions/239076/about-cnn-kernels-and-scale-rotation-invariance" class="eh ht" rel="noopener nofollow">invariant</a> to transformations of the input data.</p><p id="4113" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">An elephant is still an elephant no matter which way it’s facing!</p><figure class="li lj lk ll lm gj fs ft paragraph-image"><div role="button" tabindex="0" class="gz ha am hb v hc"><div class="fs ft nz"><div class="hi s am hj"><div class="oa hl s"><div class="fa hd ep fd ez he v hf hg hh"><img alt="" class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/1*nyHJUPw_2UOmRqGQ5mAzkg.png?q=20" width="1728" height="1719" role="presentation"/></div><img alt="" class="fa hd ep fd ez he v c" width="1728" height="1719" role="presentation"/><noscript><img alt="" class="ep fd ez he v" src="https://miro.medium.com/max/3456/1*nyHJUPw_2UOmRqGQ5mAzkg.png" width="1728" height="1719" srcSet="https://miro.medium.com/max/552/1*nyHJUPw_2UOmRqGQ5mAzkg.png 276w, https://miro.medium.com/max/1104/1*nyHJUPw_2UOmRqGQ5mAzkg.png 552w, https://miro.medium.com/max/1280/1*nyHJUPw_2UOmRqGQ5mAzkg.png 640w, https://miro.medium.com/max/1400/1*nyHJUPw_2UOmRqGQ5mAzkg.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="hp hq fu fs ft hr hs az b ba bb dt">Image transformations of training data.</figcaption></figure><p id="9366" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Augmentation is generally only done during training (although <a href="https://blog.floydhub.com/ten-techniques-from-fast-ai/" class="eh ht" rel="noopener nofollow">test time augmentation is possible in the </a><code class="hj ob oc od ns b"><a href="https://blog.floydhub.com/ten-techniques-from-fast-ai/" class="eh ht" rel="noopener nofollow">fast.ai</a></code> library). Each epoch — one iteration through all the training images — a different random transformation is applied to each training image. This means that if we iterate through the data 20 times, our model will see 20 slightly different versions of each image. The overall result should be a model that learns the objects themselves and not how they are presented or artifacts in the image.</p><h1 id="7031" class="lv lw hx az lx ly oe kj ma mb of kl md me og mg mh mi oh mk ml mm oi mo mp mq iu">Image Preprocessing</h1><p id="d57d" class="kg kh hx ki b iw mr kj kk iz ms kl km kn mt ko kp kq mu kr ks kt mv ku kv kx gf iu">This is the most important step of working with image data. During image preprocessing, we simultaneously prepare the images for our network and apply data augmentation to the training set. Each model will have different input requirements, <a href="https://github.com/pytorch/vision/issues/39" class="eh ht" rel="noopener nofollow">but if we read through what Imagenet requires</a>, we figure out that our images need to be 224x224 and normalized to a range.</p><p id="9ca0" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">To process an image in PyTorch, we use <code class="hj ob oc od ns b">transforms</code> , simple operations applied to arrays. The validation (and testing) transforms are as follows:</p><ul class=""><li id="8799" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx oj mz na iu">Resize</li><li id="8b19" class="kg kh hx ki b iw nb kj kk iz nc kl km kn nd ko kp kq ne kr ks kt nf ku kv kx oj mz na iu">Center crop to 224 x 224</li><li id="37de" class="kg kh hx ki b iw nb kj kk iz nc kl km kn nd ko kp kq ne kr ks kt nf ku kv kx oj mz na iu">Convert to a tensor</li><li id="65b3" class="kg kh hx ki b iw nb kj kk iz nc kl km kn nd ko kp kq ne kr ks kt nf ku kv kx oj mz na iu">Normalize with mean and standard deviation</li></ul><p id="fda7" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">The end result of passing through these transforms are tensors that can go into our network. The training transformations are similar but with the addition of random augmentations.</p><p id="e8e5" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">First up, we define the training and validation transformations:</p><figure class="li lj lk ll lm gj"><div class="hi s am"><div class="ok hl s"></div></div></figure><p id="1e22" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Then, we create <code class="hj ob oc od ns b">datasets</code> and <code class="hj ob oc od ns b">DataLoaders</code> . By using <code class="hj ob oc od ns b">datasets.ImageFolder</code> to make a dataset, PyTorch will automatically associate images with the correct labels provided our directory is set up as above. The datasets are then passed to a <code class="hj ob oc od ns b">DataLoader</code> , an iterator that yield batches of images and labels.</p><figure class="li lj lk ll lm gj"><div class="hi s am"><div class="ok hl s"></div></div></figure><p id="2ab7" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">We can see the iterative behavior of the <code class="hj ob oc od ns b">DataLoader</code> using the following:</p><pre class="li lj lk ll lm np nq nr"><span id="5976" class="iu ng lw hx ns b dj nt nu s nv"># Iterate through the dataloader once<br/>trainiter = iter(dataloaders[&#x27;train&#x27;])<br/>features, labels = next(trainiter)<br/>features.shape, labels.shape</span><span id="2ccf" class="iu ng lw hx ns b dj ol om on oo op nu s nv"><strong class="ns hy">(torch.Size([128, 3, 224, 224]), torch.Size([128]))</strong></span></pre><p id="c587" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">The shape of a batch is <code class="hj ob oc od ns b">(batch_size, color_channels, height, width)</code>. During training, validation, and eventually testing, we’ll iterate through the <code class="hj ob oc od ns b">DataLoaders</code>, with one pass through the complete dataset comprising one epoch. Every epoch, the training <code class="hj ob oc od ns b">DataLoader</code> will apply a slightly different random transformation to the images for training data augmentation.</p></div></div></section><div class="n p cv lo lp lq" role="separator"><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt"></span></div><section class="gf gg gh dq gi"><div class="n p"><div class="ao ap aq ar as hu au v"><h1 id="4d4c" class="lv lw hx az lx ly lz kj ma mb mc kl md me mf mg mh mi mj mk ml mm mn mo mp mq iu">Pre-Trained Models for Image Recognition</h1><p id="9e76" class="kg kh hx ki b iw mr kj kk iz ms kl km kn mt ko kp kq mu kr ks kt mv ku kv kx gf iu">With our data in shape, we next turn our attention to the model. For this, we’ll use a pre-trained convolutional neural network. PyTorch has a number of models that have already been trained on millions of images from 1000 classes in <a href="http://www.image-net.org/" class="eh ht" rel="noopener nofollow">Imagenet</a>. The complete list of models can be <a href="https://pytorch.org/docs/stable/torchvision/models.html" class="eh ht" rel="noopener nofollow">seen here</a>. The performance of these models on Imagenet is shown below:</p><figure class="li lj lk ll lm gj fs ft paragraph-image"><div class="fs ft oq"><div class="hi s am hj"><div class="or hl s"><div class="fa hd ep fd ez he v hf hg hh"><img alt="" class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/40/1*0W310-cMNHPWjErqPuGXpw.png?q=20" width="471" height="710" role="presentation"/></div><img alt="" class="fa hd ep fd ez he v c" width="471" height="710" role="presentation"/><noscript><img alt="" class="ep fd ez he v" src="https://miro.medium.com/max/942/1*0W310-cMNHPWjErqPuGXpw.png" width="471" height="710" srcSet="https://miro.medium.com/max/552/1*0W310-cMNHPWjErqPuGXpw.png 276w, https://miro.medium.com/max/942/1*0W310-cMNHPWjErqPuGXpw.png 471w" sizes="471px" role="presentation"/></noscript></div></div></div><figcaption class="hp hq fu fs ft hr hs az b ba bb dt">Pretrained models in PyTorch and performance on Imagenet (<a href="https://pytorch.org/docs/stable/torchvision/models.html" class="eh ht" rel="noopener nofollow">Source</a>).</figcaption></figure><p id="5aa6" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">For this implementation, we’ll be using the <code class="hj ob oc od ns b">VGG-16</code>. Although it didn’t record the lowest error, I found it worked well for the task and was quicker to train than other models. The process to use a pre-trained model is well-established:</p><ol class=""><li id="a02b" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx my mz na iu">Load in pre-trained weights from a network trained on a large dataset</li><li id="586e" class="kg kh hx ki b iw nb kj kk iz nc kl km kn nd ko kp kq ne kr ks kt nf ku kv kx my mz na iu">Freeze all the weights in the lower (convolutional) layers: the layers to freeze are adjusted depending on similarity of new task to original dataset</li><li id="9e26" class="kg kh hx ki b iw nb kj kk iz nc kl km kn nd ko kp kq ne kr ks kt nf ku kv kx my mz na iu">Replace the upper layers of the network with a custom classifier: the number of outputs must be set equal to the number of classes</li><li id="ecc6" class="kg kh hx ki b iw nb kj kk iz nc kl km kn nd ko kp kq ne kr ks kt nf ku kv kx my mz na iu">Train only the custom classifier layers for the task thereby optimizing the model for smaller dataset</li></ol><p id="f6d7" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Loading in a pre-trained model in PyTorch is simple:</p><pre class="li lj lk ll lm np nq nr"><span id="3f19" class="iu ng lw hx ns b dj nt nu s nv">from torchvision import models<br/>model = model.vgg16(pretrained=True)</span></pre><p id="bd1e" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">This model has over 130 million parameters, but we’ll train only the very last few fully-connected layers. Initially, we freeze all of the model’s weights:</p><pre class="li lj lk ll lm np nq nr"><span id="a916" class="iu ng lw hx ns b dj nt nu s nv"># Freeze model weights<br/>for param in model.parameters():<br/>    param.requires_grad = False</span></pre><p id="84b9" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Then, we add on our own custom classifier with the following layers:</p><ul class=""><li id="eb22" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx oj mz na iu">Fully connected with ReLU activation, shape = (n_inputs, 256)</li><li id="2a73" class="kg kh hx ki b iw nb kj kk iz nc kl km kn nd ko kp kq ne kr ks kt nf ku kv kx oj mz na iu">Dropout with 40% chance of dropping</li><li id="61f6" class="kg kh hx ki b iw nb kj kk iz nc kl km kn nd ko kp kq ne kr ks kt nf ku kv kx oj mz na iu">Fully connected with log softmax output, shape = (256, n_classes)</li></ul><pre class="li lj lk ll lm np nq nr"><span id="3e2d" class="iu ng lw hx ns b dj nt nu s nv">import torch.nn as nn</span><span id="f2f9" class="iu ng lw hx ns b dj ol om on oo op nu s nv"># Add on classifier<br/>model.classifier[6] = nn.Sequential(<br/>                      nn.Linear(n_inputs, 256), <br/>                      nn.ReLU(), <br/>                      nn.Dropout(0.4),<br/>                      nn.Linear(256, n_classes),                   <br/>                      nn.LogSoftmax(dim=1))</span></pre><p id="e5f3" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">When the extra layers are added to the model, they are set to trainable by default ( <code class="hj ob oc od ns b">require_grad=True</code> ). For the VGG-16, we’re only changing the very last original fully-connected layer. All of the weights in the convolutional layers and the the first 5 fully-connected layers are not trainable.</p><pre class="li lj lk ll lm np nq nr"><span id="d763" class="iu ng lw hx ns b dj nt nu s nv"># Only training classifier[6]<br/>model.classifier</span><span id="44f3" class="iu ng lw hx ns b dj ol om on oo op nu s nv"><strong class="ns hy">Sequential(<br/>  (0): Linear(in_features=25088, out_features=4096, bias=True)<br/>  (1): ReLU(inplace)<br/>  (2): Dropout(p=0.5)<br/>  (3): Linear(in_features=4096, out_features=4096, bias=True)<br/>  (4): ReLU(inplace)<br/>  (5): Dropout(p=0.5)<br/>  (6): Sequential(<br/>    (0): Linear(in_features=4096, out_features=256, bias=True)<br/>    (1): ReLU()<br/>    (2): Dropout(p=0.4)<br/>    (3): Linear(in_features=256, out_features=100, bias=True)<br/>    (4): LogSoftmax()<br/>  )<br/>)</strong></span></pre><p id="73cb" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">The final outputs from the network are <em class="lg">log probabilities </em>for each of the 100 classes in our dataset. The model has a total of 135 million parameters, of which just over 1 million will be trained.</p><pre class="li lj lk ll lm np nq nr"><span id="669f" class="iu ng lw hx ns b dj nt nu s nv"># Find total parameters and trainable parameters<br/>total_params = sum(p.numel() for p in model.parameters())<br/>print(f&#x27;{total_params:,} total parameters.&#x27;)<br/>total_trainable_params = sum(<br/>    p.numel() for p in model.parameters() if p.requires_grad)<br/>print(f&#x27;{total_trainable_params:,} training parameters.&#x27;)</span><span id="c22d" class="iu ng lw hx ns b dj ol om on oo op nu s nv"><strong class="ns hy">135,335,076 total parameters.<br/>1,074,532 training parameters.</strong></span></pre><h2 id="9ef6" class="ng lw hx az lx nh ni iy ma nj nk jb md jc nl je mh jf nm jh ml ji nn jk mp no iu">Moving Model to GPU(s)</h2><p id="f2a8" class="kg kh hx ki b iw mr kj kk iz ms kl km kn mt ko kp kq mu kr ks kt mv ku kv kx gf iu">One of the best aspects of <a href="https://pytorch.org/docs/stable/notes/cuda.html" class="eh ht" rel="noopener nofollow">PyTorch is the ease of moving different parts of a model to one or more gpus</a> so you can <a href="https://medium.com/@colinshaw_36798/fully-utilizing-your-deep-learning-gpus-61ee7acd3e57" class="eh ht" rel="noopener">make full use of your hardware</a>. Since I’m using 2 gpus for training, I first move the model to <code class="hj ob oc od ns b">cuda</code> and then create a <code class="hj ob oc od ns b">DataParallel</code> model distributed over the gpus:</p><pre class="li lj lk ll lm np nq nr"><span id="9af5" class="iu ng lw hx ns b dj nt nu s nv"># Move to gpu<br/>model = model.to(&#x27;cuda&#x27;)<br/># Distribute across 2 gpus<br/>model = nn.DataParallel(model)</span></pre><p id="981f" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">(This notebook should be run on a gpu to complete in a reasonable amount of time. The speedup over a cpu can easily by 10x or more.)</p><h2 id="e115" class="ng lw hx az lx nh ni iy ma nj nk jb md jc nl je mh jf nm jh ml ji nn jk mp no iu">Training Loss and Optimizer</h2><p id="cf28" class="kg kh hx ki b iw mr kj kk iz ms kl km kn mt ko kp kq mu kr ks kt mv ku kv kx gf iu">The training loss (the error or difference between predictions and true values) is the <a href="https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/" class="eh ht" rel="noopener nofollow">negative log likelihood</a> (NLL). (The NLL loss in PyTorch expects log probabilities, so we pass in the raw output from the model’s final layer.) <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" class="eh ht" rel="noopener nofollow">PyTorch uses automatic differentiation</a> which means that tensors keep track of not only their value, but also every operation (multiply, addition, activation, etc.) which contributes to the value. This means we can compute the gradient for any tensor in the network with respect to any prior tensor.</p><p id="84ff" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">What this means in practice is that the loss tracks not only the <em class="lg">error</em>, but also the <em class="lg">contribution to the error by each weight and bias</em> in the model. After we calculate the loss, we can then find the gradients of the loss with respect to each model parameter, a process known as <a href="http://neuralnetworksanddeeplearning.com/chap2.html" class="eh ht" rel="noopener nofollow">backpropagation</a>. Once we have the gradients, we use them to update the parameters with the optimizer. (If this doesn’t sink in at first, don’t worry, it takes a little while to grasp! This <a href="https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf" class="eh ht" rel="noopener nofollow">powerpoint</a> helps to clarify some points.)</p><p id="c33f" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">The <a href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" class="eh ht" rel="noopener nofollow">optimizer is Adam</a>, an efficient variant of <a href="https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html" class="eh ht" rel="noopener nofollow">gradient descent</a> that generally does not require hand-tuning the learning rate. During training, the optimizer uses the gradients of the loss to try and reduce the error (“optimize”) of the model output by adjusting the parameters. <em class="lg">Only the parameters we added in the custom classifier will be optimized</em>.</p><p id="d448" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">The loss and optimizer are initialized as follows:</p><pre class="li lj lk ll lm np nq nr"><span id="b986" class="iu ng lw hx ns b dj nt nu s nv">from torch import optim</span><span id="4f11" class="iu ng lw hx ns b dj ol om on oo op nu s nv"># Loss and optimizer<br/>criteration = nn.NLLLoss()<br/>optimizer = optim.Adam(model.parameters())</span></pre><p id="acfc" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">With the pre-trained model, the custom classifier, the loss, the optimizer, and most importantly, the data, we’re ready for training.</p></div></div></section><div class="n p cv lo lp lq" role="separator"><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt"></span></div><section class="gf gg gh dq gi"><div class="n p"><div class="ao ap aq ar as hu au v"><h1 id="ab6e" class="lv lw hx az lx ly lz kj ma mb mc kl md me mf mg mh mi mj mk ml mm mn mo mp mq iu">Training</h1><p id="e1bc" class="kg kh hx ki b iw mr kj kk iz ms kl km kn mt ko kp kq mu kr ks kt mv ku kv kx gf iu">Model training in PyTorch is a little more hands-on than in Keras because we have to do the backpropagation and parameter update step ourselves. The main loop iterates over a number of epochs and on each epoch we iterate through the train <code class="hj ob oc od ns b">DataLoader</code> . The <code class="hj ob oc od ns b">DataLoader</code> yields one batch of data and targets which we pass through the model. After each training batch, we calculate the loss, backpropagate the gradients of the loss with respect to the model parameters, and then update the parameters with the optimizer.</p><p id="c020" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">I’d encourage you to look at the <a href="https://github.com/WillKoehrsen/pytorch_challenge/blob/master/Transfer%20Learning%20in%20PyTorch.ipynb" class="eh ht" rel="noopener nofollow">notebook</a> for the complete training details, but the basic pseudo-code is as follows:</p><figure class="li lj lk ll lm gj"><div class="hi s am"><div class="ok hl s"></div></div></figure><p id="aad7" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">We can continue to iterate through the data until we reach a given number of epochs. However, one problem with this approach is that our model will eventually start <em class="lg">overfitting to the training data</em>. To prevent this, we use our validation data and <em class="lg">early stopping</em>.</p><h2 id="5caf" class="ng lw hx az lx nh ni iy ma nj nk jb md jc nl je mh jf nm jh ml ji nn jk mp no iu">Early Stopping</h2><p id="9377" class="kg kh hx ki b iw mr kj kk iz ms kl km kn mt ko kp kq mu kr ks kt mv ku kv kx gf iu"><a href="https://en.wikipedia.org/wiki/Early_stopping" class="eh ht" rel="noopener nofollow">Early stopping</a> means halting training when the validation loss has not decreased for a number of epochs. As we continue training, the training loss will only decrease, but the validation loss will eventually reach a minimum and plateau or start to increase. We ideally want to stop training when the validation loss is at a minimum in the hope that this model will generalize best to the testing data. When using early stopping, every epoch in which the validation loss decreases, we save the parameters so we can later retrieve those with the best validation performance.</p><p id="ded9" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">We implement early stopping by iterating through the validation <code class="hj ob oc od ns b">DataLoader</code> at the end of each training epoch. We calculate the validation loss and compare this to the lowest validation loss. If the loss is the lowest so far, we save the model. If the loss has not improved for a certain number of epochs, we halt training and return the best model which has been saved to disk.</p><p id="a4c9" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Again, the complete code is in the notebook, but pseudo-code is:</p><figure class="li lj lk ll lm gj"><div class="hi s am"><div class="ok hl s"></div></div></figure><p id="4c8c" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">To see the benefits of early stopping, we can look at the training curves showing the training and validation losses and accuracy:</p></div></div><div class="gj"><div class="n p"><div class="gk gl gm gn go gp ar gq as gr au v"><div class="li lj lk ll lm n aw"><figure class="os gj ot ou gy gx ov paragraph-image"><div class="hi s am hj"><div class="ow hl s"><div class="fa hd ep fd ez he v hf hg hh"><img alt="" class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/1*xega4-IjqB_YJq_u0aHuzA.png?q=20" width="501" height="387" role="presentation"/></div><img alt="" class="fa hd ep fd ez he v c" width="501" height="387" role="presentation"/><noscript><img alt="" class="ep fd ez he v" src="https://miro.medium.com/max/1002/1*xega4-IjqB_YJq_u0aHuzA.png" width="501" height="387" srcSet="https://miro.medium.com/max/552/1*xega4-IjqB_YJq_u0aHuzA.png 276w, https://miro.medium.com/max/1002/1*xega4-IjqB_YJq_u0aHuzA.png 501w" sizes="501px" role="presentation"/></noscript></div></div></figure><figure class="os gj ox ou gy gx ov paragraph-image"><div class="hi s am hj"><div class="oy hl s"><div class="fa hd ep fd ez he v hf hg hh"><img alt="" class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/1*Gx-V-eTwCbVF6O6fXdjsPA.png?q=20" width="497" height="387" role="presentation"/></div><img alt="" class="fa hd ep fd ez he v c" width="497" height="387" role="presentation"/><noscript><img alt="" class="ep fd ez he v" src="https://miro.medium.com/max/994/1*Gx-V-eTwCbVF6O6fXdjsPA.png" width="497" height="387" srcSet="https://miro.medium.com/max/552/1*Gx-V-eTwCbVF6O6fXdjsPA.png 276w, https://miro.medium.com/max/994/1*Gx-V-eTwCbVF6O6fXdjsPA.png 497w" sizes="497px" role="presentation"/></noscript></div></div><figcaption class="hp hq fu fs ft hr hs az b ba bb dt oz am pa pb">Negative log likelihood and accuracy training curves</figcaption></figure></div></div></div></div><div class="n p"><div class="ao ap aq ar as hu au v"><p id="edc7" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">As expected, the training loss only continues to decrease with further training. The validation loss, on the other hand, reaches a minimum and plateaus. At a certain epoch, there is no return (or even a negative return) to further training. Our model will only start to memorize the training data and will not be able to generalize to testing data.</p><blockquote class="ld le lf"><p id="5683" class="kg kh lg ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Without early stopping, our model will train for longer than necessary <em class="hx">and </em>will overfit to the training data.</p></blockquote><p id="2f11" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Another point we can see from the training curves is that our model is not overfitting greatly. There is some overfitting as is always be the case, but the <a href="https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5" class="eh ht" rel="noopener">dropout</a> after the first trainable fully connected layer prevents the training and validation losses from diverging too much.</p></div></div></section><div class="n p cv lo lp lq" role="separator"><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt"></span></div><section class="gf gg gh dq gi"><div class="n p"><div class="ao ap aq ar as hu au v"><h1 id="ad96" class="lv lw hx az lx ly lz kj ma mb mc kl md me mf mg mh mi mj mk ml mm mn mo mp mq iu">Making Predictions: Inference</h1><p id="0de8" class="kg kh hx ki b iw mr kj kk iz ms kl km kn mt ko kp kq mu kr ks kt mv ku kv kx gf iu">In the notebook I take care of some boring — but necessary — details of saving and loading PyTorch models, but here we’ll move right to the best part: making predictions on new images. We know our model does well on training and even validation data, but the ultimate test is how it performs on a hold-out testing set it has not seen before. We saved 25% of the data for the purpose of determining if our model can generalize to new data.</p><p id="ea77" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Predicting with a trained model is pretty simple. We use the same syntax as for training and validation:</p><pre class="li lj lk ll lm np nq nr"><span id="6b0a" class="iu ng lw hx ns b dj nt nu s nv">for data, targets in testloader:<br/>    log_ps = model(data)<br/>    # Convert to probabilities<br/>    ps = torch.exp(log_ps)</span><span id="eabe" class="iu ng lw hx ns b dj ol om on oo op nu s nv">ps.shape()</span><span id="f9bd" class="iu ng lw hx ns b dj ol om on oo op nu s nv"><strong class="ns hy">(128, 100)</strong></span></pre><p id="284b" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">The shape of our probabilities are ( <code class="hj ob oc od ns b">batch_size</code> , <code class="hj ob oc od ns b">n_classes</code> ) because we have a probability for every class. We can find the accuracy by finding the highest probability for each example and compare these to the labels:</p><pre class="li lj lk ll lm np nq nr"><span id="0add" class="iu ng lw hx ns b dj nt nu s nv"># Find predictions and correct<br/>pred = torch.max(ps, dim=1)<br/>equals = pred == targets</span><span id="ffbd" class="iu ng lw hx ns b dj ol om on oo op nu s nv"># Calculate accuracy<br/>accuracy = torch.mean(equals)</span></pre><p id="b3c4" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">When <a href="https://www.coursera.org/lecture/machine-learning/model-selection-and-train-validation-test-sets-QGKbr" class="eh ht" rel="noopener nofollow">diagnosing a network</a> used for object recognition, it can be helpful to look at both overall performance on the test set and individual predictions.</p><h2 id="c781" class="ng lw hx az lx nh ni iy ma nj nk jb md jc nl je mh jf nm jh ml ji nn jk mp no iu">Model Results</h2><p id="d3ba" class="kg kh hx ki b iw mr kj kk iz ms kl km kn mt ko kp kq mu kr ks kt mv ku kv kx gf iu">Here are two predictions the model nails:</p></div></div><div class="gj"><div class="n p"><div class="gk gl gm gn go gp ar gq as gr au v"><figure class="li lj lk ll lm gj gx gy paragraph-image"><div role="button" tabindex="0" class="gz ha am hb v hc"><div class="fs ft at"><div class="hi s am hj"><div class="pc hl s"><div class="fa hd ep fd ez he v hf hg hh"><img alt="" class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/1*RWI7Ct5JSUZz5gkFyD6Ntg.png?q=20" width="1034" height="352" role="presentation"/></div><img alt="" class="fa hd ep fd ez he v c" width="1034" height="352" role="presentation"/><noscript><img alt="" class="ep fd ez he v" src="https://miro.medium.com/max/2068/1*RWI7Ct5JSUZz5gkFyD6Ntg.png" width="1034" height="352" srcSet="https://miro.medium.com/max/552/1*RWI7Ct5JSUZz5gkFyD6Ntg.png 276w, https://miro.medium.com/max/1104/1*RWI7Ct5JSUZz5gkFyD6Ntg.png 552w, https://miro.medium.com/max/1280/1*RWI7Ct5JSUZz5gkFyD6Ntg.png 640w, https://miro.medium.com/max/1456/1*RWI7Ct5JSUZz5gkFyD6Ntg.png 728w, https://miro.medium.com/max/1632/1*RWI7Ct5JSUZz5gkFyD6Ntg.png 816w, https://miro.medium.com/max/1808/1*RWI7Ct5JSUZz5gkFyD6Ntg.png 904w, https://miro.medium.com/max/1984/1*RWI7Ct5JSUZz5gkFyD6Ntg.png 992w, https://miro.medium.com/max/2000/1*RWI7Ct5JSUZz5gkFyD6Ntg.png 1000w" sizes="1000px" role="presentation"/></noscript></div></div></div></div></figure><figure class="os gj gx gy paragraph-image"><div role="button" tabindex="0" class="gz ha am hb v hc"><div class="fs ft at"><div class="hi s am hj"><div class="pd hl s"><div class="fa hd ep fd ez he v hf hg hh"><img alt="" class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/1*QWozitnzhJz4wFFrQsAVZw.png?q=20" width="1035" height="352" role="presentation"/></div><img alt="" class="fa hd ep fd ez he v c" width="1035" height="352" role="presentation"/><noscript><img alt="" class="ep fd ez he v" src="https://miro.medium.com/max/2070/1*QWozitnzhJz4wFFrQsAVZw.png" width="1035" height="352" srcSet="https://miro.medium.com/max/552/1*QWozitnzhJz4wFFrQsAVZw.png 276w, https://miro.medium.com/max/1104/1*QWozitnzhJz4wFFrQsAVZw.png 552w, https://miro.medium.com/max/1280/1*QWozitnzhJz4wFFrQsAVZw.png 640w, https://miro.medium.com/max/1456/1*QWozitnzhJz4wFFrQsAVZw.png 728w, https://miro.medium.com/max/1632/1*QWozitnzhJz4wFFrQsAVZw.png 816w, https://miro.medium.com/max/1808/1*QWozitnzhJz4wFFrQsAVZw.png 904w, https://miro.medium.com/max/1984/1*QWozitnzhJz4wFFrQsAVZw.png 992w, https://miro.medium.com/max/2000/1*QWozitnzhJz4wFFrQsAVZw.png 1000w" sizes="1000px" role="presentation"/></noscript></div></div></div></div></figure></div></div></div><div class="n p"><div class="ao ap aq ar as hu au v"><p id="0c12" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">These are pretty easy, so I’m glad the model has no trouble!</p></div></div></section><div class="n p cv lo lp lq" role="separator"><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt"></span></div><section class="gf gg gh dq gi"><div class="n p"><div class="ao ap aq ar as hu au v"><p id="3b48" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">We don’t just want to focus on the correct predictions and we’ll take a look at some wrong outputs shortly. For now let’s evaluate the performance on the entire test set. For this, we want to iterate over the test <code class="hj ob oc od ns b">DataLoader</code> and calculate the loss and accuracy for every example.</p><p id="fdc3" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Convolutional neural networks for object recognition are generally measured in terms of <a href="https://stats.stackexchange.com/questions/95391/what-is-the-definition-of-top-n-accuracy" class="eh ht" rel="noopener nofollow">topk accuracy</a>. This refers to the whether or not the real class was in the k most likely predicted classes. For example, top 5 accuracy is the % the right class was in the 5 highest probability predictions. You can get the topk most likely probabilities and classes from a PyTorch tensor as follows:</p><pre class="li lj lk ll lm np nq nr"><span id="91fd" class="iu ng lw hx ns b dj nt nu s nv">top_5_ps, top_5_classes = ps.topk(5, dim=1)<br/>top_5_ps.shape</span><span id="24de" class="iu ng lw hx ns b dj ol om on oo op nu s nv"><strong class="ns hy">(128, 5)</strong></span></pre><p id="3bd9" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Evaluating the model on the entire testing set, we calculate the metrics:</p><pre class="li lj lk ll lm np nq nr"><span id="b108" class="iu ng lw hx ns b dj nt nu s nv"><strong class="ns hy">Final test top 1 weighted accuracy = 88.65%<br/>Final test top 5 weighted accuracy = 98.00%<br/>Final test cross entropy per image = 0.3772.</strong></span></pre><p id="9dc8" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">These compare favorably to the near 90% top 1 accuracy on the validation data. <strong class="ki hy">Overall, we conclude our pre-trained model was able to successfully transfer its knowledge from Imagenet to our smaller dataset.</strong></p></div></div></section><div class="n p cv lo lp lq" role="separator"><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt"></span></div><section class="gf gg gh dq gi"><div class="n p"><div class="ao ap aq ar as hu au v"><h2 id="e528" class="ng lw hx az lx nh ni iy ma nj nk jb md jc nl je mh jf nm jh ml ji nn jk mp no iu">Model Investigation</h2><p id="ed16" class="kg kh hx ki b iw mr kj kk iz ms kl km kn mt ko kp kq mu kr ks kt mv ku kv kx gf iu">Although the model does well, there’s likely steps to take which can make it even better. Often, the best way to figure out how to improve a model is to investigate its errors (note: this is also an effective self-improvement method.)</p><p id="148f" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Our model isn’t great at identifying crocodiles, so let’s look at some test predictions from this category:</p></div></div><div class="gj"><div class="n p"><div class="gk gl gm gn go gp ar gq as gr au v"><figure class="li lj lk ll lm gj gx gy paragraph-image"><div role="button" tabindex="0" class="gz ha am hb v hc"><div class="fs ft at"><div class="hi s am hj"><div class="pe hl s"><div class="fa hd ep fd ez he v hf hg hh"><img alt="" class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/1*0Iei4WIP-_MzAiPDKRL79g.png?q=20" width="1039" height="352" role="presentation"/></div><img alt="" class="fa hd ep fd ez he v c" width="1039" height="352" role="presentation"/><noscript><img alt="" class="ep fd ez he v" src="https://miro.medium.com/max/2078/1*0Iei4WIP-_MzAiPDKRL79g.png" width="1039" height="352" srcSet="https://miro.medium.com/max/552/1*0Iei4WIP-_MzAiPDKRL79g.png 276w, https://miro.medium.com/max/1104/1*0Iei4WIP-_MzAiPDKRL79g.png 552w, https://miro.medium.com/max/1280/1*0Iei4WIP-_MzAiPDKRL79g.png 640w, https://miro.medium.com/max/1456/1*0Iei4WIP-_MzAiPDKRL79g.png 728w, https://miro.medium.com/max/1632/1*0Iei4WIP-_MzAiPDKRL79g.png 816w, https://miro.medium.com/max/1808/1*0Iei4WIP-_MzAiPDKRL79g.png 904w, https://miro.medium.com/max/1984/1*0Iei4WIP-_MzAiPDKRL79g.png 992w, https://miro.medium.com/max/2000/1*0Iei4WIP-_MzAiPDKRL79g.png 1000w" sizes="1000px" role="presentation"/></noscript></div></div></div></div></figure><figure class="os gj gx gy paragraph-image"><div role="button" tabindex="0" class="gz ha am hb v hc"><div class="fs ft at"><div class="hi s am hj"><div class="pf hl s"><div class="fa hd ep fd ez he v hf hg hh"><img alt="" class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/1*kEmyc_lZ0TWYOgvgLlUsqw.png?q=20" width="1037" height="352" role="presentation"/></div><img alt="" class="fa hd ep fd ez he v c" width="1037" height="352" role="presentation"/><noscript><img alt="" class="ep fd ez he v" src="https://miro.medium.com/max/2074/1*kEmyc_lZ0TWYOgvgLlUsqw.png" width="1037" height="352" srcSet="https://miro.medium.com/max/552/1*kEmyc_lZ0TWYOgvgLlUsqw.png 276w, https://miro.medium.com/max/1104/1*kEmyc_lZ0TWYOgvgLlUsqw.png 552w, https://miro.medium.com/max/1280/1*kEmyc_lZ0TWYOgvgLlUsqw.png 640w, https://miro.medium.com/max/1456/1*kEmyc_lZ0TWYOgvgLlUsqw.png 728w, https://miro.medium.com/max/1632/1*kEmyc_lZ0TWYOgvgLlUsqw.png 816w, https://miro.medium.com/max/1808/1*kEmyc_lZ0TWYOgvgLlUsqw.png 904w, https://miro.medium.com/max/1984/1*kEmyc_lZ0TWYOgvgLlUsqw.png 992w, https://miro.medium.com/max/2000/1*kEmyc_lZ0TWYOgvgLlUsqw.png 1000w" sizes="1000px" role="presentation"/></noscript></div></div></div></div></figure><figure class="os gj gx gy paragraph-image"><div role="button" tabindex="0" class="gz ha am hb v hc"><div class="fs ft at"><div class="hi s am hj"><div class="pe hl s"><div class="fa hd ep fd ez he v hf hg hh"><img alt="" class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/1*CSBH0Ow44MBMmSo-F4tKDA.png?q=20" width="1039" height="352" role="presentation"/></div><img alt="" class="fa hd ep fd ez he v c" width="1039" height="352" role="presentation"/><noscript><img alt="" class="ep fd ez he v" src="https://miro.medium.com/max/2078/1*CSBH0Ow44MBMmSo-F4tKDA.png" width="1039" height="352" srcSet="https://miro.medium.com/max/552/1*CSBH0Ow44MBMmSo-F4tKDA.png 276w, https://miro.medium.com/max/1104/1*CSBH0Ow44MBMmSo-F4tKDA.png 552w, https://miro.medium.com/max/1280/1*CSBH0Ow44MBMmSo-F4tKDA.png 640w, https://miro.medium.com/max/1456/1*CSBH0Ow44MBMmSo-F4tKDA.png 728w, https://miro.medium.com/max/1632/1*CSBH0Ow44MBMmSo-F4tKDA.png 816w, https://miro.medium.com/max/1808/1*CSBH0Ow44MBMmSo-F4tKDA.png 904w, https://miro.medium.com/max/1984/1*CSBH0Ow44MBMmSo-F4tKDA.png 992w, https://miro.medium.com/max/2000/1*CSBH0Ow44MBMmSo-F4tKDA.png 1000w" sizes="1000px" role="presentation"/></noscript></div></div></div></div></figure></div></div></div><div class="n p"><div class="ao ap aq ar as hu au v"><p id="aa33" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Given the subtle distinction between <code class="hj ob oc od ns b">crocodile</code> and <code class="hj ob oc od ns b">crocodile_head</code> , and the difficulty of the second image, I’d say our model is not entirely unreasonable in these predictions. The ultimate goal in image recognition is to exceed human capabilities, and our model is nearly there!</p><p id="ec98" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Finally, we’d expect the model to perform better on categories with more images, so we can look at a graph of accuracy in a given category versus the number of training images in that category:</p><figure class="li lj lk ll lm gj fs ft paragraph-image"><div role="button" tabindex="0" class="gz ha am hb v hc"><div class="fs ft pg"><div class="hi s am hj"><div class="ph hl s"><div class="fa hd ep fd ez he v hf hg hh"><img alt="" class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/58/1*fd6oQi9p4adNAg36aOs1Ow.png?q=20" width="424" height="436" role="presentation"/></div><img alt="" class="fa hd ep fd ez he v c" width="424" height="436" role="presentation"/><noscript><img alt="" class="ep fd ez he v" src="https://miro.medium.com/max/848/1*fd6oQi9p4adNAg36aOs1Ow.png" width="424" height="436" srcSet="https://miro.medium.com/max/552/1*fd6oQi9p4adNAg36aOs1Ow.png 276w, https://miro.medium.com/max/848/1*fd6oQi9p4adNAg36aOs1Ow.png 424w" sizes="424px" role="presentation"/></noscript></div></div></div></div></figure><p id="2cd6" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">There does appear to be a positive correlation between the number of training images and the top 1 test accuracy. This indicates that <em class="lg">more training data augmentation could be helpfu</em>l, or, even that we should use <a href="https://forums.fast.ai/t/change-to-how-tta-works/8474/3" class="eh ht" rel="noopener nofollow">test time augmentation</a>. We could also try a different pre-trained model, or build another custom classifier. At the moment, deep learning is still an empirical field meaning experimentation is often required!</p></div></div></section><div class="n p cv lo lp lq" role="separator"><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt"></span></div><section class="gf gg gh dq gi"><div class="n p"><div class="ao ap aq ar as hu au v"><h1 id="eefa" class="lv lw hx az lx ly lz kj ma mb mc kl md me mf mg mh mi mj mk ml mm mn mo mp mq iu">Conclusions</h1><p id="00c8" class="kg kh hx ki b iw mr kj kk iz ms kl km kn mt ko kp kq mu kr ks kt mv ku kv kx gf iu">While there are easier deep learning libraries to use, the <a href="http://www.goldsborough.me/ml/ai/python/2018/02/04/20-17-20-a_promenade_of_pytorch/" class="eh ht" rel="noopener nofollow">benefits of PyTorch</a> are <a href="https://www.netguru.co/blog/deep-learning-frameworks-comparison" class="eh ht" rel="noopener nofollow">speed</a>, control over every aspect of model architecture / training, efficient implementation of backpropagation with <a class="eh ht" rel="noopener" href="/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec">tensor auto differentiation</a>, and ease of debugging code due to the <a href="https://medium.com/intuitionmachine/pytorch-dynamic-computational-graphs-and-modular-deep-learning-7e7f89f18d1" class="eh ht" rel="noopener">dynamic nature of PyTorch graphs</a>. For production code or your own projects, I’m not sure there is<strong class="ki hy"> yet</strong> a compelling argument for using PyTorch instead of a library with a gentler learning curve such as <a href="http://keras.io" class="eh ht" rel="noopener nofollow">Keras</a>, but it’s helpful to know how to use different options.</p><p id="6f76" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">Through this project, we were able to see the basics of using PyTorch as well as the concept of <em class="lg">transfer learning</em>, an effective method for object recognition. Instead of training a model from scratch, we can use existing architectures that have been trained on a large dataset and then tune them for our task. This reduces the time to train and often results in better overall performance. The outcome of this project is some knowledge of transfer learning and PyTorch that we can build on to build more complex applications.</p><p id="ffdd" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">We truly live in an incredible age for deep learning, where anyone can build deep learning models with easily <a href="https://colab.research.google.com" class="eh ht" rel="noopener nofollow">available resources</a>! Now get out there and take advantage of these resources by building your own project.</p></div></div></section><div class="n p cv lo lp lq" role="separator"><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt lu"></span><span class="lr jp bu ls lt"></span></div><section class="gf gg gh dq gi"><div class="n p"><div class="ao ap aq ar as hu au v"><p id="c5ca" class="kg kh hx ki b iw ky kj kk iz kz kl km kn la ko kp kq lb kr ks kt lc ku kv kx gf iu">As always, I welcome feedback and constructive criticism. I can be reached on Twitter <a href="http://twitter.com/@koehrsen_will" class="eh ht" rel="noopener nofollow">@koehrsen_will </a>or through my personal website <a href="https://willk.online" class="eh ht" rel="noopener nofollow">willk.online</a>.</p></div></div></section></div></article><div class="fa ge fb pp v pq fd pn pr" data-test-id="post-sidebar"><div class="n p"><div class="ao ap aq ar as at au v"><div class="ps n ah"><div class="ge"><div><div class="pt pu s"><div class="gx s"><a href="https://williamkoehrsen.medium.com/?source=post_sidebar--------------------------post_sidebar-----------" class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" rel="noopener"><h2 class="az lx dj bb hw iu gf">Will Koehrsen</h2></a></div><div class="pv s"><p class="az b ba bb dt">Data Scientist at Cortex Intel, Data Science Communicator</p></div><div class="pw s"><span><a href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9%2Fdd09190245ce&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&amp;source=post_sidebar-e2f299e30cb9-------------------------follow_card-----------" class="az b ba bb dx bd dy dz ea eb ec bk bl bm ed ee bq br bs bt bu bv" rel="noopener">Follow</a></span></div></div><div class="px py pz n"><div class="n o"><div class="s am qa qb qc qd qe"><span><a href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd09190245ce&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&amp;source=post_sidebar-----dd09190245ce---------------------clap_sidebar-----------" class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" rel="noopener"><div class="cd qf qg qh eu qi qj qk r ql qm"><svg width="29" height="29" aria-label="clap"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></span></div><div class="s qn qo qp qq qr qs qt"><div class="qu"><p class="az b ba bb dt"><button class="eh ei by bz ca cb cc cd ce bk ej ek cf el em">1.7K<!-- --> </button></p></div></div></div></div><div class="py s"><button class="eu qg cd"><div class="qw n o aw"><svg width="25" height="25" class="r qv eu qm" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg><div class="s am qx qy qz ra rb rc rd re"><p class="az b ba bb dt">14<!-- --> </p></div></div></button></div><div class="kf"><span><a href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd09190245ce&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&amp;source=post_sidebar-----dd09190245ce---------------------bookmark_sidebar-----------" class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></span></div></div></div></div></div></div></div><div class="fa ge pi fb pj pk pl pm pn po"></div><div><div class="rf gj n ah p"><div class="n p"><div class="ao ap aq ar as hu au v"><div class="n cs"></div><div class="n o cs"></div><div class="rg rh ri rj rk rl"><div class="rm s"><h2 class="az lx nh iy ma nj jb md jc je mh jf jh ml ji jk mp iu">Sign up for The Variable</h2></div><div class="rn s"><h3 class="az b ro bb iu">By Towards Data Science</h3></div><div class="lq rp s"><p class="az b rq rr rs rt ru rv rw rx ry rz iu">Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don&#x27;t want to miss.<!-- --> <a href="https://medium.com/towards-data-science/newsletters/the-variable?source=newsletter_v3_promo--------------------------newsletter_v3_promo-----------" class="eh ei by bz ca cb cc cd ce bk cf el em ht" rel="noopener">Take a look.</a></p></div><div class="n cs"><div class="n cp ah sa"><div id="g-recaptcha"></div><div class="sb"><div class="sc n o cj sd se cm cn"><div class="sf s sg"><div class=""><div class="sy ss n o ah sw"><div class="am"><div class="sz s"><p class="az b dj dk dt"><input type="text" aria-label="email" class="sh si sj sk sl sm sn so sp sq sr ss st su sv sw iu sx" pattern=".*" placeholder="Your email" value=""/></p></div></div></div></div></div><div class="ta tb s jy tc"><div><button class="az b dj dk dx td dy dz ea eb ec bk bl bm ed ee bq br bs bt bu bv"><span class="ke" aria-hidden="true"><svg width="20" height="16" viewBox="0 0 20 16"><path d="M0 .35v15.3h20V.35H0zm6.95 9.38l3.05 2.5 3.05-2.5 4.88 4.73H2.07l4.88-4.73zM1.2 13.64V5.02l4.82 3.94-4.82 4.68zm12.78-4.68l4.82-3.94v8.62l-4.82-4.68zm4.82-7.42v1.94l-8.8 7.2-8.8-7.2V1.54h17.6z"></path></svg></span>Get this newsletter</button></div></div></div><div class="te hp s sg"><p class="az b tf tg iu">By signing up, you will create a <!-- -->Medium<!-- --> account if you don’t already have one. Review our<!-- --> <a href="https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=newsletter_v3_promo--------------------------newsletter_v3_promo-----------" class="eh ei by bz ca cb cc cd ce bk cf el em ht" target="_blank" rel="noopener">Privacy Policy</a> <!-- -->for more information about our privacy practices.</p></div></div><div class="th ti aj"><p class="az b ba bb iu"><b>Check your inbox</b><br/>Medium<!-- --> sent you an email at <!-- --> to complete your subscription.</p></div></div></div></div><div class="tj rf s"><div class="tk n cj kc"><div class="n aw"><div class="tl s"><span class="s tm tn to e d"><div class="n o"><div class="s am qa qb qc qd qe"><span><a href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd09190245ce&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&amp;source=post_actions_footer-----dd09190245ce---------------------clap_footer-----------" class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" rel="noopener"><div class="cd qf qg qh eu qi qj qk r ql qm"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></span></div><div class="s qn qo qp qq qr qs qt"><div class="am tp qu"><p class="az b ba bb iu"><button class="eh ei by bz ca cb cc cd ce bk ej ek cf el em">1.7K<span class="s h g f tq tr"> </span></button><span class="s h g f tq tr"></span></p></div></div></div></span><span class="s h g f tq tr"><div class="n cp"><div class="s am qa qb"><span><a href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd09190245ce&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&amp;source=post_actions_footer-----dd09190245ce---------------------clap_footer-----------" class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" rel="noopener"><div class="cd qf qg qh eu qi qj qk r ql qm"><svg width="33" height="33" viewBox="0 0 33 33" aria-label="clap"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></div></a></span></div><div class="s qn qo qp qq ts tt tu tv tw tx"><div class="am tp qu"><p class="az b ba bb iu"><button class="eh ei by bz ca cb cc cd ce bk ej ek cf el em">1.7K<span class="s h g f tq tr"> </span></button><span class="s h g f tq tr"></span></p></div></div></div></span></div><div class="s ty tz ua ub uc"></div><button class="eu qg cd"><div class="qw n o aw"><span class="ud s h g f tq tr"><svg width="33" height="33" viewBox="0 0 33 33" fill="none" class="r qv eu qm" aria-label="responses"><path fill-rule="evenodd" clip-rule="evenodd" d="M24.28 25.5l.32-.29c2.11-1.94 3.4-4.61 3.4-7.56C28 11.83 22.92 7 16.5 7S5 11.83 5 17.65s5.08 10.66 11.5 10.66c1.22 0 2.4-.18 3.5-.5l.5-.15.41.33a8.86 8.86 0 0 0 4.68 2.1 7.34 7.34 0 0 1-1.3-4.15v-.43zm1 .45c0 1.5.46 2.62 1.69 4.44.22.32.01.75-.38.75a9.69 9.69 0 0 1-6.31-2.37c-1.2.35-2.46.54-3.78.54C9.6 29.3 4 24.09 4 17.65 4 11.22 9.6 6 16.5 6S29 11.22 29 17.65c0 3.25-1.42 6.18-3.72 8.3z"></path></svg></span><span class="ue s tm tn to e d"><svg width="25" height="25" class="r qv eu qm" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg></span><div class="s am uf qy ug ra uh rc ui uj uk ul"><p class="az b ba bb dt">14<!-- --> </p></div></div></button></div><div class="n o"><div class="kd s"><div class="bu" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bu" role="tooltip" aria-hidden="false"><button class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div></div></div><div class="kd s da"><div class="kf"><span><a href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd09190245ce&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&amp;source=post_actions_footer--------------------------bookmark_footer-----------" class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></span></div></div></div></div></div><div class="um tk s"><ul class="cd ce"><li class="bu un ke uo"><a href="/tagged/machine-learning" class="az b ro up dt uq ur bv s nq">Machine Learning</a></li><li class="bu un ke uo"><a href="/tagged/data-science" class="az b ro up dt uq ur bv s nq">Data Science</a></li><li class="bu un ke uo"><a href="/tagged/deep-learning" class="az b ro up dt uq ur bv s nq">Deep Learning</a></li><li class="bu un ke uo"><a href="/tagged/pytorch" class="az b ro up dt uq ur bv s nq">Pytorch</a></li><li class="bu un ke uo"><a href="/tagged/education" class="az b ro up dt uq ur bv s nq">Education</a></li></ul></div></div></div><div><div class="n p"><div class="ao ap aq ar as hu au v"></div></div><div class="s kc"><div class="us ut s rk"><div class="n p"><div class="ao ap aq ar as hu au v"><div class="n o cj"><h2 class="az lx uu rr uv ma uw rt ux md uy rv uz mh va rx vb ml vc rz vd mp hf ve vf vg vh vi iu"><a href="https://towardsdatascience.com/?source=follow_footer-------------------------------------" class="eh ei by bz ca cb cc cd ce bk ej ek cf el em" rel="noopener">More from Towards Data Science</a></h2><div class="bu" aria-hidden="false" aria-describedby="collectionFollowPopover" aria-labelledby="collectionFollowPopover"><span><a href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdd09190245ce&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce&amp;source=follow_footer--------------------------follow_footer-----------" class="az b ba bb dx bd dy dz ea eb ec bk bl bm ed ee bq br bs bt bu bv" rel="noopener"><div class="n aw">Follow</div></a></span></div></div><div class="gy vj s"><p class="az b ba bb dt">Your home for data science. A Medium publication sharing concepts, ideas and codes.</p></div></div></div></div></div><div class="vk s rk kc"><div class="n p"><div class="gk gm go vl vm vn au v"><div class="vo rf s"><div class="vn s hq"><a href="https://towardsdatascience.com/?source=follow_footer-------------------------------------" class="az b ba bb dx bd dy dz ea eb ec bk bl bm ed ee bq br bs bt bu bv" rel="noopener">Read more from <!-- -->Towards Data Science</a></div></div></div></div></div><div class="s fv kc"><div class="n p"><div class="ao ap aq ar as at au v"><div class="vp kw s"><div class="vq pu vr kw s vs vt"><h2 class="az lx nh iy ma nj jb md jc je mh jf jh ml ji jk mp iu">More From Medium</h2></div><div class="cp n aw cs vu vv vw vx vy vz wa wb wc wd we wf wg wh wi"><div class="wj wk wl gl wm wn wo gn wp wq wr ws wt wu wv ww wx wy wz xa xb"><div class="xc xd s"><div class="v he"><div class="n cj"><div class="s xe xf xg xh"><div class="xi s"><h2 class="az lx uu rr ma uw rt md ru xj mh rw xk ml ry xl mp iu"><a rel="noopener" href="/a-complete-yet-simple-guide-to-move-from-excel-to-python-d664e5683039?source=post_internal_links---------0----------------------------">A Complete Yet Simple Guide to Move From Excel to Python</a></h2></div><div class="o n"><div></div><div class="v s"><div class="n"><div style="flex:1"><span class="az b ba bb iu"><div class="cq n o xm"><span class="az b ro bb iu"><a href="https://frank-andrade.medium.com/?source=post_internal_links---------0----------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf el em" rel="noopener">Frank Andrade</a><span> <!-- -->in<!-- --> <a href="https://towardsdatascience.com/?source=post_internal_links---------0----------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf el em" rel="noopener">Towards Data Science</a></span></span></div></span></div></div></div></div></div><div class="en ke s xo xp"><a class="eh ei by bz ca cb cc cd ce bk ej ek cf el em s" rel="noopener" href="/a-complete-yet-simple-guide-to-move-from-excel-to-python-d664e5683039?source=post_internal_links---------0----------------------------"><div class="hi s am hj"><div class="xq hl s"><div class="fa hd ep fd ez he v hf hg hh"><img class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/1*xmRGG86D5OMh1crRfOcExA.png?q=20" width="70" height="70" role="presentation"/></div><img class="fa hd xr xs xt xu xv xw xx xy xz ya c" width="70" height="70" role="presentation"/><noscript><img class="xr xs xt xu xv xw xx xy xz ya" src="https://miro.medium.com/fit/c/140/140/1*xmRGG86D5OMh1crRfOcExA.png" width="70" height="70" srcSet="https://miro.medium.com/fit/c/96/140/1*xmRGG86D5OMh1crRfOcExA.png 48w, https://miro.medium.com/fit/c/140/140/1*xmRGG86D5OMh1crRfOcExA.png 70w" sizes="70px" role="presentation"/></noscript></div></div></a></div></div></div></div></div><div class="wj wk wl gl wm wn wo gn wp wq wr ws wt wu wv ww wx wy wz xa xb"><div class="xc xd s"><div class="v he"><div class="n cj"><div class="s xe xf xg xh"><div class="xi s"><h2 class="az lx uu rr ma uw rt md ru xj mh rw xk ml ry xl mp iu"><a rel="noopener" href="/five-things-i-have-learned-after-solving-500-leetcode-questions-b794c152f7a1?source=post_internal_links---------1----------------------------">Five things I have learned after solving 500+ Leetcode questions</a></h2></div><div class="o n"><div></div><div class="v s"><div class="n"><div style="flex:1"><span class="az b ba bb iu"><div class="cq n o xm"><span class="az b ro bb iu"><a href="https://medium.com/@federicomannucci_31459?source=post_internal_links---------1----------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf el em" rel="noopener">Federico Mannucci</a><span> <!-- -->in<!-- --> <a href="https://towardsdatascience.com/?source=post_internal_links---------1----------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf el em" rel="noopener">Towards Data Science</a></span></span></div></span></div></div></div></div></div><div class="en ke s xo xp"><a class="eh ei by bz ca cb cc cd ce bk ej ek cf el em s" rel="noopener" href="/five-things-i-have-learned-after-solving-500-leetcode-questions-b794c152f7a1?source=post_internal_links---------1----------------------------"><div class="hi s am hj"><div class="xq hl s"><div class="fa hd ep fd ez he v hf hg hh"><img class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/0*hySI975thrTTxhUW?q=20" width="70" height="70" role="presentation"/></div><img class="fa hd xr xs xt xu xv xw xx xy xz ya c" width="70" height="70" role="presentation"/><noscript><img class="xr xs xt xu xv xw xx xy xz ya" src="https://miro.medium.com/fit/c/140/140/0*hySI975thrTTxhUW" width="70" height="70" srcSet="https://miro.medium.com/fit/c/96/140/0*hySI975thrTTxhUW 48w, https://miro.medium.com/fit/c/140/140/0*hySI975thrTTxhUW 70w" sizes="70px" role="presentation"/></noscript></div></div></a></div></div></div></div></div><div class="wj wk wl gl wm wn wo gn wp wq wr ws wt wu wv ww wx wy wz xa xb"><div class="xc xd s"><div class="v he"><div class="n cj"><div class="s xe xf xg xh"><div class="xi s"><h2 class="az lx uu rr ma uw rt md ru xj mh rw xk ml ry xl mp iu"><a rel="noopener" href="/how-to-create-mathematical-animations-like-3blue1brown-using-python-f571fb9da3d1?source=post_internal_links---------2----------------------------">How to Create Mathematical Animations like 3Blue1Brown Using Python</a></h2></div><div class="o n"><div></div><div class="v s"><div class="n"><div style="flex:1"><span class="az b ba bb iu"><div class="cq n o xm"><span class="az b ro bb iu"><a href="https://khuyentran1476.medium.com/?source=post_internal_links---------2----------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf el em" rel="noopener">Khuyen Tran</a><span> <!-- -->in<!-- --> <a href="https://towardsdatascience.com/?source=post_internal_links---------2----------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf el em" rel="noopener">Towards Data Science</a></span></span></div></span></div></div></div></div></div><div class="en ke s xo xp"><a class="eh ei by bz ca cb cc cd ce bk ej ek cf el em s" rel="noopener" href="/how-to-create-mathematical-animations-like-3blue1brown-using-python-f571fb9da3d1?source=post_internal_links---------2----------------------------"><div class="hi s am hj"><div class="xq hl s"><div class="fa hd ep fd ez he v hf hg hh"><img class="ep fd ez he v hm hn ho" src="https://miro.medium.com/freeze/max/60/1*JaSz_Xt3eg_FOZ8iRmSJBg.gif?q=20" width="70" height="70" role="presentation"/></div><img class="fa hd xr xs xt xu xv xw xx xy xz ya c" width="70" height="70" role="presentation"/><noscript><img class="xr xs xt xu xv xw xx xy xz ya" src="https://miro.medium.com/fit/c/140/140/1*JaSz_Xt3eg_FOZ8iRmSJBg.gif" width="70" height="70" srcSet="https://miro.medium.com/fit/c/96/140/1*JaSz_Xt3eg_FOZ8iRmSJBg.gif 48w, https://miro.medium.com/fit/c/140/140/1*JaSz_Xt3eg_FOZ8iRmSJBg.gif 70w" sizes="70px" role="presentation"/></noscript></div></div></a></div></div></div></div></div><div class="wj wk wl gl wm wn wo gn wp wq wr ws wt wu wv ww wx wy wz xa xb"><div class="xc xd s"><div class="v he"><div class="n cj"><div class="s xe xf xg xh"><div class="xi s"><h2 class="az lx uu rr ma uw rt md ru xj mh rw xk ml ry xl mp iu"><a rel="noopener" href="/why-i-stopped-applying-for-data-science-jobs-7b93b3b8153b?source=post_internal_links---------3----------------------------">Why I Stopped Applying For Data Science Jobs</a></h2></div><div class="o n"><div></div><div class="v s"><div class="n"><div style="flex:1"><span class="az b ba bb iu"><div class="cq n o xm"><span class="az b ro bb iu"><a href="https://kurtispykes.medium.com/?source=post_internal_links---------3----------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf el em" rel="noopener">Kurtis Pykes</a><span> <!-- -->in<!-- --> <a href="https://towardsdatascience.com/?source=post_internal_links---------3----------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf el em" rel="noopener">Towards Data Science</a></span></span></div></span></div></div></div></div></div><div class="en ke s xo xp"><a class="eh ei by bz ca cb cc cd ce bk ej ek cf el em s" rel="noopener" href="/why-i-stopped-applying-for-data-science-jobs-7b93b3b8153b?source=post_internal_links---------3----------------------------"><div class="hi s am hj"><div class="xq hl s"><div class="fa hd ep fd ez he v hf hg hh"><img class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/0*PEP5fb8Hx3ir8NOF?q=20" width="70" height="70" role="presentation"/></div><img class="fa hd xr xs xt xu xv xw xx xy xz ya c" width="70" height="70" role="presentation"/><noscript><img class="xr xs xt xu xv xw xx xy xz ya" src="https://miro.medium.com/fit/c/140/140/0*PEP5fb8Hx3ir8NOF" width="70" height="70" srcSet="https://miro.medium.com/fit/c/96/140/0*PEP5fb8Hx3ir8NOF 48w, https://miro.medium.com/fit/c/140/140/0*PEP5fb8Hx3ir8NOF 70w" sizes="70px" role="presentation"/></noscript></div></div></a></div></div></div></div></div><div class="wj wk wl gl wm wn wo gn wp wq wr ws wt wu wv ww wx wy wz xa xb"><div class="xc xd s"><div class="v he"><div class="n cj"><div class="s xe xf xg xh"><div class="xi s"><h2 class="az lx uu rr ma uw rt md ru xj mh rw xk ml ry xl mp iu"><a rel="noopener" href="/hidden-gems-of-python-76020b14e42f?source=post_internal_links---------4----------------------------">Hidden Gems of Python</a></h2></div><div class="o n"><div></div><div class="v s"><div class="n"><div style="flex:1"><span class="az b ba bb iu"><div class="cq n o xm"><span class="az b ro bb iu"><a href="https://charudattamanwatkar.medium.com/?source=post_internal_links---------4----------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf el em" rel="noopener">Charudatta Manwatkar</a><span> <!-- -->in<!-- --> <a href="https://towardsdatascience.com/?source=post_internal_links---------4----------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf el em" rel="noopener">Towards Data Science</a></span></span></div></span></div></div></div></div></div><div class="en ke s xo xp"><a class="eh ei by bz ca cb cc cd ce bk ej ek cf el em s" rel="noopener" href="/hidden-gems-of-python-76020b14e42f?source=post_internal_links---------4----------------------------"><div class="hi s am hj"><div class="xq hl s"><div class="fa hd ep fd ez he v hf hg hh"><img class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/0*EGGjZrrAvQ9400GZ?q=20" width="70" height="70" role="presentation"/></div><img class="fa hd xr xs xt xu xv xw xx xy xz ya c" width="70" height="70" role="presentation"/><noscript><img class="xr xs xt xu xv xw xx xy xz ya" src="https://miro.medium.com/fit/c/140/140/0*EGGjZrrAvQ9400GZ" width="70" height="70" srcSet="https://miro.medium.com/fit/c/96/140/0*EGGjZrrAvQ9400GZ 48w, https://miro.medium.com/fit/c/140/140/0*EGGjZrrAvQ9400GZ 70w" sizes="70px" role="presentation"/></noscript></div></div></a></div></div></div></div></div><div class="wj wk wl gl wm wn wo gn wp wq wr ws wt wu wv ww wx wy wz xa xb"><div class="xc xd s"><div class="v he"><div class="n cj"><div class="s xe xf xg xh"><div class="xi s"><h2 class="az lx uu rr ma uw rt md ru xj mh rw xk ml ry xl mp iu"><a rel="noopener" href="/5-reasons-why-i-left-the-ai-industry-2c88ea183cdd?source=post_internal_links---------5----------------------------">5 Reasons Why I Left the AI Industry</a></h2></div><div class="o n"><div></div><div class="v s"><div class="n"><div style="flex:1"><span class="az b ba bb iu"><div class="cq n o xm"><span class="az b ro bb iu"><a href="https://albertoromgar.medium.com/?source=post_internal_links---------5----------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf el em" rel="noopener">Alberto Romero</a><span> <!-- -->in<!-- --> <a href="https://towardsdatascience.com/?source=post_internal_links---------5----------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf el em" rel="noopener">Towards Data Science</a></span></span></div></span></div></div></div></div></div><div class="en ke s xo xp"><a class="eh ei by bz ca cb cc cd ce bk ej ek cf el em s" rel="noopener" href="/5-reasons-why-i-left-the-ai-industry-2c88ea183cdd?source=post_internal_links---------5----------------------------"><div class="hi s am hj"><div class="xq hl s"><div class="fa hd ep fd ez he v hf hg hh"><img class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/1*s6r29KXtOL-t1Gwv6IUtjw.jpeg?q=20" width="70" height="70" role="presentation"/></div><img class="fa hd xr xs xt xu xv xw xx xy xz ya c" width="70" height="70" role="presentation"/><noscript><img class="xr xs xt xu xv xw xx xy xz ya" src="https://miro.medium.com/fit/c/140/140/1*s6r29KXtOL-t1Gwv6IUtjw.jpeg" width="70" height="70" srcSet="https://miro.medium.com/fit/c/96/140/1*s6r29KXtOL-t1Gwv6IUtjw.jpeg 48w, https://miro.medium.com/fit/c/140/140/1*s6r29KXtOL-t1Gwv6IUtjw.jpeg 70w" sizes="70px" role="presentation"/></noscript></div></div></a></div></div></div></div></div><div class="wj wk wl gl wm wn wo gn wp wq wr ws wt wu wv ww wx wy wz xa xb"><div class="xc xd s"><div class="v he"><div class="n cj"><div class="s xe xf xg xh"><div class="xi s"><h2 class="az lx uu rr ma uw rt md ru xj mh rw xk ml ry xl mp iu"><a rel="noopener" href="/how-microlearning-can-help-you-improve-your-data-science-skills-in-less-than-10-minutes-per-day-6499348228d7?source=post_internal_links---------6----------------------------">How Microlearning Can Help You Improve Your Data Science Skills in Less Than 10 Minutes Per Day</a></h2></div><div class="o n"><div></div><div class="v s"><div class="n"><div style="flex:1"><span class="az b ba bb iu"><div class="cq n o xm"><span class="az b ro bb iu"><a href="https://madison13.medium.com/?source=post_internal_links---------6----------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf el em" rel="noopener">Madison Hunter</a><span> <!-- -->in<!-- --> <a href="https://towardsdatascience.com/?source=post_internal_links---------6----------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf el em" rel="noopener">Towards Data Science</a></span></span></div></span></div></div></div></div></div><div class="en ke s xo xp"><a class="eh ei by bz ca cb cc cd ce bk ej ek cf el em s" rel="noopener" href="/how-microlearning-can-help-you-improve-your-data-science-skills-in-less-than-10-minutes-per-day-6499348228d7?source=post_internal_links---------6----------------------------"><div class="hi s am hj"><div class="xq hl s"><div class="fa hd ep fd ez he v hf hg hh"><img class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/0*SMePvoIcDINwtAqb?q=20" width="70" height="70" role="presentation"/></div><img class="fa hd xr xs xt xu xv xw xx xy xz ya c" width="70" height="70" role="presentation"/><noscript><img class="xr xs xt xu xv xw xx xy xz ya" src="https://miro.medium.com/fit/c/140/140/0*SMePvoIcDINwtAqb" width="70" height="70" srcSet="https://miro.medium.com/fit/c/96/140/0*SMePvoIcDINwtAqb 48w, https://miro.medium.com/fit/c/140/140/0*SMePvoIcDINwtAqb 70w" sizes="70px" role="presentation"/></noscript></div></div></a></div></div></div></div></div><div class="wj wk wl gl wm wn wo gn wp wq wr ws wt wu wv ww wx wy wz xa xb"><div class="xc xd s"><div class="v he"><div class="n cj"><div class="s xe xf xg xh"><div class="xi s"><h2 class="az lx uu rr ma uw rt md ru xj mh rw xk ml ry xl mp iu"><a rel="noopener" href="/4-top-python-ide-for-data-scientist-ccb92d143ca3?source=post_internal_links---------7----------------------------">4 Top Python IDE for Data Scientist</a></h2></div><div class="o n"><div></div><div class="v s"><div class="n"><div style="flex:1"><span class="az b ba bb iu"><div class="cq n o xm"><span class="az b ro bb iu"><a href="https://medium.com/@cornelliusyudhawijaya?source=post_internal_links---------7----------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf el em" rel="noopener">Cornellius Yudha Wijaya</a><span> <!-- -->in<!-- --> <a href="https://towardsdatascience.com/?source=post_internal_links---------7----------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf el em" rel="noopener">Towards Data Science</a></span></span></div></span></div></div></div></div></div><div class="en ke s xo xp"><a class="eh ei by bz ca cb cc cd ce bk ej ek cf el em s" rel="noopener" href="/4-top-python-ide-for-data-scientist-ccb92d143ca3?source=post_internal_links---------7----------------------------"><div class="hi s am hj"><div class="xq hl s"><div class="fa hd ep fd ez he v hf hg hh"><img class="ep fd ez he v hm hn ho" src="https://miro.medium.com/max/60/0*pWa6Aq-dgM3B0Cbb?q=20" width="70" height="70" role="presentation"/></div><img class="fa hd xr xs xt xu xv xw xx xy xz ya c" width="70" height="70" role="presentation"/><noscript><img class="xr xs xt xu xv xw xx xy xz ya" src="https://miro.medium.com/fit/c/140/140/0*pWa6Aq-dgM3B0Cbb" width="70" height="70" srcSet="https://miro.medium.com/fit/c/96/140/0*pWa6Aq-dgM3B0Cbb 48w, https://miro.medium.com/fit/c/140/140/0*pWa6Aq-dgM3B0Cbb 70w" sizes="70px" role="presentation"/></noscript></div></div></a></div></div></div></div></div></div></div></div></div></div></div></div></div><div class="yb s yc yd"><div class="n p"><div class="ao ap aq ar as at au v"><div class="n ah"><div class="n o cj"><a href="https://medium.com/?source=post_page-----dd09190245ce--------------------------------" class="eh ei by bz ca cb cc cd ce bk ye yf cf yg yh" rel="noopener"><svg viewBox="0 0 3940 610" class="dy yi"><path d="M594.79 308.2c0 163.76-131.85 296.52-294.5 296.52S5.8 472 5.8 308.2 137.65 11.69 300.29 11.69s294.5 132.75 294.5 296.51M917.86 308.2c0 154.16-65.93 279.12-147.25 279.12s-147.25-125-147.25-279.12S689.29 29.08 770.61 29.08s147.25 125 147.25 279.12M1050 308.2c0 138.12-23.19 250.08-51.79 250.08s-51.79-112-51.79-250.08 23.19-250.08 51.8-250.08S1050 170.09 1050 308.2M1862.77 37.4l.82-.18v-6.35h-167.48l-155.51 365.5-155.51-365.5h-180.48v6.35l.81.18c30.57 6.9 46.09 17.19 46.09 54.3v434.45c0 37.11-15.58 47.4-46.15 54.3l-.81.18V587H1327v-6.35l-.81-.18c-30.57-6.9-46.09-17.19-46.09-54.3V116.9L1479.87 587h11.33l205.59-483.21V536.9c-2.62 29.31-18 38.36-45.68 44.61l-.82.19v6.3h213.3v-6.3l-.82-.19c-27.71-6.25-43.46-15.3-46.08-44.61l-.14-445.2h.14c0-37.11 15.52-47.4 46.08-54.3m97.43 287.8c3.49-78.06 31.52-134.4 78.56-135.37 14.51.24 26.68 5 36.14 14.16 20.1 19.51 29.55 60.28 28.09 121.21zm-2.11 22h250v-1.05c-.71-59.69-18-106.12-51.34-138-28.82-27.55-71.49-42.71-116.31-42.71h-1c-23.26 0-51.79 5.64-72.09 15.86-23.11 10.7-43.49 26.7-60.45 47.7-27.3 33.83-43.84 79.55-47.86 130.93-.13 1.54-.24 3.08-.35 4.62s-.18 2.92-.25 4.39a332.64 332.64 0 0 0-.36 21.69C1860.79 507 1923.65 600 2035.3 600c98 0 155.07-71.64 169.3-167.8l-7.19-2.53c-25 51.68-69.9 83-121 79.18-69.76-5.22-123.2-75.95-118.35-161.63m532.69 157.68c-8.2 19.45-25.31 30.15-48.24 30.15s-43.89-15.74-58.78-44.34c-16-30.7-24.42-74.1-24.42-125.51 0-107 33.28-176.21 84.79-176.21 21.57 0 38.55 10.7 46.65 29.37zm165.84 76.28c-30.57-7.23-46.09-18-46.09-57V5.28L2424.77 60v6.7l1.14-.09c25.62-2.07 43 1.47 53.09 10.79 7.9 7.3 11.75 18.5 11.75 34.26v71.14c-18.31-11.69-40.09-17.38-66.52-17.38-53.6 0-102.59 22.57-137.92 63.56-36.83 42.72-56.3 101.1-56.3 168.81C2230 518.72 2289.53 600 2378.13 600c51.83 0 93.53-28.4 112.62-76.3V588h166.65v-6.66zm159.29-505.33c0-37.76-28.47-66.24-66.24-66.24-37.59 0-67 29.1-67 66.24s29.44 66.24 67 66.24c37.77 0 66.24-28.48 66.24-66.24m43.84 505.33c-30.57-7.23-46.09-18-46.09-57h-.13V166.65l-166.66 47.85v6.5l1 .09c36.06 3.21 45.93 15.63 45.93 57.77V588h166.8v-6.66zm427.05 0c-30.57-7.23-46.09-18-46.09-57V166.65L3082 212.92v6.52l.94.1c29.48 3.1 38 16.23 38 58.56v226c-9.83 19.45-28.27 31-50.61 31.78-36.23 0-56.18-24.47-56.18-68.9V166.66l-166.66 47.85V221l1 .09c36.06 3.2 45.94 15.62 45.94 57.77v191.27a214.48 214.48 0 0 0 3.47 39.82l3 13.05c14.11 50.56 51.08 77 109 77 49.06 0 92.06-30.37 111-77.89v66h166.66v-6.66zM3934.2 588v-6.67l-.81-.19c-33.17-7.65-46.09-22.07-46.09-51.43v-243.2c0-75.83-42.59-121.09-113.93-121.09-52 0-95.85 30.05-112.73 76.86-13.41-49.6-52-76.86-109.06-76.86-50.12 0-89.4 26.45-106.25 71.13v-69.87l-166.66 45.89v6.54l1 .09c35.63 3.16 45.93 15.94 45.93 57V588h155.5v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66V255.72c7-16.35 21.11-35.72 49-35.72 34.64 0 52.2 24 52.2 71.28V588h155.54v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66v-248a160.45 160.45 0 0 0-2.2-27.68c7.42-17.77 22.34-38.8 51.37-38.8 35.13 0 52.2 23.31 52.2 71.28V588z"></path></svg></a><div class="rp yj n cj yk cl"><p class="az b dj dk yl"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----dd09190245ce--------------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf yg yh" rel="noopener">About</a></p><p class="az b dj dk yl"><a href="https://help.medium.com/hc/en-us?source=post_page-----dd09190245ce--------------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf yg yh" rel="noopener">Help</a></p><p class="az b dj dk yl"><a href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dd09190245ce--------------------------------" class="eh ei by bz ca cb cc cd ce bk xn cf yg yh" rel="noopener">Legal</a></p></div></div><div class="aj ym yn cl"><p class="az b dj dk yo">Get the Medium app</p></div><div class="aj ym yp cl yq"><div class="ay s"><a href="https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&amp;mt=8&amp;ct=post_page&amp;source=post_page-----dd09190245ce--------------------------------" class="eh ei by bz ca cb cc cd ce bk ye yf cf yg yh" rel="noopener nofollow"><img alt="A button that says &#x27;Download on the App Store&#x27;, and if clicked it will lead you to the iOS App store" class="" src="https://miro.medium.com/max/270/1*Crl55Tm6yDNMoucPo1tvDg.png" width="135" height="41"/></a></div><div class="s"><a href="https://play.google.com/store/apps/details?id=com.medium.reader&amp;source=post_page-----dd09190245ce--------------------------------" class="eh ei by bz ca cb cc cd ce bk ye yf cf yg yh" rel="noopener nofollow"><img alt="A button that says &#x27;Get it on, Google Play&#x27;, and if clicked it will lead you to the Google Play store" class="" src="https://miro.medium.com/max/270/1*W_RAPQ62h0em559zluJLdQ.png" width="135" height="41"/></a></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__ = "main-20210416-005135-3792dfbcdc"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"auroraPage":{"isAuroraPageEnabled":true},"bookReader":{"assets":{},"reader":{"currentAsset":null,"settingsPanelIsOpen":false,"settings":{"fontFamily":"CHARTER","fontScale":"M","publisherStyling":true,"textAlignment":"start","theme":"White","lineSpacing":0,"wordSpacing":0,"letterSpacing":0},"internalNavCounter":0,"currentSelection":null,"highlights":[]}},"cache":{"experimentGroupSet":false,"reason":"This request is not using the cache middleware worker","group":"disabled","tags":[],"serverVariantState":"","middlewareEnabled":false},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"routingEntity":{"type":"COLLECTION","id":"7f60cf5620c9","explicit":true}},"debug":{"requestId":"1d5a76b7-2d45-46ce-b2d8-eda3e4cd4814","branchDeployConfig":null,"hybridDevServices":[],"originalSpanCarrier":{"ot-tracer-spanid":"66be843d16d5701e","ot-tracer-traceid":"2e20e191565469f8","ot-tracer-sampled":"true"}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"https:\u002F\u002Fwww.google.com\u002F","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false},"tracing":{},"config":{"nodeEnv":"production","version":"main-20210416-005135-3792dfbcdc","isTaggedVersion":false,"target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"lightstep.medium.systems","token":"ce5be895bef60919541332990ac9fef2","appVersion":"main-20210416-005135-3792dfbcdc"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","datadog":{"clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","context":{"deployment":{"target":"production","tag":"main-20210416-005135-3792dfbcdc","commit":"3792dfbcdce04d2f61acec07a6cbf73102a86c47"}},"datacenter":"us"},"googleAnalyticsCode":"UA-24232453-2","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium"},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618","7b6769f2748b","fc8964313712","ef8e90590e66","191186aaafa0","d944778ce714","bdc4052bbdba","88d9857e584e","9dc80918cc93","8a9336e5bb4","cef6983b292","54c98c43354d","193b68bd4fba","b7e45b22fec3","55760f21cdc5"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*kFrc4tBFM_tCis-2Ic87WA.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl"},"session":{"xsrf":""}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","variantFlags":[{"__typename":"VariantFlag","name":"allow_access","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_signup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_test_auth","valueType":{"__typename":"VariantFlagString","value":"disallow"}},{"__typename":"VariantFlag","name":"android_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"assign_default_topic_to_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_annual_plan","valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"}},{"__typename":"VariantFlag","name":"available_monthly_plan","valueType":{"__typename":"VariantFlagString","value":"60e220181034"}},{"__typename":"VariantFlag","name":"bane_add_user","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"bane_verify_domain","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"branch_seo_metadata","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"browsable_stream_config_bucket","valueType":{"__typename":"VariantFlagString","value":"curated-topics"}},{"__typename":"VariantFlag","name":"coronavirus_topic_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"covid_19_cdc_banner","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"default_seo_post_titles","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"deindex_from_external_search_threshold","valueType":{"__typename":"VariantFlagString","value":"1577865600000"}},{"__typename":"VariantFlag","name":"disable_android_subscription_activity_carousel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_ios_resume_reading_toast","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_ios_subscription_activity_carousel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_mobile_featured_chunk","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_post_recommended_from_friends_provider","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_local_currency","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_annual_renewal_reminder_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_app_flirty_thirty","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_parse_expires_at","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook_renewal_failure","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_about_page_routing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_general_admission","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_nav","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_profile_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_pub_follower_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_sticky_nav","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_tag_page_routing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_autotier","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards_byline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_automated_mission_control_triggers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_blogrolls","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_apple_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_client","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_integration","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_paypal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_trial_membership","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_io","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_text_me_the_app","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branding_fonts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cleansweep_double_writes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_client_error_tracking","valueType":{"__typename":"VariantFlagString","value":"none"}},{"__typename":"VariantFlag","name":"enable_confirm_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cta_meter","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_custom_domain_v2_settings","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_dedicated_series_tab_api_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_feature_logging","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_generation_pipeline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_tagline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_earn_redirect","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_edit_alt_text","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_email_sign_in_captcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_embedding_based_diversification","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_end_of_post_cleanup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_evhead_com_to_ev_medium_com_redirect","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_expanded_feature_chunk_pool","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_explicit_tag_filtering","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_filter_by_resend_rules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_filter_expire_processor","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_footer_app_buttons","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_from_creators_you_are_enjoying_below_todays_highlights","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_global_susi_modal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_one_tap","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook_subscription_cancelled","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_highlander_member_digest","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_hightower_user_minimum_guarantee","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_homepage_who_to_follow_module","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_homepage_write_button","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_hot_topics","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_post_stats","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_json_logs_trained_ranker","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_kbfd_rex","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_kbfd_rex_app_highlights","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_continue_this_thread","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_homepage","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_homepage_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_notifications","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_pay_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_pub_homepage_for_selected_domains","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_server_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_stories","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_topics","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_unread_notification_count_mutation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_login_code_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_marketing_emails","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_media_resource_try_catch","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_medium2_kbfd","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_member_only_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_membership_remove_section_a","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_miro_on_kubernetes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mission_control","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_modules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_rex_anno","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mps_digest_rendering","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mps_welcome_digest_rendering","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mute","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_checkout_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_collaborative_filtering_data","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_login_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_three_dot_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_nsfw_filtering","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_parsely","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_patronus_on_kubernetes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_popularity_feature","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_import","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_page_nav_stickiness_removal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_settings_screen","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_tax_status_clarity","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_primary_topic_for_mobile","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_profile_design_reminder","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_profile_page_seo_titles","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_publish_to_email_for_publication_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_receipt_notes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_all","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_edit_and_delete","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_moderation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_follow_feed_cache","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rito_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rtr_channel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_s3_sites","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_save_to_medium","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_signup_friction","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace_ranker_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_stripegate","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tick_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipalti_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_trending_posts_diversification","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tribute_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_triton_predictions","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_trumpland_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_twitter_auth_suggestions","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"entity_driven_subscription_milestone_1","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"entity_driven_subscription_milestone_2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"glyph_font_set","valueType":{"__typename":"VariantFlagString","value":"m2-unbound"}},{"__typename":"VariantFlag","name":"google_sign_in_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_generic_home_modules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_home_post_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_iceland_nux","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_pub_follow_email_opt_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"is_not_medium_subscriber","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"kill_fastrak","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"kill_stripe_express","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_post_referrers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_user_follows","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"low_signal_writer_level","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"make_nav_sticky","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"new_transition_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"post_edge_cache_enabled","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"posts_under_quota_fair_distribution","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"provider_for_credit_card_form","valueType":{"__typename":"VariantFlagString","value":"BRAINTREE"}},{"__typename":"VariantFlag","name":"pub_sidebar","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"redefine_average_post_reading_time","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"remove_low_quality_content_from_tags","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"remove_low_quality_posts_from_internal_search","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"remove_post_post_similarity","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"retrained_ranker","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"sign_up_with_email_button","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signin_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"signup_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"skip_sign_in_recaptcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"suppress_apple_missing_expires_date_alert","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"use_new_admin_topic_backend","valueType":{"__typename":"VariantFlagBoolean","value":true}}],"viewer":null,"meterPost({\"postId\":\"dd09190245ce\",\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\",\"sk\":null,\"source\":null}})":{"__ref":"MeteringInfo:{}"},"postResult({\"id\":\"dd09190245ce\"})":{"__ref":"Post:dd09190245ce"}},"MeteringInfo:{}":{"__typename":"MeteringInfo","postIds":[],"maxUnlockCount":3,"unlocksRemaining":0},"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png":{"id":"1*ChFMdf--f5jbm-AYv6VdYA@2x.png","__typename":"ImageMetadata"},"CustomStyleSheet:63d23b36fcaa":{"id":"63d23b36fcaa","__typename":"CustomStyleSheet","global":{"__typename":"GlobalStyles","colorPalette":{"__typename":"StyleSheetColorPalette","primary":{"__typename":"ColorValue","colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}}},"background":null},"fonts":{"__typename":"StyleSheetFonts","font1":{"__typename":"StyleSheetFont","name":"SANS_SERIF_1"},"font2":{"__typename":"StyleSheetFont","name":"SANS_SERIF_1"},"font3":{"__typename":"StyleSheetFont","name":"SERIF_2"}}},"header":{"__typename":"HeaderStyles","backgroundColor":{"__typename":"ColorValue","colorPalette":{"__typename":"ColorPalette","tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"rgb":"355876","alpha":"99"},"postBackgroundColor":null,"backgroundImage":{"__ref":"ImageMetadata:1*sfUruIusLq6tbpLx0sDYZQ.png"},"headerScale":"HEADER_SCALE_MEDIUM","horizontalAlignment":"CENTER","backgroundImageDisplayMode":"IMAGE_DISPLAY_MODE_FILL","backgroundImageVerticalAlignment":"END","backgroundColorDisplayMode":"COLOR_DISPLAY_MODE_SOLID","secondaryBackgroundColor":null,"nameColor":null,"nameTreatment":"NAME_TREATMENT_LOGO","postNameTreatment":"NAME_TREATMENT_LOGO","logoImage":{"__ref":"ImageMetadata:1*AGyTPCaRzVqL77kFwUwHKg.png"},"logoScale":"HEADER_SCALE_LARGE","taglineColor":{"__typename":"ColorValue","rgb":"ffffff","alpha":"ff"},"taglineTreatment":"TAGLINE_TREATMENT_HEADER"},"navigation":{"__typename":"HeaderNavigation","navItems":[{"__typename":"HeaderNavigationItem","name":"Editors' Picks","href":null,"type":"NAV_TYPE_TAG","tags":[{"__ref":"Tag:editors-pick"}],"tagSlugs":["editors-pick"]},{"__typename":"HeaderNavigationItem","name":"Features","href":null,"type":"NAV_TYPE_TAG","tags":[{"__ref":"Tag:tds-features"}],"tagSlugs":["tds-features"]},{"__typename":"HeaderNavigationItem","name":"Deep Dives","href":null,"type":"NAV_TYPE_TAG","tags":[{"__ref":"Tag:deep-dives"}],"tagSlugs":["deep-dives"]},{"__typename":"HeaderNavigationItem","name":"Grow","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fhow-to-get-the-most-out-of-towards-data-science-3bf37f75a345","type":"NAV_TYPE_LINK","tags":[],"tagSlugs":[]},{"__typename":"HeaderNavigationItem","name":"Contribute","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fquestions-96667b06af5","type":"NAV_TYPE_LINK","tags":[],"tagSlugs":[]}]},"postBody":null,"blogroll":null},"ImageMetadata:1*sfUruIusLq6tbpLx0sDYZQ.png":{"id":"1*sfUruIusLq6tbpLx0sDYZQ.png","__typename":"ImageMetadata","originalWidth":1401},"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png":{"id":"1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","__typename":"ImageMetadata","originalWidth":337,"originalHeight":122},"User:7e12c71dfa81":{"id":"7e12c71dfa81","__typename":"User","atsQualifiedAt":1612205680542},"ImageMetadata:1*eLxNtw6hQ4-3HrHda5BCCw.png":{"id":"1*eLxNtw6hQ4-3HrHda5BCCw.png","__typename":"ImageMetadata"},"NewsletterV3:d6fe9076899":{"id":"d6fe9076899","__typename":"NewsletterV3","slug":"the-variable","isSubscribed":false,"showPromo":true,"name":"The Variable","description":"Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss.","type":"NEWSLETTER_TYPE_COLLECTION","user":{"__ref":"User:895063a310f4"},"collection":{"__ref":"Collection:7f60cf5620c9"}},"Collection:7f60cf5620c9":{"id":"7f60cf5620c9","__typename":"Collection","domain":"towardsdatascience.com","googleAnalyticsId":null,"slug":"towards-data-science","colorBehavior":"ACCENT_COLOR_AND_FILL_BACKGROUND","isAuroraVisible":true,"favicon":{"__ref":"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png"},"name":"Towards Data Science","colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"customStyleSheet":{"__ref":"CustomStyleSheet:63d23b36fcaa"},"tagline":"A Medium publication sharing concepts, ideas and codes.","isAuroraEligible":true,"viewerIsEditor":false,"logo":{"__ref":"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"},"navItems":[{"__typename":"NavItem","title":"Data Science","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-science\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Machine Learning","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fmachine-learning\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Programming","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fprogramming\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Visualization","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-visualization\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Video","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fvideo\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"★","url":"https:\u002F\u002Ftowardsdatascience.com\u002Feditors-picks\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"About","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fabout-us\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Contribute","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fcontribute\u002Fhome","type":"EXTERNAL_LINK_NAV_ITEM"}],"creator":{"__ref":"User:7e12c71dfa81"},"subscriberCount":581154,"avatar":{"__ref":"ImageMetadata:1*eLxNtw6hQ4-3HrHda5BCCw.png"},"newsletterV3":{"__ref":"NewsletterV3:d6fe9076899"},"viewerIsFollowing":false,"viewerIsSubscribedToLetters":false,"canToggleEmail":true,"isUserSubscribedToCollectionEmails":false,"viewerIsMuting":false,"viewerCanEditOwnPosts":false,"viewerCanEditPosts":false,"description":"Your home for data science. A Medium publication sharing concepts, ideas and codes.","ampEnabled":false,"twitterUsername":"TDataScience","facebookPageId":null,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","status":"ACTIVE","isSubdomain":false}},"ptsQualifiedAt":1616092952992},"User:e2f299e30cb9":{"id":"e2f299e30cb9","__typename":"User","isFollowing":null,"viewerIsUser":false,"customStyleSheet":null,"isSuspended":false,"name":"Will Koehrsen","hasCompletedProfile":false,"bio":"Data Scientist at Cortex Intel, Data Science Communicator","imageId":"1*SckxdIFfjlR-cWXkL5ya-g.jpeg","username":"williamkoehrsen","customDomainState":{"__typename":"CustomDomainState","pending":null,"live":{"__typename":"CustomDomain","status":"ACTIVE","domain":"williamkoehrsen.medium.com","isSubdomain":true}},"isAuroraVisible":true,"createdAt":0,"mediumMemberAt":0,"lastPostCreatedAt":0,"socialStats":{"__typename":"SocialStats","followerCount":34256,"followingCount":15},"hasSubdomain":true,"isAllowEdsEnabled":false,"isBlocking":null,"isMuting":null,"allowNotes":true,"newsletterV3":null,"twitterScreenName":"koehrsen_will","followedCollections":5,"atsQualifiedAt":1612205648291},"ImageMetadata:1*AGyTPCaRzVqL77kFwUwHKg.png":{"id":"1*AGyTPCaRzVqL77kFwUwHKg.png","__typename":"ImageMetadata","originalWidth":1376,"originalHeight":429},"Tag:editors-pick":{"id":"editors-pick","__typename":"Tag","normalizedTagSlug":""},"Tag:tds-features":{"id":"tds-features","__typename":"Tag","normalizedTagSlug":""},"Tag:deep-dives":{"id":"deep-dives","__typename":"Tag","normalizedTagSlug":""},"Topic:ae5d4995e225":{"id":"ae5d4995e225","__typename":"Topic","name":"Data Science","slug":"data-science","isFollowing":null},"Paragraph:399b0743c0cf_0":{"id":"399b0743c0cf_0","__typename":"Paragraph","name":"694f","text":"(Source)","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*niPDCrom9F0lrdFa3LrBqA.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":1,"end":7,"type":"A","href":"https:\u002F\u002Fwww.flickr.com\u002Fphotos\u002Fpinks2000\u002F19160002254","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_1":{"id":"399b0743c0cf_1","__typename":"Paragraph","name":"4987","text":"Transfer Learning with Convolutional Neural Networks in PyTorch","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_2":{"id":"399b0743c0cf_2","__typename":"Paragraph","name":"d748","text":"How to use a pre-trained convolutional neural network for object recognition with PyTorch","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_3":{"id":"399b0743c0cf_3","__typename":"Paragraph","name":"16dc","text":"Although Keras is a great library with a simple API for building neural networks, the recent excitement about PyTorch finally got me interested in exploring this library. While I’m one to blindly follow the hype, the adoption by researchers and inclusion in the fast.ai library convinced me there must be something behind this new entry in deep learning.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":86,"end":117,"type":"A","href":"https:\u002F\u002Fwww.oreilly.com\u002Fideas\u002Fwhy-ai-and-machine-learning-researchers-are-beginning-to-embrace-pytorch","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":216,"end":240,"type":"A","href":"https:\u002F\u002Fhub.packtpub.com\u002Fwhat-is-pytorch-and-how-does-it-work\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":245,"end":278,"type":"A","href":"https:\u002F\u002Fwww.fast.ai\u002F2017\u002F09\u002F08\u002Fintroducing-pytorch-for-fastai\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_4":{"id":"399b0743c0cf_4","__typename":"Paragraph","name":"3743","text":"Since the best way to learn a new technology is by using it to solve a problem, my efforts to learn PyTorch started out with a simple project: use a pre-trained convolutional neural network for an object recognition task. In this article, we’ll see how to use PyTorch to accomplish this goal, along the way, learning a little about the library and about the important concept of transfer learning.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":10,"end":44,"type":"A","href":"https:\u002F\u002Ftowardsdatascience.com\u002Flearn-by-sharing-4461cc93f8c1","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_5":{"id":"399b0743c0cf_5","__typename":"Paragraph","name":"3c31","text":"While PyTorch might not be for everyone, at this point it’s impossible to say which deep learning library will come out on top, and being able to quickly learn and use different tools is crucial to succeed as a data scientist.","type":"BQ","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_6":{"id":"399b0743c0cf_6","__typename":"Paragraph","name":"1991","text":"The complete code for this project is available as a Jupyter Notebook on GitHub. This project was born out of my participation in the Udacity PyTorch scholarship challenge.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":53,"end":79,"type":"A","href":"https:\u002F\u002Fgithub.com\u002FWillKoehrsen\u002Fpytorch_challenge\u002Fblob\u002Fmaster\u002FTransfer%20Learning%20in%20PyTorch.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":134,"end":171,"type":"A","href":"https:\u002F\u002Fwww.udacity.com\u002Ffacebook-pytorch-scholarship","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_7":{"id":"399b0743c0cf_7","__typename":"Paragraph","name":"74c6","text":"Predicted from trained network","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*WFzHpV_Q6GQ_ybInr4PnCA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_8":{"id":"399b0743c0cf_8","__typename":"Paragraph","name":"a556","text":"Approach to Transfer Learning","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_9":{"id":"399b0743c0cf_9","__typename":"Paragraph","name":"4b69","text":"Our task will be to train a convolutional neural network (CNN) that can identify objects in images. We’ll be using the Caltech 101 dataset which has images in 101 categories. Most categories only have 50 images which typically isn’t enough for a neural network to learn to high accuracy. Therefore, instead of building and training a CNN from scratch, we’ll use a pre-built and pre-trained model applying transfer learning.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":119,"end":138,"type":"A","href":"http:\u002F\u002Fwww.vision.caltech.edu\u002FImage_Datasets\u002FCaltech101\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_10":{"id":"399b0743c0cf_10","__typename":"Paragraph","name":"2df7","text":"The basic premise of transfer learning is simple: take a model trained on a large dataset and transfer its knowledge to a smaller dataset. For object recognition with a CNN, we freeze the early convolutional layers of the network and only train the last few layers which make a prediction. The idea is the convolutional layers extract general, low-level features that are applicable across images — such as edges, patterns, gradients — and the later layers identify specific features within an image such as eyes or wheels.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":21,"end":38,"type":"A","href":"https:\u002F\u002Fmachinelearningmastery.com\u002Ftransfer-learning-for-deep-learning\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":139,"end":172,"type":"A","href":"http:\u002F\u002Fcs231n.github.io\u002Ftransfer-learning\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":94,"end":103,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_11":{"id":"399b0743c0cf_11","__typename":"Paragraph","name":"9579","text":"Thus, we can use a network trained on unrelated categories in a massive dataset (usually Imagenet) and apply it to our own problem because there are universal, low-level features shared between images. The images in the Caltech 101 dataset are very similar to those in the Imagenet dataset and the knowledge a model learns on Imagenet should easily transfer to this task.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":272,"end":289,"type":"A","href":"http:\u002F\u002Fwww.image-net.org\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_12":{"id":"399b0743c0cf_12","__typename":"Paragraph","name":"4bbe","text":"Idea behind Transfer Learning (source).","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*ZkPBqU8vx2vAgcLpz9pi5g.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":31,"end":37,"type":"A","href":"https:\u002F\u002Fwww.slideshare.net\u002Fxavigiro\u002Ftransfer-learning-d2l4-insightdcu-machine-learning-workshop-2017","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_13":{"id":"399b0743c0cf_13","__typename":"Paragraph","name":"f9ce","text":"Following is the general outline for transfer learning for object recognition:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_14":{"id":"399b0743c0cf_14","__typename":"Paragraph","name":"cdb2","text":"Load in a pre-trained CNN model trained on a large dataset","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_15":{"id":"399b0743c0cf_15","__typename":"Paragraph","name":"2963","text":"Freeze parameters (weights) in model’s lower convolutional layers","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_16":{"id":"399b0743c0cf_16","__typename":"Paragraph","name":"31cf","text":"Add custom classifier with several layers of trainable parameters to model","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_17":{"id":"399b0743c0cf_17","__typename":"Paragraph","name":"9b89","text":"Train classifier layers on training data available for task","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_18":{"id":"399b0743c0cf_18","__typename":"Paragraph","name":"c8f4","text":"Fine-tune hyperparameters and unfreeze more layers as needed","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_19":{"id":"399b0743c0cf_19","__typename":"Paragraph","name":"2573","text":"This approach has proven successful for a wide range of domains. It’s a great tool to have in your arsenal and generally the first approach that should be tried when confronted with a new image recognition problem.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":5,"end":63,"type":"A","href":"http:\u002F\u002Fruder.io\u002Ftransfer-learning\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_20":{"id":"399b0743c0cf_20","__typename":"Paragraph","name":"c5ec","text":"Data Set Up","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_21":{"id":"399b0743c0cf_21","__typename":"Paragraph","name":"6438","text":"With all data science problems, formatting the data correctly will determine the success or failure of the project. Fortunately, the Caltech 101 dataset images are clean and stored in the correct format. If we correctly set up the data directories, PyTorch makes it simple to associate the correct labels with each class. I separated the data into training, validation, and testing sets with a 50%, 25%, 25% split and then structured the directories as follows:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":348,"end":381,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_22":{"id":"399b0743c0cf_22","__typename":"Paragraph","name":"8644","text":"\u002Fdatadir\n    \u002Ftrain\n        \u002Fclass1\n        \u002Fclass2\n        .\n        .\n    \u002Fvalid\n        \u002Fclass1\n        \u002Fclass2\n        .\n        .\n    \u002Ftest\n        \u002Fclass1\n        \u002Fclass2\n        .\n        .","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_23":{"id":"399b0743c0cf_23","__typename":"Paragraph","name":"06ec","text":"The number of training images by classes is below (I use the terms classes and categories interchangeably):","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_24":{"id":"399b0743c0cf_24","__typename":"Paragraph","name":"3f33","text":"Number of training images by category.","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*H6FG-Tw0Ashd9_BuGxekNg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_25":{"id":"399b0743c0cf_25","__typename":"Paragraph","name":"b450","text":"We expect the model to do better on classes with more examples because it can better learn to map features to labels. To deal with the limited number of training examples we’ll use data augmentation during training (more later).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":49,"end":53,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":181,"end":198,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_26":{"id":"399b0743c0cf_26","__typename":"Paragraph","name":"4eb6","text":"As another bit of data exploration, we can also look at the size distribution.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_27":{"id":"399b0743c0cf_27","__typename":"Paragraph","name":"5fcb","text":"Distribution of average image sizes (in pixels) by category.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*NNBg34r4lVD3rIYifVEd-Q.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_28":{"id":"399b0743c0cf_28","__typename":"Paragraph","name":"f61d","text":"Imagenet models need an input size of 224 x 224 so one of the preprocessing steps will be to resize the images. Preprocessing is also where we will implement data augmentation for our training data.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":62,"end":75,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_29":{"id":"399b0743c0cf_29","__typename":"Paragraph","name":"0556","text":"Data Augmentation","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_30":{"id":"399b0743c0cf_30","__typename":"Paragraph","name":"a23d","text":"The idea of data augmentation is to artificially increase the number of training images our model sees by applying random transformations to the images. For example, we can randomly rotate or crop the images or flip them horizontally. We want our model to distinguish the objects regardless of orientation and data augmentation can also make a model invariant to transformations of the input data.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":12,"end":29,"type":"A","href":"http:\u002F\u002Fcs231n.stanford.edu\u002Freports\u002F2017\u002Fpdfs\u002F300.pdf","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":350,"end":359,"type":"A","href":"https:\u002F\u002Fstats.stackexchange.com\u002Fquestions\u002F239076\u002Fabout-cnn-kernels-and-scale-rotation-invariance","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_31":{"id":"399b0743c0cf_31","__typename":"Paragraph","name":"4113","text":"An elephant is still an elephant no matter which way it’s facing!","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_32":{"id":"399b0743c0cf_32","__typename":"Paragraph","name":"52e1","text":"Image transformations of training data.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*nyHJUPw_2UOmRqGQ5mAzkg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_33":{"id":"399b0743c0cf_33","__typename":"Paragraph","name":"9366","text":"Augmentation is generally only done during training (although test time augmentation is possible in the fast.ai library). Each epoch — one iteration through all the training images — a different random transformation is applied to each training image. This means that if we iterate through the data 20 times, our model will see 20 slightly different versions of each image. The overall result should be a model that learns the objects themselves and not how they are presented or artifacts in the image.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":104,"end":111,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":62,"end":111,"type":"A","href":"https:\u002F\u002Fblog.floydhub.com\u002Ften-techniques-from-fast-ai\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_34":{"id":"399b0743c0cf_34","__typename":"Paragraph","name":"7031","text":"Image Preprocessing","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_35":{"id":"399b0743c0cf_35","__typename":"Paragraph","name":"d57d","text":"This is the most important step of working with image data. During image preprocessing, we simultaneously prepare the images for our network and apply data augmentation to the training set. Each model will have different input requirements, but if we read through what Imagenet requires, we figure out that our images need to be 224x224 and normalized to a range.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":241,"end":286,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Fpytorch\u002Fvision\u002Fissues\u002F39","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_36":{"id":"399b0743c0cf_36","__typename":"Paragraph","name":"9ca0","text":"To process an image in PyTorch, we use transforms , simple operations applied to arrays. The validation (and testing) transforms are as follows:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":39,"end":49,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_37":{"id":"399b0743c0cf_37","__typename":"Paragraph","name":"8799","text":"Resize","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_38":{"id":"399b0743c0cf_38","__typename":"Paragraph","name":"8b19","text":"Center crop to 224 x 224","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_39":{"id":"399b0743c0cf_39","__typename":"Paragraph","name":"37de","text":"Convert to a tensor","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_40":{"id":"399b0743c0cf_40","__typename":"Paragraph","name":"65b3","text":"Normalize with mean and standard deviation","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_41":{"id":"399b0743c0cf_41","__typename":"Paragraph","name":"fda7","text":"The end result of passing through these transforms are tensors that can go into our network. The training transformations are similar but with the addition of random augmentations.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_42":{"id":"399b0743c0cf_42","__typename":"Paragraph","name":"e8e5","text":"First up, we define the training and validation transformations:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_43":{"id":"399b0743c0cf_43","__typename":"Paragraph","name":"4c0e","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:23990c80c8916fc59fa804974ea7c94d"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_44":{"id":"399b0743c0cf_44","__typename":"Paragraph","name":"1e22","text":"Then, we create datasets and DataLoaders . By using datasets.ImageFolder to make a dataset, PyTorch will automatically associate images with the correct labels provided our directory is set up as above. The datasets are then passed to a DataLoader , an iterator that yield batches of images and labels.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":16,"end":24,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":29,"end":40,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":52,"end":72,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":237,"end":247,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_45":{"id":"399b0743c0cf_45","__typename":"Paragraph","name":"686d","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:ca935a69230822516efd2c1c37f27888"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_46":{"id":"399b0743c0cf_46","__typename":"Paragraph","name":"2ab7","text":"We can see the iterative behavior of the DataLoader using the following:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":41,"end":51,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_47":{"id":"399b0743c0cf_47","__typename":"Paragraph","name":"5976","text":"# Iterate through the dataloader once\ntrainiter = iter(dataloaders['train'])\nfeatures, labels = next(trainiter)\nfeatures.shape, labels.shape","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_48":{"id":"399b0743c0cf_48","__typename":"Paragraph","name":"2ccf","text":"(torch.Size([128, 3, 224, 224]), torch.Size([128]))","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":51,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_49":{"id":"399b0743c0cf_49","__typename":"Paragraph","name":"c587","text":"The shape of a batch is (batch_size, color_channels, height, width). During training, validation, and eventually testing, we’ll iterate through the DataLoaders, with one pass through the complete dataset comprising one epoch. Every epoch, the training DataLoader will apply a slightly different random transformation to the images for training data augmentation.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":24,"end":67,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":148,"end":159,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":252,"end":262,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_50":{"id":"399b0743c0cf_50","__typename":"Paragraph","name":"4d4c","text":"Pre-Trained Models for Image Recognition","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_51":{"id":"399b0743c0cf_51","__typename":"Paragraph","name":"9e76","text":"With our data in shape, we next turn our attention to the model. For this, we’ll use a pre-trained convolutional neural network. PyTorch has a number of models that have already been trained on millions of images from 1000 classes in Imagenet. The complete list of models can be seen here. The performance of these models on Imagenet is shown below:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":234,"end":242,"type":"A","href":"http:\u002F\u002Fwww.image-net.org\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":279,"end":288,"type":"A","href":"https:\u002F\u002Fpytorch.org\u002Fdocs\u002Fstable\u002Ftorchvision\u002Fmodels.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_52":{"id":"399b0743c0cf_52","__typename":"Paragraph","name":"5369","text":"Pretrained models in PyTorch and performance on Imagenet (Source).","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*0W310-cMNHPWjErqPuGXpw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":58,"end":64,"type":"A","href":"https:\u002F\u002Fpytorch.org\u002Fdocs\u002Fstable\u002Ftorchvision\u002Fmodels.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_53":{"id":"399b0743c0cf_53","__typename":"Paragraph","name":"5aa6","text":"For this implementation, we’ll be using the VGG-16. Although it didn’t record the lowest error, I found it worked well for the task and was quicker to train than other models. The process to use a pre-trained model is well-established:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":44,"end":50,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_54":{"id":"399b0743c0cf_54","__typename":"Paragraph","name":"a02b","text":"Load in pre-trained weights from a network trained on a large dataset","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_55":{"id":"399b0743c0cf_55","__typename":"Paragraph","name":"586e","text":"Freeze all the weights in the lower (convolutional) layers: the layers to freeze are adjusted depending on similarity of new task to original dataset","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_56":{"id":"399b0743c0cf_56","__typename":"Paragraph","name":"9e26","text":"Replace the upper layers of the network with a custom classifier: the number of outputs must be set equal to the number of classes","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_57":{"id":"399b0743c0cf_57","__typename":"Paragraph","name":"ecc6","text":"Train only the custom classifier layers for the task thereby optimizing the model for smaller dataset","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_58":{"id":"399b0743c0cf_58","__typename":"Paragraph","name":"f6d7","text":"Loading in a pre-trained model in PyTorch is simple:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_59":{"id":"399b0743c0cf_59","__typename":"Paragraph","name":"3f19","text":"from torchvision import models\nmodel = model.vgg16(pretrained=True)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_60":{"id":"399b0743c0cf_60","__typename":"Paragraph","name":"bd1e","text":"This model has over 130 million parameters, but we’ll train only the very last few fully-connected layers. Initially, we freeze all of the model’s weights:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_61":{"id":"399b0743c0cf_61","__typename":"Paragraph","name":"a916","text":"# Freeze model weights\nfor param in model.parameters():\n    param.requires_grad = False","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_62":{"id":"399b0743c0cf_62","__typename":"Paragraph","name":"84b9","text":"Then, we add on our own custom classifier with the following layers:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_63":{"id":"399b0743c0cf_63","__typename":"Paragraph","name":"eb22","text":"Fully connected with ReLU activation, shape = (n_inputs, 256)","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_64":{"id":"399b0743c0cf_64","__typename":"Paragraph","name":"2a73","text":"Dropout with 40% chance of dropping","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_65":{"id":"399b0743c0cf_65","__typename":"Paragraph","name":"61f6","text":"Fully connected with log softmax output, shape = (256, n_classes)","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_66":{"id":"399b0743c0cf_66","__typename":"Paragraph","name":"3e2d","text":"import torch.nn as nn","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_67":{"id":"399b0743c0cf_67","__typename":"Paragraph","name":"f2f9","text":"# Add on classifier\nmodel.classifier[6] = nn.Sequential(\n                      nn.Linear(n_inputs, 256), \n                      nn.ReLU(), \n                      nn.Dropout(0.4),\n                      nn.Linear(256, n_classes),                   \n                      nn.LogSoftmax(dim=1))","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_68":{"id":"399b0743c0cf_68","__typename":"Paragraph","name":"e5f3","text":"When the extra layers are added to the model, they are set to trainable by default ( require_grad=True ). For the VGG-16, we’re only changing the very last original fully-connected layer. All of the weights in the convolutional layers and the the first 5 fully-connected layers are not trainable.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":85,"end":102,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_69":{"id":"399b0743c0cf_69","__typename":"Paragraph","name":"d763","text":"# Only training classifier[6]\nmodel.classifier","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_70":{"id":"399b0743c0cf_70","__typename":"Paragraph","name":"44f3","text":"Sequential(\n  (0): Linear(in_features=25088, out_features=4096, bias=True)\n  (1): ReLU(inplace)\n  (2): Dropout(p=0.5)\n  (3): Linear(in_features=4096, out_features=4096, bias=True)\n  (4): ReLU(inplace)\n  (5): Dropout(p=0.5)\n  (6): Sequential(\n    (0): Linear(in_features=4096, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.4)\n    (3): Linear(in_features=256, out_features=100, bias=True)\n    (4): LogSoftmax()\n  )\n)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":434,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_71":{"id":"399b0743c0cf_71","__typename":"Paragraph","name":"73cb","text":"The final outputs from the network are log probabilities for each of the 100 classes in our dataset. The model has a total of 135 million parameters, of which just over 1 million will be trained.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":39,"end":57,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_72":{"id":"399b0743c0cf_72","__typename":"Paragraph","name":"669f","text":"# Find total parameters and trainable parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f'{total_params:,} total parameters.')\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'{total_trainable_params:,} training parameters.')","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_73":{"id":"399b0743c0cf_73","__typename":"Paragraph","name":"c22d","text":"135,335,076 total parameters.\n1,074,532 training parameters.","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":60,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_74":{"id":"399b0743c0cf_74","__typename":"Paragraph","name":"9ef6","text":"Moving Model to GPU(s)","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_75":{"id":"399b0743c0cf_75","__typename":"Paragraph","name":"f2a8","text":"One of the best aspects of PyTorch is the ease of moving different parts of a model to one or more gpus so you can make full use of your hardware. Since I’m using 2 gpus for training, I first move the model to cuda and then create a DataParallel model distributed over the gpus:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":210,"end":214,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":233,"end":245,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":27,"end":103,"type":"A","href":"https:\u002F\u002Fpytorch.org\u002Fdocs\u002Fstable\u002Fnotes\u002Fcuda.html","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":115,"end":145,"type":"A","href":"https:\u002F\u002Fmedium.com\u002F@colinshaw_36798\u002Ffully-utilizing-your-deep-learning-gpus-61ee7acd3e57","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_76":{"id":"399b0743c0cf_76","__typename":"Paragraph","name":"9af5","text":"# Move to gpu\nmodel = model.to('cuda')\n# Distribute across 2 gpus\nmodel = nn.DataParallel(model)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_77":{"id":"399b0743c0cf_77","__typename":"Paragraph","name":"981f","text":"(This notebook should be run on a gpu to complete in a reasonable amount of time. The speedup over a cpu can easily by 10x or more.)","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_78":{"id":"399b0743c0cf_78","__typename":"Paragraph","name":"e115","text":"Training Loss and Optimizer","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_79":{"id":"399b0743c0cf_79","__typename":"Paragraph","name":"cf28","text":"The training loss (the error or difference between predictions and true values) is the negative log likelihood (NLL). (The NLL loss in PyTorch expects log probabilities, so we pass in the raw output from the model’s final layer.) PyTorch uses automatic differentiation which means that tensors keep track of not only their value, but also every operation (multiply, addition, activation, etc.) which contributes to the value. This means we can compute the gradient for any tensor in the network with respect to any prior tensor.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":87,"end":110,"type":"A","href":"https:\u002F\u002Fljvmiranda921.github.io\u002Fnotebook\u002F2017\u002F08\u002F13\u002Fsoftmax-and-the-negative-log-likelihood\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":230,"end":268,"type":"A","href":"https:\u002F\u002Fpytorch.org\u002Ftutorials\u002Fbeginner\u002Fblitz\u002Fautograd_tutorial.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_80":{"id":"399b0743c0cf_80","__typename":"Paragraph","name":"84ff","text":"What this means in practice is that the loss tracks not only the error, but also the contribution to the error by each weight and bias in the model. After we calculate the loss, we can then find the gradients of the loss with respect to each model parameter, a process known as backpropagation. Once we have the gradients, we use them to update the parameters with the optimizer. (If this doesn’t sink in at first, don’t worry, it takes a little while to grasp! This powerpoint helps to clarify some points.)","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":278,"end":293,"type":"A","href":"http:\u002F\u002Fneuralnetworksanddeeplearning.com\u002Fchap2.html","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":467,"end":477,"type":"A","href":"https:\u002F\u002Fwww.cs.toronto.edu\u002F~rgrosse\u002Fcourses\u002Fcsc321_2018\u002Fslides\u002Flec10.pdf","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":65,"end":70,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":85,"end":134,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_81":{"id":"399b0743c0cf_81","__typename":"Paragraph","name":"c33f","text":"The optimizer is Adam, an efficient variant of gradient descent that generally does not require hand-tuning the learning rate. During training, the optimizer uses the gradients of the loss to try and reduce the error (“optimize”) of the model output by adjusting the parameters. Only the parameters we added in the custom classifier will be optimized.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":4,"end":21,"type":"A","href":"https:\u002F\u002Fmachinelearningmastery.com\u002Fadam-optimization-algorithm-for-deep-learning\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":47,"end":63,"type":"A","href":"https:\u002F\u002Fwww.kdnuggets.com\u002F2017\u002F04\u002Fsimple-understand-gradient-descent-algorithm.html","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":279,"end":350,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_82":{"id":"399b0743c0cf_82","__typename":"Paragraph","name":"d448","text":"The loss and optimizer are initialized as follows:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_83":{"id":"399b0743c0cf_83","__typename":"Paragraph","name":"b986","text":"from torch import optim","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_84":{"id":"399b0743c0cf_84","__typename":"Paragraph","name":"4f11","text":"# Loss and optimizer\ncriteration = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters())","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_85":{"id":"399b0743c0cf_85","__typename":"Paragraph","name":"acfc","text":"With the pre-trained model, the custom classifier, the loss, the optimizer, and most importantly, the data, we’re ready for training.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_86":{"id":"399b0743c0cf_86","__typename":"Paragraph","name":"ab6e","text":"Training","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_87":{"id":"399b0743c0cf_87","__typename":"Paragraph","name":"e1bc","text":"Model training in PyTorch is a little more hands-on than in Keras because we have to do the backpropagation and parameter update step ourselves. The main loop iterates over a number of epochs and on each epoch we iterate through the train DataLoader . The DataLoader yields one batch of data and targets which we pass through the model. After each training batch, we calculate the loss, backpropagate the gradients of the loss with respect to the model parameters, and then update the parameters with the optimizer.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":239,"end":249,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":256,"end":266,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_88":{"id":"399b0743c0cf_88","__typename":"Paragraph","name":"c020","text":"I’d encourage you to look at the notebook for the complete training details, but the basic pseudo-code is as follows:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":33,"end":41,"type":"A","href":"https:\u002F\u002Fgithub.com\u002FWillKoehrsen\u002Fpytorch_challenge\u002Fblob\u002Fmaster\u002FTransfer%20Learning%20in%20PyTorch.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_89":{"id":"399b0743c0cf_89","__typename":"Paragraph","name":"9c2f","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:282d4e5e48dc427a8dc309006948f4d8"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_90":{"id":"399b0743c0cf_90","__typename":"Paragraph","name":"aad7","text":"We can continue to iterate through the data until we reach a given number of epochs. However, one problem with this approach is that our model will eventually start overfitting to the training data. To prevent this, we use our validation data and early stopping.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":165,"end":197,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":247,"end":261,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_91":{"id":"399b0743c0cf_91","__typename":"Paragraph","name":"5caf","text":"Early Stopping","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_92":{"id":"399b0743c0cf_92","__typename":"Paragraph","name":"9377","text":"Early stopping means halting training when the validation loss has not decreased for a number of epochs. As we continue training, the training loss will only decrease, but the validation loss will eventually reach a minimum and plateau or start to increase. We ideally want to stop training when the validation loss is at a minimum in the hope that this model will generalize best to the testing data. When using early stopping, every epoch in which the validation loss decreases, we save the parameters so we can later retrieve those with the best validation performance.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":14,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FEarly_stopping","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_93":{"id":"399b0743c0cf_93","__typename":"Paragraph","name":"ded9","text":"We implement early stopping by iterating through the validation DataLoader at the end of each training epoch. We calculate the validation loss and compare this to the lowest validation loss. If the loss is the lowest so far, we save the model. If the loss has not improved for a certain number of epochs, we halt training and return the best model which has been saved to disk.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":64,"end":74,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_94":{"id":"399b0743c0cf_94","__typename":"Paragraph","name":"a4c9","text":"Again, the complete code is in the notebook, but pseudo-code is:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_95":{"id":"399b0743c0cf_95","__typename":"Paragraph","name":"8d73","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:6aa7b0c84ee93aba08e21137a023bbd6"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_96":{"id":"399b0743c0cf_96","__typename":"Paragraph","name":"4c8c","text":"To see the benefits of early stopping, we can look at the training curves showing the training and validation losses and accuracy:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_97":{"id":"399b0743c0cf_97","__typename":"Paragraph","name":"74d8","text":"","type":"IMG","href":null,"layout":"OUTSET_ROW","metadata":{"__ref":"ImageMetadata:1*xega4-IjqB_YJq_u0aHuzA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_98":{"id":"399b0743c0cf_98","__typename":"Paragraph","name":"82ab","text":"Negative log likelihood and accuracy training curves","type":"IMG","href":null,"layout":"OUTSET_ROW_CONTINUE","metadata":{"__ref":"ImageMetadata:1*Gx-V-eTwCbVF6O6fXdjsPA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_99":{"id":"399b0743c0cf_99","__typename":"Paragraph","name":"edc7","text":"As expected, the training loss only continues to decrease with further training. The validation loss, on the other hand, reaches a minimum and plateaus. At a certain epoch, there is no return (or even a negative return) to further training. Our model will only start to memorize the training data and will not be able to generalize to testing data.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_100":{"id":"399b0743c0cf_100","__typename":"Paragraph","name":"5683","text":"Without early stopping, our model will train for longer than necessary and will overfit to the training data.","type":"BQ","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":71,"end":75,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_101":{"id":"399b0743c0cf_101","__typename":"Paragraph","name":"2f11","text":"Another point we can see from the training curves is that our model is not overfitting greatly. There is some overfitting as is always be the case, but the dropout after the first trainable fully connected layer prevents the training and validation losses from diverging too much.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":156,"end":163,"type":"A","href":"https:\u002F\u002Fmedium.com\u002F@amarbudhiraja\u002Fhttps-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_102":{"id":"399b0743c0cf_102","__typename":"Paragraph","name":"ad96","text":"Making Predictions: Inference","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_103":{"id":"399b0743c0cf_103","__typename":"Paragraph","name":"0de8","text":"In the notebook I take care of some boring — but necessary — details of saving and loading PyTorch models, but here we’ll move right to the best part: making predictions on new images. We know our model does well on training and even validation data, but the ultimate test is how it performs on a hold-out testing set it has not seen before. We saved 25% of the data for the purpose of determining if our model can generalize to new data.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_104":{"id":"399b0743c0cf_104","__typename":"Paragraph","name":"ea77","text":"Predicting with a trained model is pretty simple. We use the same syntax as for training and validation:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_105":{"id":"399b0743c0cf_105","__typename":"Paragraph","name":"6b0a","text":"for data, targets in testloader:\n    log_ps = model(data)\n    # Convert to probabilities\n    ps = torch.exp(log_ps)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_106":{"id":"399b0743c0cf_106","__typename":"Paragraph","name":"eabe","text":"ps.shape()","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_107":{"id":"399b0743c0cf_107","__typename":"Paragraph","name":"f9bd","text":"(128, 100)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":10,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_108":{"id":"399b0743c0cf_108","__typename":"Paragraph","name":"284b","text":"The shape of our probabilities are ( batch_size , n_classes ) because we have a probability for every class. We can find the accuracy by finding the highest probability for each example and compare these to the labels:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":37,"end":47,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":50,"end":59,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_109":{"id":"399b0743c0cf_109","__typename":"Paragraph","name":"0add","text":"# Find predictions and correct\npred = torch.max(ps, dim=1)\nequals = pred == targets","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_110":{"id":"399b0743c0cf_110","__typename":"Paragraph","name":"ffbd","text":"# Calculate accuracy\naccuracy = torch.mean(equals)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_111":{"id":"399b0743c0cf_111","__typename":"Paragraph","name":"b3c4","text":"When diagnosing a network used for object recognition, it can be helpful to look at both overall performance on the test set and individual predictions.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":5,"end":25,"type":"A","href":"https:\u002F\u002Fwww.coursera.org\u002Flecture\u002Fmachine-learning\u002Fmodel-selection-and-train-validation-test-sets-QGKbr","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_112":{"id":"399b0743c0cf_112","__typename":"Paragraph","name":"c781","text":"Model Results","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_113":{"id":"399b0743c0cf_113","__typename":"Paragraph","name":"d3ba","text":"Here are two predictions the model nails:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_114":{"id":"399b0743c0cf_114","__typename":"Paragraph","name":"6fde","text":"","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*RWI7Ct5JSUZz5gkFyD6Ntg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_115":{"id":"399b0743c0cf_115","__typename":"Paragraph","name":"d68d","text":"","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*QWozitnzhJz4wFFrQsAVZw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_116":{"id":"399b0743c0cf_116","__typename":"Paragraph","name":"0c12","text":"These are pretty easy, so I’m glad the model has no trouble!","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_117":{"id":"399b0743c0cf_117","__typename":"Paragraph","name":"3b48","text":"We don’t just want to focus on the correct predictions and we’ll take a look at some wrong outputs shortly. For now let’s evaluate the performance on the entire test set. For this, we want to iterate over the test DataLoader and calculate the loss and accuracy for every example.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":214,"end":224,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_118":{"id":"399b0743c0cf_118","__typename":"Paragraph","name":"fdc3","text":"Convolutional neural networks for object recognition are generally measured in terms of topk accuracy. This refers to the whether or not the real class was in the k most likely predicted classes. For example, top 5 accuracy is the % the right class was in the 5 highest probability predictions. You can get the topk most likely probabilities and classes from a PyTorch tensor as follows:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":88,"end":101,"type":"A","href":"https:\u002F\u002Fstats.stackexchange.com\u002Fquestions\u002F95391\u002Fwhat-is-the-definition-of-top-n-accuracy","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_119":{"id":"399b0743c0cf_119","__typename":"Paragraph","name":"91fd","text":"top_5_ps, top_5_classes = ps.topk(5, dim=1)\ntop_5_ps.shape","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_120":{"id":"399b0743c0cf_120","__typename":"Paragraph","name":"24de","text":"(128, 5)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":8,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_121":{"id":"399b0743c0cf_121","__typename":"Paragraph","name":"3bd9","text":"Evaluating the model on the entire testing set, we calculate the metrics:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_122":{"id":"399b0743c0cf_122","__typename":"Paragraph","name":"b108","text":"Final test top 1 weighted accuracy = 88.65%\nFinal test top 5 weighted accuracy = 98.00%\nFinal test cross entropy per image = 0.3772.","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":132,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_123":{"id":"399b0743c0cf_123","__typename":"Paragraph","name":"9dc8","text":"These compare favorably to the near 90% top 1 accuracy on the validation data. Overall, we conclude our pre-trained model was able to successfully transfer its knowledge from Imagenet to our smaller dataset.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":79,"end":207,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_124":{"id":"399b0743c0cf_124","__typename":"Paragraph","name":"e528","text":"Model Investigation","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_125":{"id":"399b0743c0cf_125","__typename":"Paragraph","name":"ed16","text":"Although the model does well, there’s likely steps to take which can make it even better. Often, the best way to figure out how to improve a model is to investigate its errors (note: this is also an effective self-improvement method.)","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_126":{"id":"399b0743c0cf_126","__typename":"Paragraph","name":"148f","text":"Our model isn’t great at identifying crocodiles, so let’s look at some test predictions from this category:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_127":{"id":"399b0743c0cf_127","__typename":"Paragraph","name":"cd94","text":"","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*0Iei4WIP-_MzAiPDKRL79g.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_128":{"id":"399b0743c0cf_128","__typename":"Paragraph","name":"ee4b","text":"","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*kEmyc_lZ0TWYOgvgLlUsqw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_129":{"id":"399b0743c0cf_129","__typename":"Paragraph","name":"7307","text":"","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*CSBH0Ow44MBMmSo-F4tKDA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_130":{"id":"399b0743c0cf_130","__typename":"Paragraph","name":"aa33","text":"Given the subtle distinction between crocodile and crocodile_head , and the difficulty of the second image, I’d say our model is not entirely unreasonable in these predictions. The ultimate goal in image recognition is to exceed human capabilities, and our model is nearly there!","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":37,"end":46,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":51,"end":65,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_131":{"id":"399b0743c0cf_131","__typename":"Paragraph","name":"ec98","text":"Finally, we’d expect the model to perform better on categories with more images, so we can look at a graph of accuracy in a given category versus the number of training images in that category:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_132":{"id":"399b0743c0cf_132","__typename":"Paragraph","name":"1982","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*fd6oQi9p4adNAg36aOs1Ow.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_133":{"id":"399b0743c0cf_133","__typename":"Paragraph","name":"2cd6","text":"There does appear to be a positive correlation between the number of training images and the top 1 test accuracy. This indicates that more training data augmentation could be helpful, or, even that we should use test time augmentation. We could also try a different pre-trained model, or build another custom classifier. At the moment, deep learning is still an empirical field meaning experimentation is often required!","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":212,"end":234,"type":"A","href":"https:\u002F\u002Fforums.fast.ai\u002Ft\u002Fchange-to-how-tta-works\u002F8474\u002F3","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":134,"end":181,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_134":{"id":"399b0743c0cf_134","__typename":"Paragraph","name":"eefa","text":"Conclusions","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:399b0743c0cf_135":{"id":"399b0743c0cf_135","__typename":"Paragraph","name":"00c8","text":"While there are easier deep learning libraries to use, the benefits of PyTorch are speed, control over every aspect of model architecture \u002F training, efficient implementation of backpropagation with tensor auto differentiation, and ease of debugging code due to the dynamic nature of PyTorch graphs. For production code or your own projects, I’m not sure there is yet a compelling argument for using PyTorch instead of a library with a gentler learning curve such as Keras, but it’s helpful to know how to use different options.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":59,"end":78,"type":"A","href":"http:\u002F\u002Fwww.goldsborough.me\u002Fml\u002Fai\u002Fpython\u002F2018\u002F02\u002F04\u002F20-17-20-a_promenade_of_pytorch\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":83,"end":88,"type":"A","href":"https:\u002F\u002Fwww.netguru.co\u002Fblog\u002Fdeep-learning-frameworks-comparison","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":199,"end":226,"type":"A","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fgetting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":266,"end":298,"type":"A","href":"https:\u002F\u002Fmedium.com\u002Fintuitionmachine\u002Fpytorch-dynamic-computational-graphs-and-modular-deep-learning-7e7f89f18d1","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":467,"end":472,"type":"A","href":"http:\u002F\u002Fkeras.io","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":363,"end":367,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_136":{"id":"399b0743c0cf_136","__typename":"Paragraph","name":"6f76","text":"Through this project, we were able to see the basics of using PyTorch as well as the concept of transfer learning, an effective method for object recognition. Instead of training a model from scratch, we can use existing architectures that have been trained on a large dataset and then tune them for our task. This reduces the time to train and often results in better overall performance. The outcome of this project is some knowledge of transfer learning and PyTorch that we can build on to build more complex applications.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":96,"end":113,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_137":{"id":"399b0743c0cf_137","__typename":"Paragraph","name":"ffdd","text":"We truly live in an incredible age for deep learning, where anyone can build deep learning models with easily available resources! Now get out there and take advantage of these resources by building your own project.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":110,"end":129,"type":"A","href":"https:\u002F\u002Fcolab.research.google.com","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:399b0743c0cf_138":{"id":"399b0743c0cf_138","__typename":"Paragraph","name":"c5ca","text":"As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my personal website willk.online.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":86,"end":101,"type":"A","href":"http:\u002F\u002Ftwitter.com\u002F@koehrsen_will","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":132,"end":144,"type":"A","href":"https:\u002F\u002Fwillk.online","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"ImageMetadata:1*niPDCrom9F0lrdFa3LrBqA.jpeg":{"id":"1*niPDCrom9F0lrdFa3LrBqA.jpeg","__typename":"ImageMetadata","originalHeight":683,"originalWidth":1024,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*WFzHpV_Q6GQ_ybInr4PnCA.png":{"id":"1*WFzHpV_Q6GQ_ybInr4PnCA.png","__typename":"ImageMetadata","originalHeight":280,"originalWidth":891,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*ZkPBqU8vx2vAgcLpz9pi5g.jpeg":{"id":"1*ZkPBqU8vx2vAgcLpz9pi5g.jpeg","__typename":"ImageMetadata","originalHeight":359,"originalWidth":638,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*H6FG-Tw0Ashd9_BuGxekNg.png":{"id":"1*H6FG-Tw0Ashd9_BuGxekNg.png","__typename":"ImageMetadata","originalHeight":453,"originalWidth":1175,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*NNBg34r4lVD3rIYifVEd-Q.png":{"id":"1*NNBg34r4lVD3rIYifVEd-Q.png","__typename":"ImageMetadata","originalHeight":387,"originalWidth":631,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*nyHJUPw_2UOmRqGQ5mAzkg.png":{"id":"1*nyHJUPw_2UOmRqGQ5mAzkg.png","__typename":"ImageMetadata","originalHeight":1719,"originalWidth":1728,"focusPercentX":null,"focusPercentY":null,"alt":null},"MediaResource:23990c80c8916fc59fa804974ea7c94d":{"id":"23990c80c8916fc59fa804974ea7c94d","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":""},"MediaResource:ca935a69230822516efd2c1c37f27888":{"id":"ca935a69230822516efd2c1c37f27888","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":""},"ImageMetadata:1*0W310-cMNHPWjErqPuGXpw.png":{"id":"1*0W310-cMNHPWjErqPuGXpw.png","__typename":"ImageMetadata","originalHeight":710,"originalWidth":471,"focusPercentX":null,"focusPercentY":null,"alt":null},"MediaResource:282d4e5e48dc427a8dc309006948f4d8":{"id":"282d4e5e48dc427a8dc309006948f4d8","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":""},"MediaResource:6aa7b0c84ee93aba08e21137a023bbd6":{"id":"6aa7b0c84ee93aba08e21137a023bbd6","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":""},"ImageMetadata:1*xega4-IjqB_YJq_u0aHuzA.png":{"id":"1*xega4-IjqB_YJq_u0aHuzA.png","__typename":"ImageMetadata","originalHeight":387,"originalWidth":501,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*Gx-V-eTwCbVF6O6fXdjsPA.png":{"id":"1*Gx-V-eTwCbVF6O6fXdjsPA.png","__typename":"ImageMetadata","originalHeight":387,"originalWidth":497,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*RWI7Ct5JSUZz5gkFyD6Ntg.png":{"id":"1*RWI7Ct5JSUZz5gkFyD6Ntg.png","__typename":"ImageMetadata","originalHeight":352,"originalWidth":1034,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*QWozitnzhJz4wFFrQsAVZw.png":{"id":"1*QWozitnzhJz4wFFrQsAVZw.png","__typename":"ImageMetadata","originalHeight":352,"originalWidth":1035,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*0Iei4WIP-_MzAiPDKRL79g.png":{"id":"1*0Iei4WIP-_MzAiPDKRL79g.png","__typename":"ImageMetadata","originalHeight":352,"originalWidth":1039,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*kEmyc_lZ0TWYOgvgLlUsqw.png":{"id":"1*kEmyc_lZ0TWYOgvgLlUsqw.png","__typename":"ImageMetadata","originalHeight":352,"originalWidth":1037,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*CSBH0Ow44MBMmSo-F4tKDA.png":{"id":"1*CSBH0Ow44MBMmSo-F4tKDA.png","__typename":"ImageMetadata","originalHeight":352,"originalWidth":1039,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*fd6oQi9p4adNAg36aOs1Ow.png":{"id":"1*fd6oQi9p4adNAg36aOs1Ow.png","__typename":"ImageMetadata","originalHeight":436,"originalWidth":424,"focusPercentX":null,"focusPercentY":null,"alt":null},"User:895063a310f4":{"id":"895063a310f4","__typename":"User","name":"Ludovic Benistant"},"Tag:machine-learning":{"id":"machine-learning","__typename":"Tag","displayTitle":"Machine Learning","normalizedTagSlug":""},"Tag:data-science":{"id":"data-science","__typename":"Tag","displayTitle":"Data Science","normalizedTagSlug":""},"Tag:deep-learning":{"id":"deep-learning","__typename":"Tag","displayTitle":"Deep Learning","normalizedTagSlug":""},"Tag:pytorch":{"id":"pytorch","__typename":"Tag","displayTitle":"Pytorch","normalizedTagSlug":""},"Tag:education":{"id":"education","__typename":"Tag","displayTitle":"Education","normalizedTagSlug":""},"ImageMetadata:1*xmRGG86D5OMh1crRfOcExA.png":{"id":"1*xmRGG86D5OMh1crRfOcExA.png","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"User:fb44e21903f3":{"id":"fb44e21903f3","__typename":"User","name":"Frank Andrade","username":"frank-andrade","bio":"Learn Python in Spanish with me on my YouTube channel → https:\u002F\u002Fwww.youtube.com\u002Fchannel\u002FUCGKngc82bux4NIDar572E5Q\u002Fvideos","imageId":"1*veEX4-CiLz5jqUjwWfQo_Q.jpeg","mediumMemberAt":0,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"frank-andrade.medium.com"}},"hasSubdomain":true,"isFollowing":null},"Post:d664e5683039":{"id":"d664e5683039","__typename":"Post","title":"A Complete Yet Simple Guide to Move From Excel to Python","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fa-complete-yet-simple-guide-to-move-from-excel-to-python-d664e5683039","previewImage":{"__ref":"ImageMetadata:1*xmRGG86D5OMh1crRfOcExA.png"},"isPublished":true,"firstPublishedAt":1617829567809,"readingTime":9.885849056603773,"statusForCollection":"APPROVED","isLocked":true,"visibility":"LOCKED","collection":{"__ref":"Collection:7f60cf5620c9"},"creator":{"__ref":"User:fb44e21903f3"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:0*hySI975thrTTxhUW":{"id":"0*hySI975thrTTxhUW","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"User:91303ad4525b":{"id":"91303ad4525b","__typename":"User","name":"Federico Mannucci","username":"federicomannucci_31459","bio":"Software Engineer in Italy, I love traveling and cooking.","imageId":"1*b6Y--lqIEaadJe_oiJqmxA.png","mediumMemberAt":0,"customDomainState":null,"hasSubdomain":false,"isFollowing":null},"Post:b794c152f7a1":{"id":"b794c152f7a1","__typename":"Post","title":"Five things I have learned after solving 500+ Leetcode questions","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Ffive-things-i-have-learned-after-solving-500-leetcode-questions-b794c152f7a1","previewImage":{"__ref":"ImageMetadata:0*hySI975thrTTxhUW"},"isPublished":true,"firstPublishedAt":1618167471216,"readingTime":4.350943396226415,"statusForCollection":"APPROVED","isLocked":true,"visibility":"LOCKED","collection":{"__ref":"Collection:7f60cf5620c9"},"creator":{"__ref":"User:91303ad4525b"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:1*JaSz_Xt3eg_FOZ8iRmSJBg.gif":{"id":"1*JaSz_Xt3eg_FOZ8iRmSJBg.gif","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"User:84a02493194a":{"id":"84a02493194a","__typename":"User","name":"Khuyen Tran","username":"khuyentran1476","bio":"Data scientist. I share a little bit of goodness every day through daily data science tips: https:\u002F\u002Fmathdatasimplified.com\u002F","imageId":"2*tiQVZEZxHMPcnVmEmN7UtA.jpeg","mediumMemberAt":1573916133000,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"khuyentran1476.medium.com"}},"hasSubdomain":true,"isFollowing":null},"Post:f571fb9da3d1":{"id":"f571fb9da3d1","__typename":"Post","title":"How to Create Mathematical Animations like 3Blue1Brown Using Python","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fhow-to-create-mathematical-animations-like-3blue1brown-using-python-f571fb9da3d1","previewImage":{"__ref":"ImageMetadata:1*JaSz_Xt3eg_FOZ8iRmSJBg.gif"},"isPublished":true,"firstPublishedAt":1617663051078,"readingTime":4.583018867924529,"statusForCollection":"APPROVED","isLocked":true,"visibility":"LOCKED","collection":{"__ref":"Collection:7f60cf5620c9"},"creator":{"__ref":"User:84a02493194a"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:0*PEP5fb8Hx3ir8NOF":{"id":"0*PEP5fb8Hx3ir8NOF","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"User:5ba760786877":{"id":"5ba760786877","__typename":"User","name":"Kurtis Pykes","username":"kurtispykes","bio":"AI Enthusiasts with writings covering AI, Data Science, and Freelancing","imageId":"0*iiP643J5leVXirxe.jpg","mediumMemberAt":1576012404000,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"kurtispykes.medium.com"}},"hasSubdomain":true,"isFollowing":null},"Post:7b93b3b8153b":{"id":"7b93b3b8153b","__typename":"Post","title":"Why I Stopped Applying For Data Science Jobs","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fwhy-i-stopped-applying-for-data-science-jobs-7b93b3b8153b","previewImage":{"__ref":"ImageMetadata:0*PEP5fb8Hx3ir8NOF"},"isPublished":true,"firstPublishedAt":1618430227277,"readingTime":7.662578616352202,"statusForCollection":"APPROVED","isLocked":true,"visibility":"LOCKED","collection":{"__ref":"Collection:7f60cf5620c9"},"creator":{"__ref":"User:5ba760786877"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:0*EGGjZrrAvQ9400GZ":{"id":"0*EGGjZrrAvQ9400GZ","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"User:5583c854307e":{"id":"5583c854307e","__typename":"User","name":"Charudatta Manwatkar","username":"charudattamanwatkar","bio":"Physics graduate. Data Scientist. Nerd. जे जे आपणासी ठावे। ते ते इतरांसी सांगावे।","imageId":"1*SWih8T2kG-v6IOfdLl3CnQ.jpeg","mediumMemberAt":1580290174061,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"charudattamanwatkar.medium.com"}},"hasSubdomain":true,"isFollowing":null},"Post:76020b14e42f":{"id":"76020b14e42f","__typename":"Post","title":"Hidden Gems of Python","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fhidden-gems-of-python-76020b14e42f","previewImage":{"__ref":"ImageMetadata:0*EGGjZrrAvQ9400GZ"},"isPublished":true,"firstPublishedAt":1617717205384,"readingTime":4.34937106918239,"statusForCollection":"APPROVED","isLocked":false,"visibility":"PUBLIC","collection":{"__ref":"Collection:7f60cf5620c9"},"creator":{"__ref":"User:5583c854307e"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:1*s6r29KXtOL-t1Gwv6IUtjw.jpeg":{"id":"1*s6r29KXtOL-t1Gwv6IUtjw.jpeg","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"User:7ba6be8a3022":{"id":"7ba6be8a3022","__typename":"User","name":"Alberto Romero","username":"albertoromgar","bio":"Half engineer, half neuroscientist. Interested in humanities | Words in The Startup, TDS, The Ascent, Mind Cafe | alber.romgar@gmail.com","imageId":"2*oMdIZBsnK8EFhQLUaAB5ZA.jpeg","mediumMemberAt":1523202691000,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"albertoromgar.medium.com"}},"hasSubdomain":true,"isFollowing":null},"Post:2c88ea183cdd":{"id":"2c88ea183cdd","__typename":"Post","title":"5 Reasons Why I Left the AI Industry","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002F5-reasons-why-i-left-the-ai-industry-2c88ea183cdd","previewImage":{"__ref":"ImageMetadata:1*s6r29KXtOL-t1Gwv6IUtjw.jpeg"},"isPublished":true,"firstPublishedAt":1617593399313,"readingTime":8.460377358490565,"statusForCollection":"APPROVED","isLocked":true,"visibility":"LOCKED","collection":{"__ref":"Collection:7f60cf5620c9"},"creator":{"__ref":"User:7ba6be8a3022"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:0*SMePvoIcDINwtAqb":{"id":"0*SMePvoIcDINwtAqb","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"User:6a8c6841e521":{"id":"6a8c6841e521","__typename":"User","name":"Madison Hunter","username":"madison13","bio":"CAN | Geoscience BSc undergrad student | Software Dev graduate | Words in The Startup, Towards Data Science, Better Programming, and Climate Conscious","imageId":"1*WE_v8m4E4u_CdWLHwYGP9A.jpeg","mediumMemberAt":1602881389000,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"madison13.medium.com"}},"hasSubdomain":true,"isFollowing":null},"Post:6499348228d7":{"id":"6499348228d7","__typename":"Post","title":"How Microlearning Can Help You Improve Your Data Science Skills in Less Than 10 Minutes Per Day","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fhow-microlearning-can-help-you-improve-your-data-science-skills-in-less-than-10-minutes-per-day-6499348228d7","previewImage":{"__ref":"ImageMetadata:0*SMePvoIcDINwtAqb"},"isPublished":true,"firstPublishedAt":1618234163666,"readingTime":5.611320754716981,"statusForCollection":"APPROVED","isLocked":true,"visibility":"LOCKED","collection":{"__ref":"Collection:7f60cf5620c9"},"creator":{"__ref":"User:6a8c6841e521"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:0*pWa6Aq-dgM3B0Cbb":{"id":"0*pWa6Aq-dgM3B0Cbb","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"User:db67131c013":{"id":"db67131c013","__typename":"User","name":"Cornellius Yudha Wijaya","username":"cornelliusyudhawijaya","bio":"Data Scientist@Allianz |LinkedIn:Cornellius Yudha Wijaya| Twitter:@CornelliusYW","imageId":"1*_dcEphSQexqOJDYq4sUh-Q.jpeg","mediumMemberAt":1553055725000,"customDomainState":null,"hasSubdomain":false,"isFollowing":null},"Post:ccb92d143ca3":{"id":"ccb92d143ca3","__typename":"Post","title":"4 Top Python IDE for Data Scientist","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002F4-top-python-ide-for-data-scientist-ccb92d143ca3","previewImage":{"__ref":"ImageMetadata:0*pWa6Aq-dgM3B0Cbb"},"isPublished":true,"firstPublishedAt":1618238533391,"readingTime":5.667924528301887,"statusForCollection":"APPROVED","isLocked":true,"visibility":"LOCKED","collection":{"__ref":"Collection:7f60cf5620c9"},"creator":{"__ref":"User:db67131c013"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"Post:dd09190245ce":{"id":"dd09190245ce","__typename":"Post","canonicalUrl":"","collection":{"__ref":"Collection:7f60cf5620c9"},"content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\",\"sk\":null,\"source\":null}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"validatedShareKey":"","isCacheableContent":false,"bodyModel":{"__typename":"RichText","paragraphs":[{"__ref":"Paragraph:399b0743c0cf_0"},{"__ref":"Paragraph:399b0743c0cf_1"},{"__ref":"Paragraph:399b0743c0cf_2"},{"__ref":"Paragraph:399b0743c0cf_3"},{"__ref":"Paragraph:399b0743c0cf_4"},{"__ref":"Paragraph:399b0743c0cf_5"},{"__ref":"Paragraph:399b0743c0cf_6"},{"__ref":"Paragraph:399b0743c0cf_7"},{"__ref":"Paragraph:399b0743c0cf_8"},{"__ref":"Paragraph:399b0743c0cf_9"},{"__ref":"Paragraph:399b0743c0cf_10"},{"__ref":"Paragraph:399b0743c0cf_11"},{"__ref":"Paragraph:399b0743c0cf_12"},{"__ref":"Paragraph:399b0743c0cf_13"},{"__ref":"Paragraph:399b0743c0cf_14"},{"__ref":"Paragraph:399b0743c0cf_15"},{"__ref":"Paragraph:399b0743c0cf_16"},{"__ref":"Paragraph:399b0743c0cf_17"},{"__ref":"Paragraph:399b0743c0cf_18"},{"__ref":"Paragraph:399b0743c0cf_19"},{"__ref":"Paragraph:399b0743c0cf_20"},{"__ref":"Paragraph:399b0743c0cf_21"},{"__ref":"Paragraph:399b0743c0cf_22"},{"__ref":"Paragraph:399b0743c0cf_23"},{"__ref":"Paragraph:399b0743c0cf_24"},{"__ref":"Paragraph:399b0743c0cf_25"},{"__ref":"Paragraph:399b0743c0cf_26"},{"__ref":"Paragraph:399b0743c0cf_27"},{"__ref":"Paragraph:399b0743c0cf_28"},{"__ref":"Paragraph:399b0743c0cf_29"},{"__ref":"Paragraph:399b0743c0cf_30"},{"__ref":"Paragraph:399b0743c0cf_31"},{"__ref":"Paragraph:399b0743c0cf_32"},{"__ref":"Paragraph:399b0743c0cf_33"},{"__ref":"Paragraph:399b0743c0cf_34"},{"__ref":"Paragraph:399b0743c0cf_35"},{"__ref":"Paragraph:399b0743c0cf_36"},{"__ref":"Paragraph:399b0743c0cf_37"},{"__ref":"Paragraph:399b0743c0cf_38"},{"__ref":"Paragraph:399b0743c0cf_39"},{"__ref":"Paragraph:399b0743c0cf_40"},{"__ref":"Paragraph:399b0743c0cf_41"},{"__ref":"Paragraph:399b0743c0cf_42"},{"__ref":"Paragraph:399b0743c0cf_43"},{"__ref":"Paragraph:399b0743c0cf_44"},{"__ref":"Paragraph:399b0743c0cf_45"},{"__ref":"Paragraph:399b0743c0cf_46"},{"__ref":"Paragraph:399b0743c0cf_47"},{"__ref":"Paragraph:399b0743c0cf_48"},{"__ref":"Paragraph:399b0743c0cf_49"},{"__ref":"Paragraph:399b0743c0cf_50"},{"__ref":"Paragraph:399b0743c0cf_51"},{"__ref":"Paragraph:399b0743c0cf_52"},{"__ref":"Paragraph:399b0743c0cf_53"},{"__ref":"Paragraph:399b0743c0cf_54"},{"__ref":"Paragraph:399b0743c0cf_55"},{"__ref":"Paragraph:399b0743c0cf_56"},{"__ref":"Paragraph:399b0743c0cf_57"},{"__ref":"Paragraph:399b0743c0cf_58"},{"__ref":"Paragraph:399b0743c0cf_59"},{"__ref":"Paragraph:399b0743c0cf_60"},{"__ref":"Paragraph:399b0743c0cf_61"},{"__ref":"Paragraph:399b0743c0cf_62"},{"__ref":"Paragraph:399b0743c0cf_63"},{"__ref":"Paragraph:399b0743c0cf_64"},{"__ref":"Paragraph:399b0743c0cf_65"},{"__ref":"Paragraph:399b0743c0cf_66"},{"__ref":"Paragraph:399b0743c0cf_67"},{"__ref":"Paragraph:399b0743c0cf_68"},{"__ref":"Paragraph:399b0743c0cf_69"},{"__ref":"Paragraph:399b0743c0cf_70"},{"__ref":"Paragraph:399b0743c0cf_71"},{"__ref":"Paragraph:399b0743c0cf_72"},{"__ref":"Paragraph:399b0743c0cf_73"},{"__ref":"Paragraph:399b0743c0cf_74"},{"__ref":"Paragraph:399b0743c0cf_75"},{"__ref":"Paragraph:399b0743c0cf_76"},{"__ref":"Paragraph:399b0743c0cf_77"},{"__ref":"Paragraph:399b0743c0cf_78"},{"__ref":"Paragraph:399b0743c0cf_79"},{"__ref":"Paragraph:399b0743c0cf_80"},{"__ref":"Paragraph:399b0743c0cf_81"},{"__ref":"Paragraph:399b0743c0cf_82"},{"__ref":"Paragraph:399b0743c0cf_83"},{"__ref":"Paragraph:399b0743c0cf_84"},{"__ref":"Paragraph:399b0743c0cf_85"},{"__ref":"Paragraph:399b0743c0cf_86"},{"__ref":"Paragraph:399b0743c0cf_87"},{"__ref":"Paragraph:399b0743c0cf_88"},{"__ref":"Paragraph:399b0743c0cf_89"},{"__ref":"Paragraph:399b0743c0cf_90"},{"__ref":"Paragraph:399b0743c0cf_91"},{"__ref":"Paragraph:399b0743c0cf_92"},{"__ref":"Paragraph:399b0743c0cf_93"},{"__ref":"Paragraph:399b0743c0cf_94"},{"__ref":"Paragraph:399b0743c0cf_95"},{"__ref":"Paragraph:399b0743c0cf_96"},{"__ref":"Paragraph:399b0743c0cf_97"},{"__ref":"Paragraph:399b0743c0cf_98"},{"__ref":"Paragraph:399b0743c0cf_99"},{"__ref":"Paragraph:399b0743c0cf_100"},{"__ref":"Paragraph:399b0743c0cf_101"},{"__ref":"Paragraph:399b0743c0cf_102"},{"__ref":"Paragraph:399b0743c0cf_103"},{"__ref":"Paragraph:399b0743c0cf_104"},{"__ref":"Paragraph:399b0743c0cf_105"},{"__ref":"Paragraph:399b0743c0cf_106"},{"__ref":"Paragraph:399b0743c0cf_107"},{"__ref":"Paragraph:399b0743c0cf_108"},{"__ref":"Paragraph:399b0743c0cf_109"},{"__ref":"Paragraph:399b0743c0cf_110"},{"__ref":"Paragraph:399b0743c0cf_111"},{"__ref":"Paragraph:399b0743c0cf_112"},{"__ref":"Paragraph:399b0743c0cf_113"},{"__ref":"Paragraph:399b0743c0cf_114"},{"__ref":"Paragraph:399b0743c0cf_115"},{"__ref":"Paragraph:399b0743c0cf_116"},{"__ref":"Paragraph:399b0743c0cf_117"},{"__ref":"Paragraph:399b0743c0cf_118"},{"__ref":"Paragraph:399b0743c0cf_119"},{"__ref":"Paragraph:399b0743c0cf_120"},{"__ref":"Paragraph:399b0743c0cf_121"},{"__ref":"Paragraph:399b0743c0cf_122"},{"__ref":"Paragraph:399b0743c0cf_123"},{"__ref":"Paragraph:399b0743c0cf_124"},{"__ref":"Paragraph:399b0743c0cf_125"},{"__ref":"Paragraph:399b0743c0cf_126"},{"__ref":"Paragraph:399b0743c0cf_127"},{"__ref":"Paragraph:399b0743c0cf_128"},{"__ref":"Paragraph:399b0743c0cf_129"},{"__ref":"Paragraph:399b0743c0cf_130"},{"__ref":"Paragraph:399b0743c0cf_131"},{"__ref":"Paragraph:399b0743c0cf_132"},{"__ref":"Paragraph:399b0743c0cf_133"},{"__ref":"Paragraph:399b0743c0cf_134"},{"__ref":"Paragraph:399b0743c0cf_135"},{"__ref":"Paragraph:399b0743c0cf_136"},{"__ref":"Paragraph:399b0743c0cf_137"},{"__ref":"Paragraph:399b0743c0cf_138"}],"sections":[{"__typename":"Section","name":"9b6e","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"d148","startIndex":8,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"427a","startIndex":20,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"84dd","startIndex":50,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"e9b3","startIndex":86,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"b205","startIndex":102,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"d3d1","startIndex":117,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"b7b7","startIndex":124,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"369b","startIndex":134,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"6d6c","startIndex":138,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}]}},"creator":{"__ref":"User:e2f299e30cb9"},"customStyleSheet":{"__ref":"CustomStyleSheet:63d23b36fcaa"},"firstPublishedAt":1543260273425,"isLocked":false,"isPublished":true,"isShortform":false,"layerCake":3,"primaryTopic":{"__ref":"Topic:ae5d4995e225"},"title":"Transfer Learning with Convolutional Neural Networks in PyTorch","isMarkedPaywallOnly":false,"readCreatorPostsCount":0,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Ftransfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce","isLimitedState":false,"visibility":"PUBLIC","license":"ALL_RIGHTS_RESERVED","allowResponses":true,"newsletterId":"","sequence":null,"tags":[{"__ref":"Tag:machine-learning"},{"__ref":"Tag:data-science"},{"__ref":"Tag:deep-learning"},{"__ref":"Tag:pytorch"},{"__ref":"Tag:education"}],"topics":[{"__typename":"Topic","topicId":"1eca0103fff3","name":"Machine Learning"},{"__typename":"Topic","topicId":"ae5d4995e225","name":"Data Science"}],"viewerClapCount":0,"showSubscribeToProfilePromo":false,"showSubscribeToCollectionNewsletterV3Promo":true,"inResponseToPostResult":null,"isNewsletter":false,"socialTitle":"","socialDek":"","noIndex":null,"curationStatus":null,"metaDescription":"","latestPublishedAt":1543261021733,"readingTime":14.171698113207547,"previewContent":{"__typename":"PreviewContent","subtitle":"How to use a pre-trained convolutional neural network for object recognition with PyTorch"},"previewImage":{"__ref":"ImageMetadata:1*niPDCrom9F0lrdFa3LrBqA.jpeg"},"creatorPartnerProgramEnrollmentStatus":"PERMISSION_DENIED","clapCount":1796,"lockedSource":"LOCKED_POST_SOURCE_NONE","isSuspended":false,"pendingCollection":null,"statusForCollection":"APPROVED","pinnedAt":0,"pinnedByCreatorAt":0,"curationEligibleAt":0,"responseDistribution":"NOT_DISTRIBUTED","shareKey":null,"internalLinks({\"paging\":{\"limit\":8}})":{"__typename":"InternalLinksConnection","items":[{"__ref":"Post:d664e5683039"},{"__ref":"Post:b794c152f7a1"},{"__ref":"Post:f571fb9da3d1"},{"__ref":"Post:7b93b3b8153b"},{"__ref":"Post:76020b14e42f"},{"__ref":"Post:2c88ea183cdd"},{"__ref":"Post:6499348228d7"},{"__ref":"Post:ccb92d143ca3"}]},"collaborators":[],"translationSourcePost":null,"inResponseToMediaResource":null,"audioVersionUrl":"","seoTitle":"","updatedAt":1543266958017,"shortformType":"SHORTFORM_TYPE_LINK","structuredData":"","seoDescription":"","postResponses":{"__typename":"PostResponses","count":14},"latestPublishedVersion":"399b0743c0cf","isPublishToEmail":false,"readingList":"READING_LIST_NONE","voterCount":424,"recommenders":[]}}</script><script src="https://cdn-client.medium.com/lite/static/js/manifest.574c1120.js"></script><script src="https://cdn-client.medium.com/lite/static/js/8464.d0ef046c.js"></script><script src="https://cdn-client.medium.com/lite/static/js/main.2cbfebac.js"></script><script src="https://cdn-client.medium.com/lite/static/js/5573.159bf40f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/instrumentation.89edd61c.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/reporting.9dcab9da.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1752.a348f767.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2833.383a48e6.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8342.6aa0b45e.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4930.d16bc692.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9692.f53a0a5c.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4586.06957e16.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5064.e0fb94df.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9046.264aa59f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2846.c5274fde.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9990.2a2673c0.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7012.7479f8d3.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9972.7dc06316.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4379.7144202f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5127.c2b8ad8b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8751.7541205c.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2955.bca4ef08.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7131.30fc9acf.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6163.fec1eafd.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8127.1c99358b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2514.eb9a5ab7.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6371.e219583c.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7496.22ce1eef.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1725.34157f91.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/3874.e925b298.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8953.58065604.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8286.960df19c.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9454.4c3bc6eb.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/Post.49ad0332.chunk.js"></script><script>window.main();</script></body></html>